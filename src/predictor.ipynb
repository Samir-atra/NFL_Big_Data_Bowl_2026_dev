{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"1afe4cc5","cell_type":"code","source":"!pip install --upgrade tensorflow keras\n\nimport os\nimport pandas as pd\nimport tensorflow as tf\nimport jax.numpy as jnp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport kaggle_evaluation.nfl_inference_server\nimport joblib\n\ndef get_feature_label_specs(dataset):\n    \"\"\"\n    Gets the feature and label specifications from a TensorFlow Dataset.\n\n    Args:\n        dataset (tf.data.Dataset): The TensorFlow Dataset.\n\n    Returns:\n        tuple: A tuple containing the feature and label specifications.\n               (feature_spec, label_spec)\n    \"\"\"\n    element_spec = dataset.element_spec\n    return element_spec[0], element_spec[1]\n\ndef create_preprocessor(features_df: pd.DataFrame):\n    \"\"\"\n    Creates a preprocessor for the NFL Big Data Bowl 2026 prediction data.\n\n    Args:\n        features_df (pd.DataFrame): The dataframe with the features.\n\n    Returns:\n        ColumnTransformer: The preprocessor.\n    \"\"\"\n    categorical_features = ['play_direction', 'player_position', 'player_side', 'player_role', 'nfl_id']\n    numerical_features = ['x', 'y', 's', 'a', 'dir', 'o', 'absolute_yardline_number', 'player_weight', 'num_frames_output', 'ball_land_x', 'ball_land_y', 'age', 'height_inches']\n    boolean_features = ['player_to_predict']\n\n    numerical_transformer = StandardScaler()\n    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n    boolean_transformer = FunctionTransformer(lambda x: x.astype(int))\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numerical_transformer, numerical_features),\n            ('cat', categorical_transformer, categorical_features),\n            ('bool', boolean_transformer, boolean_features)\n        ],\n        remainder='drop'\n    )\n\n    return preprocessor\n\ndef height_to_inches(height_str):\n    \"\"\"\n    Converts height string 'feet-inches' to inches.\n    \"\"\"\n    if isinstance(height_str, str):\n        feet, inches = map(int, height_str.split('-'))\n        return feet * 12 + inches\n    return np.nan\n\nSEQUENCE_LENGTH = 10\n\ndef _create_sequences_for_group(group_df: pd.DataFrame, sequence_length):\n    \"\"\"\n    Creates sequences of features and corresponding labels for a single player/play group.\n    \"\"\"\n    # Ensure the DataFrame is sorted by frame_id\n    group_df = group_df.sort_values(by='frame_id').reset_index(drop=True)\n\n    num_frames = len(group_df)\n    if num_frames < sequence_length + 1:\n        return np.array([]), np.array([])\n\n    # Extract features and labels as numpy arrays\n    feature_cols = [col for col in group_df.columns if col not in ['game_id', 'play_id', 'nfl_id', 'frame_id', 'x_label', 'y_label']]\n    features_array = group_df[feature_cols].values\n    labels_array = group_df[['x_label', 'y_label']].values\n\n    sequences = []\n    labels = []\n\n    # Number of complete sequences that can be formed\n    num_sequences = num_frames - sequence_length\n\n    for i in range(num_sequences):\n        sequences.append(features_array[i : i + sequence_length])\n        labels.append(labels_array[i + sequence_length])\n\n    return np.array(sequences), np.array(labels)\n\n\ndef load_and_prepare_data(data_dir, test_size=0.2, random_state=42):\n    \"\"\"\n    Loads input and output data from CSV files in the specified directory,\n    merges them, preprocesses the features, splits them into training and \n    validation sets, and returns them as TensorFlow Datasets.\n    The data is prepared into sequences of SEQUENCE_LENGTH frames.\n\n    Args:\n        data_dir (str): The path to the directory containing the training data.\n        test_size (float): The proportion of the dataset to allocate to the validation set.\n        random_state (int): The seed for the random number generator used for the split.\n\n    Returns:\n        tuple: A tuple containing the training and validation TensorFlow Datasets,\n               and the preprocessor.\n               (train_dataset, val_dataset, preprocessor)\n    \"\"\"\n    input_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('input')])\n    output_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('output')])\n\n    input_dfs = [pd.read_csv(f) for f in input_files]\n    output_dfs = [pd.read_csv(f) for f in output_files]\n\n    input_df = pd.concat(input_dfs, ignore_index=True)\n    output_df = pd.concat(output_dfs, ignore_index=True)\n\n    merged_df = pd.merge(input_df, output_df, on=['game_id', 'play_id', 'nfl_id', 'frame_id'], suffixes=('', '_label'))\n\n    # Feature Engineering\n    merged_df['height_inches'] = merged_df['player_height'].apply(height_to_inches)\n    \n    game_date_str = merged_df['game_id'].astype(str).str[:8]\n    game_date = pd.to_datetime(game_date_str, format='%Y%m%d')\n    player_birth_date = pd.to_datetime(merged_df['player_birth_date'])\n    merged_df['age'] = (game_date - player_birth_date).dt.days / 365.25\n\n    all_sequences = []\n    all_labels = []\n\n    # Define the columns that will be used as features for the preprocessor\n    # This list should exclude labels and identifiers that are not model features\n    feature_cols_for_model = [\n        'x', 'y', 's', 'a', 'dir', 'o', 'absolute_yardline_number',\n        'player_weight', 'num_frames_output', 'ball_land_x', 'ball_land_y',\n        'age', 'height_inches', 'play_direction', 'player_position',\n        'player_side', 'player_role', 'nfl_id', 'player_to_predict'\n    ]\n\n    # Create a DataFrame with only the features that will be preprocessed\n    # This is what the preprocessor will be fitted on\n    features_for_preprocessor_fitting = merged_df[feature_cols_for_model]\n\n    preprocessor = create_preprocessor(features_for_preprocessor_fitting)\n    preprocessor.fit(features_for_preprocessor_fitting) # Fit the preprocessor here\n\n    # Apply preprocessing to the entire feature set\n    # This will return a sparse matrix, convert to dense array for sequence creation\n    processed_features_array = preprocessor.transform(features_for_preprocessor_fitting).toarray()\n    \n    # Create a DataFrame from the processed features to easily merge back with identifiers\n    processed_features_df = pd.DataFrame(processed_features_array, index=merged_df.index)\n    \n    # Add back identifiers needed for grouping and labels\n    processed_df = pd.concat([merged_df[['game_id', 'play_id', 'nfl_id', 'frame_id', 'x_label', 'y_label']], processed_features_df], axis=1)\n\n    all_sequences = []\n    all_labels = []\n\n    # Group by game, play, and player to create sequences\n    for (game_id, play_id, nfl_id), group_df in processed_df.groupby(['game_id', 'play_id', 'nfl_id']):\n        sequences, labels = _create_sequences_for_group(group_df, SEQUENCE_LENGTH)\n        # print(f\"Group: {game_id}, {play_id}, {nfl_id} - Sequences length: {len(sequences)}, Labels length: {len(labels)}\")\n        if sequences.size > 0 and labels.size > 0:\n            all_sequences.append(sequences)\n            all_labels.append(labels)\n\n    if not all_sequences:\n        raise ValueError(\"No sequences could be created. Please check data and SEQUENCE_LENGTH.\")\n\n    X = np.concatenate(all_sequences, axis=0)\n    y = np.concatenate(all_labels, axis=0)\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n\n    return train_dataset, val_dataset, preprocessor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:27:14.703980Z","iopub.execute_input":"2025-11-16T06:27:14.704234Z","iopub.status.idle":"2025-11-16T06:27:47.833676Z","shell.execute_reply.started":"2025-11-16T06:27:14.704213Z","shell.execute_reply":"2025-11-16T06:27:47.832573Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/site-packages (2.20.0)\nRequirement already satisfied: keras in /usr/local/lib/python3.12/site-packages (3.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.3.1)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/site-packages (from tensorflow) (25.9.23)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/site-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/site-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from tensorflow) (25.0)\nRequirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (6.33.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.32.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from tensorflow) (80.9.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (3.2.0)\nRequirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/site-packages (from tensorflow) (4.15.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.0.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/site-packages (from tensorflow) (1.76.0)\nRequirement already satisfied: tensorboard~=2.20.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.20.0)\nRequirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.3.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (3.15.1)\nRequirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/site-packages (from tensorflow) (0.5.3)\nRequirement already satisfied: rich in /usr/local/lib/python3.12/site-packages (from keras) (14.2.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.12/site-packages (from keras) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.12/site-packages (from keras) (0.17.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich->keras) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich->keras) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"id":"3429fbe7","cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\n# from data_loader import load_and_prepare_data, SEQUENCE_LENGTH\n\ndef build_model(input_features, output_shape, lstm_units=64):\n    \"\"\"\n    Builds a sequential model with two LSTM layers.\n\n    Args:\n        input_features (int): The number of input features per timestep.\n        output_shape (int): The number of output units.\n        lstm_units (int): The number of units in the LSTM layers.\n\n    Returns:\n        keras.Model: The compiled Keras model.\n    \"\"\"\n    model = keras.Sequential([\n        layers.Input(shape=(SEQUENCE_LENGTH, input_features)),  # Input shape for a sequence of timesteps\n        layers.LSTM(lstm_units),\n        layers.Dense(output_shape)\n    ])\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n                  loss='mse',\n                  metrics=['mae'])\n    return model\n\ndef train_model(model, train_dataset, val_dataset, epochs, batch_size):\n    \"\"\"\n    Trains the Keras model.\n    \"\"\"\n    train_dataset = train_dataset.shuffle(buffer_size=30000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    if val_dataset:\n        val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    print(\"Starting model training...\")\n    history = model.fit(train_dataset,\n                        epochs=epochs,\n                        validation_data=val_dataset)\n    print(\"Model training finished.\")\n    return history\n\n# def main():\n\"\"\"\nMain function to load data, build, and train the model.\n\"\"\"\nprediction_data_dir = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n\nbatch_size = 32\nepochs = 1000\n\ntrain_ds, val_ds, preprocessor = load_and_prepare_data(prediction_data_dir)\n\nif train_ds.cardinality().numpy() == 0:\n    print(\"No training data generated. Please check data loading and feature engineering.\")\n    # return\n\n# Get the input and output shapes from the dataset specs\nfeature_spec, label_spec = train_ds.element_spec\ninput_features = feature_spec.shape[1] # Now shape is (SEQUENCE_LENGTH, input_features)\noutput_shape = label_spec.shape[0]\n\nmodel = build_model(input_features, output_shape)\nmodel.summary()\n\ntrain_model(model, train_ds, val_ds, epochs, batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T06:27:52.493446Z","iopub.execute_input":"2025-11-16T06:27:52.493927Z","execution_failed":"2025-11-16T06:32:46.041Z"}},"outputs":[{"name":"stderr","text":"2025-11-16 06:29:01.005633: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m327,680\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,680</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m327,810\u001b[0m (1.25 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">327,810</span> (1.25 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m327,810\u001b[0m (1.25 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">327,810</span> (1.25 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Starting model training...\nEpoch 1/1000\n\u001b[1m3476/3476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 7ms/step - loss: 320.7545 - mae: 11.4321 - val_loss: 35.1308 - val_mae: 4.4241\nEpoch 2/1000\n\u001b[1m3476/3476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 24.4792 - mae: 3.6497 - val_loss: 19.7515 - val_mae: 3.2890\nEpoch 3/1000\n\u001b[1m3476/3476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 16.7832 - mae: 2.9901 - val_loss: 15.2901 - val_mae: 2.8371\nEpoch 4/1000\n\u001b[1m3476/3476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 13.5158 - mae: 2.6629 - val_loss: 13.1122 - val_mae: 2.5962\nEpoch 5/1000\n\u001b[1m3476/3476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 7ms/step - loss: 11.2978 - mae: 2.4227 - val_loss: 11.4441 - val_mae: 2.4307\nEpoch 6/1000\n\u001b[1m3476/3476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 9.6872 - mae: 2.2353 - val_loss: 9.9758 - val_mae: 2.2664\nEpoch 7/1000\n\u001b[1m3476/3476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 8.4887 - mae: 2.0874 - val_loss: 8.9739 - val_mae: 2.1432\nEpoch 8/1000\n\u001b[1m3476/3476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - loss: 7.4909 - mae: 1.9579 - val_loss: 8.2673 - val_mae: 2.0465\nEpoch 9/1000\n\u001b[1m2070/3476\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 6.9006 - mae: 1.8762","output_type":"stream"}],"execution_count":null},{"id":"d656f4e3-f3d0-489f-bf46-ac450b023485","cell_type":"code","source":"  model_save_path = 'nfl_model.h5'\n    preprocessor_save_path = 'preprocessor.joblib'\n\n    model.save(model_save_path)\n    joblib.dump(preprocessor, preprocessor_save_path)\n\n    print(f\"Model saved to {model_save_path}\")\n    print(f\"Preprocessor saved to {preprocessor_save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}