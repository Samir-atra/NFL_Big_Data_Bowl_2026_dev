{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"sourceType":"competition"}],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a0b1c2d3","cell_type":"markdown","source":"### 1. Installs and Imports\n\nThis cell ensures the necessary libraries (`tensorflow`, `keras`) are up-to-date and then imports all required modules for data loading, preprocessing, model building, and training.","metadata":{}},{"id":"1afe4cc5","cell_type":"code","source":"!pip install --upgrade tensorflow keras\n!pip install --upgrade polars\n!pip install --upgrade keras-tuner\n\nimport os\nimport pandas as pd\nimport tensorflow as tf\nimport jax.numpy as jnp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport kaggle_evaluation.nfl_inference_server\nimport joblib\nimport scipy\nimport keras\nimport keras_tuner\nimport sys\n\n# os.environ['KAGGLE_IS_COMPETITION_RERUN'] = '1'\n","metadata":{"execution":{"iopub.status.busy":"2025-11-20T16:10:21.141749Z","iopub.execute_input":"2025-11-20T16:10:21.141996Z","iopub.status.idle":"2025-11-20T16:11:00.255829Z","shell.execute_reply.started":"2025-11-20T16:10:21.141976Z","shell.execute_reply":"2025-11-20T16:11:00.254907Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/site-packages (2.20.0)\nRequirement already satisfied: keras in /usr/local/lib/python3.12/site-packages (3.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.3.1)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/site-packages (from tensorflow) (25.9.23)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/site-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/site-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from tensorflow) (25.0)\nRequirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (6.33.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.32.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from tensorflow) (80.9.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (3.2.0)\nRequirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/site-packages (from tensorflow) (4.15.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.0.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/site-packages (from tensorflow) (1.76.0)\nRequirement already satisfied: tensorboard~=2.20.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.20.0)\nRequirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.3.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (3.15.1)\nRequirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/site-packages (from tensorflow) (0.5.3)\nRequirement already satisfied: rich in /usr/local/lib/python3.12/site-packages (from keras) (14.2.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.12/site-packages (from keras) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.12/site-packages (from keras) (0.17.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich->keras) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich->keras) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting polars\n  Downloading polars-1.35.2-py3-none-any.whl.metadata (10 kB)\nCollecting polars-runtime-32==1.35.2 (from polars)\n  Downloading polars_runtime_32-1.35.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\nDownloading polars-1.35.2-py3-none-any.whl (783 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m783.6/783.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading polars_runtime_32-1.35.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.3/41.3 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: polars-runtime-32, polars\nSuccessfully installed polars-1.35.2 polars-runtime-32-1.35.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting keras-tuner\n  Downloading keras_tuner-1.4.8-py3-none-any.whl.metadata (5.6 kB)\nRequirement already satisfied: keras in /usr/local/lib/python3.12/site-packages (from keras-tuner) (3.12.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from keras-tuner) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/site-packages (from keras-tuner) (2.32.5)\nCollecting kt-legacy (from keras-tuner)\n  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\nRequirement already satisfied: grpcio in /usr/local/lib/python3.12/site-packages (from keras-tuner) (1.76.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/site-packages (from keras-tuner) (6.33.0)\nRequirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/site-packages (from grpcio->keras-tuner) (4.15.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.12/site-packages (from keras->keras-tuner) (2.3.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from keras->keras-tuner) (2.3.4)\nRequirement already satisfied: rich in /usr/local/lib/python3.12/site-packages (from keras->keras-tuner) (14.2.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.12/site-packages (from keras->keras-tuner) (0.1.0)\nRequirement already satisfied: h5py in /usr/local/lib/python3.12/site-packages (from keras->keras-tuner) (3.15.1)\nRequirement already satisfied: optree in /usr/local/lib/python3.12/site-packages (from keras->keras-tuner) (0.17.0)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/site-packages (from keras->keras-tuner) (0.5.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests->keras-tuner) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests->keras-tuner) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests->keras-tuner) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests->keras-tuner) (2025.10.5)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich->keras->keras-tuner) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich->keras->keras-tuner) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\nDownloading keras_tuner-1.4.8-py3-none-any.whl (129 kB)\nDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\nInstalling collected packages: kt-legacy, keras-tuner\nSuccessfully installed keras-tuner-1.4.8 kt-legacy-1.0.5\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"id":"e1f2g3h4","cell_type":"markdown","source":"### 2. Data Loading and Preprocessing Functions\n\nThis section contains all the functions required to load, preprocess, and structure the data for the model. \n\n- `get_feature_label_specs`: A utility to inspect the shape of features and labels in a `tf.data.Dataset`.\n- `create_preprocessor`: Defines the feature transformation pipeline using `ColumnTransformer`. It handles scaling for numerical features, one-hot encoding for categorical features, and conversion for boolean features.\n- `height_to_inches`: A helper function to convert height from a string format to inches.\n- `_create_sequences_for_group`: This function takes data for a single player in a single play and transforms it into sequences of a fixed length (`SEQUENCE_LENGTH`), which is the required input format for an LSTM model.\n- `load_and_prepare_data`: The main data pipeline function. It reads the raw CSVs, merges them, performs feature engineering (e.g., calculating age), fits and applies the preprocessor, creates sequences using `_create_sequences_for_group`, and finally splits the data into training and validation sets, returning them as `tf.data.Dataset` objects.","metadata":{}},{"id":"i5j6k7l8","cell_type":"code","source":"def get_feature_label_specs(dataset):\n    \"\"\"\n    Gets the feature and label specifications from a TensorFlow Dataset.\n\n    Args:\n        dataset (tf.data.Dataset): The TensorFlow Dataset.\n\n    Returns:\n        tuple: A tuple containing the feature and label specifications.\n               (feature_spec, label_spec)\n    \"\"\"\n    element_spec = dataset.element_spec\n    return element_spec[0], element_spec[1]\n\ndef create_preprocessor(features_df: pd.DataFrame):\n    \"\"\"\n    Creates a preprocessor for the NFL Big Data Bowl 2026 prediction data.\n\n    Args:\n        features_df (pd.DataFrame): The dataframe with the features.\n\n    Returns:\n        ColumnTransformer: The preprocessor.\n    \"\"\"\n    # Identify column types from the dataframe\n    numerical_features = features_df.select_dtypes(include=np.number).columns.tolist()\n    categorical_features = features_df.select_dtypes(exclude=np.number).columns.tolist()\n\n    # Ensure boolean 'player_to_predict' is handled separately if it's not numeric\n    boolean_features = []\n    if 'player_to_predict' in categorical_features:\n        categorical_features.remove('player_to_predict')\n        boolean_features.append('player_to_predict')\n    elif 'player_to_predict' in numerical_features:\n        numerical_features.remove('player_to_predict')\n        boolean_features.append('player_to_predict')\n        \n    # Remove identifiers that should not be model features\n    ids_to_remove = ['game_id', 'play_id', 'frame_id']\n    for col in ids_to_remove:\n        if col in numerical_features:\n            numerical_features.remove(col)\n        if col in categorical_features:\n            categorical_features.remove(col)\n\n    boolean_features = ['player_to_predict']\n\n    numerical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('scaler', StandardScaler())\n    ])\n    categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n    boolean_transformer = FunctionTransformer(lambda x: x.fillna(0).astype(int))\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numerical_transformer, numerical_features),\n            ('cat', categorical_transformer, categorical_features),\n            ('bool', boolean_transformer, boolean_features)\n        ],\n        remainder='drop'\n    )\n\n    return preprocessor\n\ndef height_to_inches(height_str):\n    \"\"\"\n    Converts height string 'feet-inches' to inches.\n    \"\"\"\n    if isinstance(height_str, str):\n        feet, inches = map(int, height_str.split('-'))\n        return feet * 12 + inches\n    return np.nan\n\nSEQUENCE_LENGTH = 10\ndef _create_sequences_for_group(group_df: pd.DataFrame, sequence_length):\n    \"\"\"\n    Creates sequences of features and corresponding labels for a single player/play group.\n    \"\"\"\n    # The group is already sorted by frame_id from the previous step\n    num_frames = len(group_df)\n    if num_frames < sequence_length + 1:\n        return np.array([]), np.array([])\n\n    # Extract features and labels as numpy arrays\n    feature_cols = [col for col in group_df.columns if col not in ['game_id', 'play_id', 'nfl_id', 'frame_id', 'x_label', 'y_label']]\n    features_array = group_df[feature_cols].values\n    labels_array = group_df[['x_label', 'y_label']].values\n\n    sequences = []\n    labels = []\n\n    # Number of complete sequences that can be formed\n    num_sequences = num_frames - sequence_length\n\n    for i in range(num_sequences):\n        sequences.append(features_array[i : i + sequence_length])\n        labels.append(labels_array[i + sequence_length])\n\n    return np.array(sequences), np.array(labels)\n\ndef load_and_prepare_data(data_dir, test_size=0.2, random_state=42):\n    \"\"\"\n    Loads input and output data from CSV files in the specified directory,\n    merges them, preprocesses the features, splits them into training and \n    validation sets, and returns them as TensorFlow Datasets.\n    The data is prepared into sequences of SEQUENCE_LENGTH frames.\n\n    Args:\n        data_dir (str): The path to the directory containing the training data.\n        test_size (float): The proportion of the dataset to allocate to the validation set.\n        random_state (int): The seed for the random number generator used for the split.\n\n    Returns:\n        tuple: A tuple containing the training and validation TensorFlow Datasets,\n               and the preprocessor.\n               (train_dataset, val_dataset, preprocessor)\n    \"\"\"\n    input_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('input')])\n    output_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('output')])\n\n    input_dfs = [pd.read_csv(f) for f in input_files]\n    output_dfs = [pd.read_csv(f) for f in output_files]\n\n    input_df = pd.concat(input_dfs, ignore_index=True)\n    output_df = pd.concat(output_dfs, ignore_index=True)\n\n    merged_df = pd.merge(input_df, output_df, on=['game_id', 'play_id', 'nfl_id', 'frame_id'], suffixes=('', '_label'))\n\n    all_sequences = []\n    all_labels = []\n\n    # Use all columns from the original input dataframe as features, except for labels\n    feature_cols_for_model = [col for col in input_df.columns]\n\n    # Create a DataFrame with only the features that will be preprocessed\n    # This is what the preprocessor will be fitted on\n    features_for_preprocessor_fitting = merged_df[feature_cols_for_model]\n\n    preprocessor = create_preprocessor(features_for_preprocessor_fitting)\n    preprocessor.fit(features_for_preprocessor_fitting) # Fit the preprocessor here\n\n    # Apply preprocessing to the entire feature set\n    # This will return a sparse matrix, convert to dense array for sequence creation\n    processed_features_array = preprocessor.transform(features_for_preprocessor_fitting)\n    \n    # Create a DataFrame from the processed features to easily merge back with identifiers\n    processed_features_df = pd.DataFrame(processed_features_array, index=merged_df.index)\n    \n    # Add back identifiers needed for grouping and labels\n    processed_df = pd.concat([merged_df[['game_id', 'play_id', 'nfl_id', 'frame_id', 'x_label', 'y_label']], processed_features_df], axis=1)\n\n    # Sort by frame_id within each group to ensure correct sequence order\n    processed_df = processed_df.sort_values(by=['game_id', 'play_id', 'nfl_id', 'frame_id']).reset_index(drop=True)\n\n    all_sequences = []\n    all_labels = []\n\n    # Group by game, play, and player to create sequences\n    for (game_id, play_id, nfl_id), group_df in processed_df.groupby(['game_id', 'play_id', 'nfl_id']):\n        sequences, labels = _create_sequences_for_group(group_df, SEQUENCE_LENGTH)\n        # print(f\"Group: {game_id}, {play_id}, {nfl_id} - Sequences length: {len(sequences)}, Labels length: {len(labels)}\")\n        if sequences.size > 0 and labels.size > 0:\n            all_sequences.append(sequences)\n            all_labels.append(labels)\n\n    if not all_sequences:\n        raise ValueError(\"No sequences could be created. Please check data and SEQUENCE_LENGTH.\")\n\n    X = np.concatenate(all_sequences, axis=0)\n    y = np.concatenate(all_labels, axis=0)\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n\n    return train_dataset, val_dataset, preprocessor","metadata":{"execution":{"iopub.status.busy":"2025-11-20T16:11:05.225825Z","iopub.execute_input":"2025-11-20T16:11:05.226308Z","iopub.status.idle":"2025-11-20T16:11:05.239771Z","shell.execute_reply.started":"2025-11-20T16:11:05.226287Z","shell.execute_reply":"2025-11-20T16:11:05.239036Z"},"trusted":true},"outputs":[],"execution_count":2},{"id":"cf0f3c41-eb01-4eda-8c65-a56df953bf32","cell_type":"markdown","source":"## Keras tuner","metadata":{}},{"id":"49f348fe-0bde-4de8-a5e3-e80d759a4ff3","cell_type":"code","source":"\n\ndef build_model(hp):\n    \"\"\"\n    Builds a compiled Keras LSTM model with hyperparameters to be experimented on.\n\n    This function defines the architecture of the LSTM model, including multiple\n    LSTM layers and a final dense layer with softmax activation. It incorporates\n    hyperparameter search spaces for key model parameters like learning rate,\n    number of LSTM units, kernel regularization, and activation functions.\n\n    Args:\n        hp (keras_tuner.HyperParameters): An instance of Keras Tuner's HyperParameters class,\n                                          used to define the search space for hyperparameters.\n\n    Returns:\n        keras.Model: The compiled Keras LSTM model with hyperparameters set by Keras Tuner.\n    \"\"\"\n    \n    SEQUENCE_LENGTH = 10\n    \n    # Define hyperparameter search spaces for tuning\n    learning_rate = hp.Float(\"lr\", min_value=1e-6, max_value=1e-3, sampling=\"log\")        # Learning rate search space\n    layer_u = hp.Int(\"lu\", min_value=20, max_value=100, step=4)                          # Number of units in LSTM layers search space\n    kernel_r = hp.Float(\"kr\", min_value=1e-10, max_value=1e-5, sampling=\"log\")            # Kernel regularization (L2) search space limits\n    acti_f = hp.Choice(\"af\", [\"selu\", \"tanh\", \"relu\", \"leaky_relu\"])                     # Activation function choices search space\n    weight_d = hp.Float(\"wd\", min_value=1e-10, max_value=0.0009, sampling=\"log\")          # Weight decay search space limits\n\n    # Define the model structure using Keras Sequential API\n    model = keras.Sequential(\n        [   keras.layers.Input(shape=(SEQUENCE_LENGTH, input_features)),\n            \n            keras.layers.LSTM(\n                units=34,\n                activation=\"selu\", # Fixed activation for the first layer\n                return_sequences=True,\n                kernel_regularizer=keras.regularizers.L2(l2=0.00000195),\n                seed=42,\n            ),\n            keras.layers.LSTM(\n                units=26,\n                activation=acti_f, # Tunable activation function\n                return_sequences=True,\n                kernel_regularizer=keras.regularizers.L2(l2=kernel_r), # Tunable kernel regularization\n                seed=42,\n            ),\n            keras.layers.LSTM(\n                units=layer_u, # Tunable number of units\n                activation=acti_f,\n                return_sequences=True,\n                kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n                seed=42,\n            ),\n            keras.layers.LSTM(\n                units=layer_u,\n                activation=acti_f,\n                return_sequences=True,\n                kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n                seed=42,\n            ),\n            keras.layers.LSTM(\n                units=layer_u,\n                activation=acti_f,\n                return_sequences=True,\n                kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n                seed=42,\n            ),\n            keras.layers.LSTM(\n                units=layer_u,\n                activation=acti_f,\n                return_sequences=True,\n                kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n                seed=42,\n            ),\n            keras.layers.LSTM(\n                units=30,\n                activation=acti_f,\n                return_sequences=False, # Last LSTM layer does not return sequences\n                kernel_regularizer=keras.regularizers.L2(l2=0.00000195),\n                seed=42,\n            ),\n            keras.layers.Dense(units=2, activation=\"softmax\"), # Output layer for 3 classes\n        ]\n    )\n\n    # Compile the model with a tunable optimizer and metrics\n    model.compile(\n        loss=keras.losses.MeanSquaredError(), # Categorical cross-entropy for one-hot encoded labels\n        optimizer=keras.optimizers.Adam(\n            learning_rate=learning_rate, # Tunable learning rate\n            global_clipnorm=1, # Global norm clipping for gradients\n            amsgrad=True,\n            weight_decay=weight_d, # Tunable weight decay\n        ),\n        metrics=[tf.keras.metrics.MeanAbsoluteError()],\n    )\n\n    return model\n\n\ndef experimenting(training_dataset, validation_data):\n    \"\"\"\n    Runs Keras Tuner experiments for the LSTM model using the RandomSearch algorithm.\n\n    This function initializes a `RandomSearch` tuner with the `build_model` function,\n    configures the search objective (minimizing validation loss), and then executes\n    the hyperparameter search across the defined search spaces. It prints summaries\n    of the search space and the results.\n\n    Args:\n        x_train (np.array): The blendshapes (features) of the training set.\n        y_train (np.array): The one-hot encoded labels of the training set.\n        x_val (np.array): The blendshapes (features) of the validation set.\n        y_val (np.array): The one-hot encoded labels of the validation set.\n\n    \"\"\"\n\n    hp = keras_tuner.HyperParameters()\n    feature_spec, label_spec = training_dataset.element_spec\n    global input_features\n    input_features = feature_spec.shape[1]\n    BATCH_SIZE = 32\n    build_model(hp) # Instantiate a dummy model to build the search space\n\n    # Initialize Keras Tuner's RandomSearch algorithm\n    tuner = keras_tuner.RandomSearch(\n        hypermodel=build_model,\n        max_trials=3, # Maximum number of hyperparameter combinations to try\n        objective=keras_tuner.Objective(\"val_loss\", \"min\"),   # Objective is to minimize validation loss\n        executions_per_trial=1, # Number of models to train for each trial (1 for efficiency)\n        overwrite=True, # Overwrite previous results in the directory\n        directory=os.getenv(\"KERAS_TUNER_EXPERIMENTS_DIR\"), # Directory to save experiment logs and checkpoints\n        project_name=\"Emotion_estimation_tuning\", # Name of the Keras Tuner project\n    )\n\n    tuner.search_space_summary() # Print a summary of the hyperparameter search space\n\n\n    training_dataset = training_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    validation_data = validation_data.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n    # Run the hyperparameter search experiments\n    tuner.search(\n        training_dataset, validation_data = validation_data, epochs=10, batch_size=32\n    )\n\n    \n    tuner.results_summary() # Print a summary of the best performing trials\n\n\nif __name__ == \"__main__\":\n    prediction_data_dir = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n\n    train_ds, val_ds, preprocessor = load_and_prepare_data(prediction_data_dir)\n\n    # Run the hyperparameter experimentation\n    experimenting(train_ds, val_ds)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T16:35:51.162582Z","iopub.execute_input":"2025-11-20T16:35:51.162905Z"}},"outputs":[{"name":"stdout","text":"this is input features 2209\nthis is input features 2209\nthis is input features 2209\nthis is input features 2209\nSearch space summary\nDefault search space size: 5\nlr (Float)\n{'default': 1e-06, 'conditions': [], 'min_value': 1e-06, 'max_value': 0.001, 'step': None, 'sampling': 'log'}\nlu (Int)\n{'default': None, 'conditions': [], 'min_value': 20, 'max_value': 100, 'step': 4, 'sampling': 'linear'}\nkr (Float)\n{'default': 1e-10, 'conditions': [], 'min_value': 1e-10, 'max_value': 1e-05, 'step': None, 'sampling': 'log'}\naf (Choice)\n{'default': 'selu', 'conditions': [], 'values': ['selu', 'tanh', 'relu', 'leaky_relu'], 'ordered': False}\nwd (Float)\n{'default': 1e-10, 'conditions': [], 'min_value': 1e-10, 'max_value': 0.0009, 'step': None, 'sampling': 'log'}\n\nSearch: Running Trial #1\n\nValue             |Best Value So Far |Hyperparameter\n0.00047828        |0.00047828        |lr\n100               |100               |lu\n3.0246e-09        |3.0246e-09        |kr\ntanh              |tanh              |af\n5.2304e-09        |5.2304e-09        |wd\n\nthis is input features 2209\nthis is input features 2209\nEpoch 1/10\n\u001b[1m3476/3476\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 29ms/step - loss: 2571.3076 - mean_absolute_error: 42.6230 - val_loss: 2571.7764 - val_mean_absolute_error: 42.7422\nEpoch 2/10\n\u001b[1m3476/3476\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 29ms/step - loss: 2571.1560 - mean_absolute_error: 42.6229 - val_loss: 2571.7510 - val_mean_absolute_error: 42.7422\nEpoch 3/10\n\u001b[1m  11/3476\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:34\u001b[0m 27ms/step - loss: 2275.3305 - mean_absolute_error: 39.9407","output_type":"stream"}],"execution_count":null},{"id":"m9n0o1p2","cell_type":"markdown","source":"### 3. Model Definition and Training Functions\n\nThis section defines the model architecture and the training loop.\n\n- `build_model`: Creates a simple Keras Sequential model with an LSTM layer followed by a Dense output layer. It's compiled with the Adam optimizer and Mean Squared Error (MSE) loss, suitable for this regression task.\n- `train_model`: A wrapper function that handles the training process. It shuffles and batches the datasets for efficiency and then calls `model.fit` to train the model.","metadata":{}},{"id":"3429fbe7","cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\n# from data_loader import load_and_prepare_data, SEQUENCE_LENGTH\n\ndef build_model(input_features, output_shape, lstm_units=64):\n    \"\"\"\n    Builds a sequential model with two LSTM layers.\n\n    Args:\n        input_features (int): The number of input features per timestep.\n        output_shape (int): The number of output units.\n        lstm_units (int): The number of units in the LSTM layers.\n\n    Returns:\n        keras.Model: The compiled Keras model.\n    \"\"\"\n    model = keras.Sequential([\n        layers.Input(shape=(SEQUENCE_LENGTH, input_features)),  # Input shape for a sequence of timesteps\n        layers.LSTM(lstm_units),\n        layers.Dense(output_shape)\n    ])\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n                  loss=tf.keras.losses.MeanSquaredError(),\n                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n    return model\n\ndef train_model(model, train_dataset, val_dataset, epochs, batch_size):\n    \"\"\"\n    Trains the Keras model.\n    \"\"\"\n    train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    if val_dataset:\n        val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    print(\"Starting model training...\")\n    history = model.fit(train_dataset,\n                        epochs=epochs,\n                        validation_data=val_dataset)\n    print(\"Model training finished.\")\n    return history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"q3r4s5t6","cell_type":"markdown","source":"### 4. Main Training Execution\n\nThis is the main execution block of the notebook. It sets hyperparameters like the data directory, batch size, and number of epochs. It then calls the functions defined above to:\n1. Load and prepare the data.\n2. Determine the input and output shapes for the model from the dataset.\n3. Build the model.\n4. Train the model.","metadata":{}},{"id":"d2455736","cell_type":"code","source":"# def main():\n\"\"\"\nMain function to load data, build, and train the model.\n\"\"\"\nprediction_data_dir = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n\nbatch_size = 32\nepochs = 3\n\ntrain_ds, val_ds, preprocessor = load_and_prepare_data(prediction_data_dir)\n\nif train_ds.cardinality().numpy() == 0:\n    print(\"No training data generated. Please check data loading and feature engineering.\")\n    # return\n\n# Detect and initialize hardware strategy\ntpu_resolver = None\ntry:\n    tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('TPU found with resolver: ', tpu_resolver.master())\nexcept ValueError:\n    print(\"Could not initialize TPU resolver. Falling back to other checks.\")\n\nif tpu_resolver:\n    tf.config.experimental_connect_to_cluster(tpu_resolver)\n    tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n    strategy = tf.distribute.TPUStrategy(tpu_resolver)\n    print(\"Running on TPU\")\nelse:\n    # If no TPU is found, check for GPUs\n    gpus = tf.config.list_physical_devices('GPU')\n    if len(gpus) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print(f'Running on {len(gpus)} GPU(s).')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Running on CPU.')\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n# Get the input and output shapes from the dataset specs\nfeature_spec, label_spec = train_ds.element_spec\ninput_features = feature_spec.shape[1] # Now shape is (SEQUENCE_LENGTH, input_features)\noutput_shape = label_spec.shape[0]\n\n# Build and compile the model within the strategy scope to run on TPU\nwith strategy.scope():\n    model = build_model(input_features, output_shape)\n\nmodel.summary()\n\ntrain_model(model, train_ds, val_ds, epochs, batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cache_explanation","cell_type":"markdown","source":"### 4.5 Save Preprocessed Data Cache (OPTIONAL)\\n\n\\n\n**Run this cell ONCE after the first data processing to save the cache.**\\n\nThis saves the preprocessed `X_train`, `X_val`, `y_train`, `y_val` arrays to disk.\\n\n\\n\n**Benefits:**\\n\n- âš¡ Loads in 5-10 seconds instead of 3-5 minutes\\n\n- ðŸ’¾ Cache size: ~1-1.5 GB\\n\n- ðŸ”„ Reusable across notebook sessions\\n\n\\n\n**Instructions:**\\n\n1. Run the training cell above (cell 4) to get `train_ds`, `val_ds`, `preprocessor`\\n\n2. Run this cell to save the cache\\n\n3. Download the `/kaggle/working/training_cache/` folder from Kaggle\\n\n4. Upload it as a Kaggle dataset\\n\n5. Use the loading cell below in future runs","metadata":{}},{"id":"save_training_cache","cell_type":"code","source":"# ============================================================\\n\n# SAVE TRAINING DATA CACHE\\n\n# Run this cell ONCE to generate the cache files\\n\n# ============================================================\\n\n\nimport os\nimport numpy as np\nimport joblib\nfrom datetime import datetime\n\n# Configuration\nCACHE_DIR = '/kaggle/working/training_cache'  # Kaggle working directory\\n\nos.makedirs(CACHE_DIR, exist_ok=True)\n\nprint(\"Converting TensorFlow datasets to numpy arrays...\")\n\n# Convert train dataset to arrays\\n\nX_train_list = []\ny_train_list = []\nfor features, labels in train_ds:\n    X_train_list.append(features.numpy())\n    y_train_list.append(labels.numpy())\n\n# Convert validation dataset to arrays\\n\nX_val_list = []\ny_val_list = []\nfor features, labels in val_ds:\n    X_val_list.append(features.numpy())\n    y_val_list.append(labels.numpy())\n\n# Stack into single arrays\\n\nX_train = np.vstack(X_train_list)\ny_train = np.vstack(y_train_list)\nX_val = np.vstack(X_val_list)\ny_val = np.vstack(y_val_list)\n\nprint(f\"âœ… Conversion complete!\")\nprint(f\"   X_train shape: {X_train.shape}\")\nprint(f\"   X_val shape: {X_val.shape}\")\nprint(f\"   y_train shape: {y_train.shape}\")\nprint(f\"   y_val shape: {y_val.shape}\")\n\n# Save arrays as Parquet files\nprint(f\"\\nðŸ’¾ Saving cache to {CACHE_DIR} using Parquet...\")\npd.DataFrame(X_train).to_parquet(f'{CACHE_DIR}/X_train.parquet')\npd.DataFrame(X_val).to_parquet(f'{CACHE_DIR}/X_val.parquet')\npd.DataFrame(y_train).to_parquet(f'{CACHE_DIR}/y_train.parquet')\npd.DataFrame(y_val).to_parquet(f'{CACHE_DIR}/y_val.parquet')\njoblib.dump(preprocessor, f'{CACHE_DIR}/preprocessor.joblib')\n\n# Save metadata\\n\nmetadata = {\n    'created': datetime.now().isoformat(),\n    'X_train_shape': X_train.shape,\n    'X_val_shape': X_val.shape,\n    'y_train_shape': y_train.shape,\n    'y_val_shape': y_val.shape,\n    'sequence_length': SEQUENCE_LENGTH\n}\n\nwith open(f'{CACHE_DIR}/metadata.txt', 'w') as f:\n    for key, value in metadata.items():\n        f.write(f\"{key}: {value}\\n\")\n\n# Calculate total size\n    total_size = sum(\n        os.path.getsize(f'{CACHE_DIR}/{fname}') \n        for fname in ['X_train.parquet', 'X_val.parquet', 'y_train.parquet', 'y_val.parquet', 'preprocessor.joblib']\n)\n\nprint(f\"âœ… Cache saved successfully!\")\nprint(f\"   Location: {CACHE_DIR}\")\nprint(f\"   Total size: {total_size / 1024 / 1024:.2f} MB\")\nprint(f\"ðŸ“¥ NEXT STEPS:\")\nprint(f\"   1. Download the '{CACHE_DIR}' folder from Kaggle\")\nprint(f\"   2. Upload it as a new Kaggle dataset\")\nprint(f\"   3. Add that dataset to your notebook\")\nprint(f\"   4. Use the loading cell below to load from cache\")\n\n# Clean up to free memory\ndel X_train_list, y_train_list, X_val_list, y_val_list\ndel X_train, X_val, y_train, y_val","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"load_cache_explanation","cell_type":"markdown","source":"### 4.6 Load Preprocessed Data from Cache (OPTIONAL)\\n\n\\n\n**Use this cell instead of cell 4 to load from cache.**\\n\n\\n\n**Prerequisites:**\\n\n1. You've run the save cache cell above at least once\\n\n2. You've uploaded the cache folder as a Kaggle dataset\\n\n3. You've added that dataset to this notebook\\n\n\\n\n**Update the CACHE_INPUT_PATH below to point to your cache dataset!**","metadata":{}},{"id":"load_training_cache","cell_type":"code","source":"# ============================================================\\n\n# LOAD TRAINING DATA FROM CACHE\\n\n# Use this cell INSTEAD of the training execution cell (cell 4)\\n\n# when you have cached data available\\n\n# ============================================================\\n\n\nimport os\nimport numpy as np\nimport joblib\nimport tensorflow as tf\n\n# IMPORTANT: Update this path to your cache dataset!\\n\n# After uploading cache as a dataset, it will be at:\\n\n# /kaggle/input/your-cache-dataset-name/training_cache/\\n\nCACHE_INPUT_PATH = '/kaggle/input/nfl-training-cache/training_cache'\n\n# Check if cache exists\\n\nif not os.path.exists(CACHE_INPUT_PATH):\n    print(f\"âŒ Cache not found at {CACHE_INPUT_PATH}\")\n    print(f\"   Please update CACHE_INPUT_PATH or run the data processing cell instead.\")\n    raise FileNotFoundError(f\"Cache directory not found: {CACHE_INPUT_PATH}\")\n\n    print(f\"ðŸ“‚ Loading preprocessed data from cache (Parquet)...\")\n    print(f\"   Location: {CACHE_INPUT_PATH}\")\n\n# Load Parquet files and convert to numpy arrays\nX_train = pd.read_parquet(f'{CACHE_INPUT_PATH}/X_train.parquet').to_numpy()\nX_val = pd.read_parquet(f'{CACHE_INPUT_PATH}/X_val.parquet').to_numpy()\ny_train = pd.read_parquet(f'{CACHE_INPUT_PATH}/y_train.parquet').to_numpy()\ny_val = pd.read_parquet(f'{CACHE_INPUT_PATH}/y_val.parquet').to_numpy()\npreprocessor = joblib.load(f'{CACHE_INPUT_PATH}/preprocessor.joblib')\n\nprint(f\" Cache loaded successfully!\")\nprint(f\"   X_train shape: {X_train.shape}\")\nprint(f\"   X_val shape: {X_val.shape}\")\nprint(f\"   y_val shape: {y_val.shape}\")\n\n# Create TensorFlow datasets from cached arrays\\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\nval_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n\nprint(f\" TensorFlow datasets created!\")\nprint(f\"   Training samples: {len(X_train):,}\")\nprint(f\"   Validation samples: {len(X_val):,}\")\nprint(f\" Ready for model training! Continue to the model building cell.\")\n\n# Continue with rest of training (hardware detection, model building, etc.)\\n\nbatch_size = 32\nepochs = 3\n\nif train_ds.cardinality().numpy() == 0:\n    print(\"No training data generated. Please check data loading.\")\n\n# Detect and initialize hardware strategy\\n\ntpu_resolver = None\ntry:\n    tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('TPU found with resolver: ', tpu_resolver.master())\nexcept ValueError:\n    print(\"Could not initialize TPU resolver. Falling back to other checks.\")\n\nif tpu_resolver:\n    tf.config.experimental_connect_to_cluster(tpu_resolver)\n    tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n    strategy = tf.distribute.TPUStrategy(tpu_resolver)\n    print(\"Running on TPU\")\nelse:\n    gpus = tf.config.list_physical_devices('GPU')\n    if len(gpus) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print(f'Running on {len(gpus)} GPU(s).')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Running on CPU.')\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n# Get the input and output shapes from the dataset specs\\n\nfeature_spec, label_spec = train_ds.element_spec\ninput_features = feature_spec.shape[1]\noutput_shape = label_spec.shape[0]\n\n# Build and compile the model within the strategy scope\\n\nwith strategy.scope():\n    model = build_model(input_features, output_shape)\n\nmodel.summary()\n\ntrain_model(model, train_ds, val_ds, epochs, batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"u7v8w9x0","cell_type":"markdown","source":"### 5. Save Artifacts\n\nAfter training is complete, this cell saves the two essential artifacts for inference: the trained Keras model (`nfl_model.h5`) and the fitted `preprocessor` object (`preprocessor.joblib`). These files are required by the `submission.ipynb` notebook to make predictions.","metadata":{}},{"id":"d656f4e3-f3d0-489f-bf46-ac450b023485","cell_type":"code","source":"model_save_path = 'nfl_model.h5'\npreprocessor_save_path = 'preprocessor.joblib'\n\nmodel.save(model_save_path)\njoblib.dump(preprocessor, preprocessor_save_path)\n\nprint(f\"Model saved to {model_save_path}\")\nprint(f\"Preprocessor saved to {preprocessor_save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"y1z2a3b4","cell_type":"markdown","source":"### 6. Inference Functions (for submission)\nThis section contains the functions that will be used in the `submission.ipynb` notebook. They are included here for completeness and to ensure the entire pipeline is defined in one place before being split for submission.\n\n- `load_artifacts`: Loads the saved model and preprocessor.\n- `preprocess_features`: Replicates the feature engineering and sequence creation for the test data.\n- `predict`: The main prediction function that ties preprocessing and model inference together.","metadata":{}},{"id":"5e77717a-a133-41fc-a94f-c06cae579250","cell_type":"code","source":"MODEL_PATH = 'nfl_model.h5'\nPREPROCESSOR_PATH = 'preprocessor.joblib'\n\ndef load_artifacts():\n    \"\"\"\n    Loads the trained Keras model and the preprocessor from disk.\n    Raises FileNotFoundError if either artifact is missing.\n    \"\"\"\n    if not os.path.exists(MODEL_PATH):\n        raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}. Please train the model first by running predictor.py.\")\n    if not os.path.exists(PREPROCESSOR_PATH):\n        raise FileNotFoundError(f\"Preprocessor file not found at {PREPROCESSOR_PATH}. Please train the model first.\")\n\n    print(f\"Loading model from {MODEL_PATH}\")\n    model = tf.keras.models.load_model(MODEL_PATH)\n    \n    print(f\"Loading preprocessor from {PREPROCESSOR_PATH}\")\n    preprocessor = joblib.load(PREPROCESSOR_PATH)\n    \n    return model, preprocessor","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0899e0e5-0a35-43b9-98b5-6e643c88e63f","cell_type":"code","source":"# Load the model globally to avoid reloading it for each batch.\n# model, preprocessor = load_artifacts()\n\ndef preprocess_features(test_df, test_input_df):\n    \"\"\"\n    Preprocesses the raw input dataframes into a format the model expects.\n    This function replicates the feature engineering and sequence creation from\n    the training pipeline (`data_loader.py`).\n    \n    Args:\n        test_df (pd.DataFrame): The dataframe with the rows to predict.\n        test_input_df (pd.DataFrame): The dataframe with the input features for the play.\n\n    Returns:\n        np.array: A 3D array of shape (num_predictions, SEQUENCE_LENGTH, num_features)\n                  ready to be fed into the LSTM model.\n    \"\"\"\n    num_predictions = len(test_df)\n    if num_predictions == 0:\n        return np.array([])\n\n    # Combine input data for the entire play. `test_input_df` contains frame 0 (the context),\n    # and `test_df` contains the frames we need to predict for.\n    play_df = pd.concat([test_input_df, test_df], ignore_index=True)\n    play_df = play_df.sort_values(by=['nfl_id', 'frame_id']).reset_index(drop=True)\n\n    # 1. Recreate the exact same features as in training\n    play_df['height_inches'] = play_df['player_height'].apply(height_to_inches)\n    game_date_str = play_df['game_id'].astype(str).str[:8]\n    game_date = pd.to_datetime(game_date_str, format='%Y%m%d')\n    player_birth_date = pd.to_datetime(play_df['player_birth_date'])\n    play_df['age'] = (game_date - player_birth_date).dt.days / 365.25\n\n    # 2. Apply the pre-fitted preprocessor\n    feature_cols = preprocessor.feature_names_in_\n    processed_features_array = preprocessor.transform(play_df[feature_cols])\n    processed_features_df = pd.DataFrame(processed_features_array, index=play_df.index)\n\n    # 3. Create sequences for each row in the original `test_df` (each row to predict)\n    processed_df_with_ids = pd.concat([play_df[['nfl_id', 'frame_id']], processed_features_df], axis=1)\n    sequences = []\n    for _, row_to_predict in test_df.iterrows():\n        player_id = row_to_predict['nfl_id']\n        frame_id = row_to_predict['frame_id']\n        \n        # Find the player's data and the exact frame we need to predict\n        player_data_with_ids = processed_df_with_ids[processed_df_with_ids['nfl_id'] == player_id]\n        prediction_frame_index = player_data_with_ids[player_data_with_ids['frame_id'] == frame_id].index[0]\n        \n        # The sequence consists of the `SEQUENCE_LENGTH` frames *before* the prediction frame\n        start_idx = prediction_frame_index - SEQUENCE_LENGTH\n        end_idx = prediction_frame_index\n        \n        if start_idx < 0:\n            # If we don't have enough history, pad with the first frame or zeros\n            # Here we slice from 0 to end_idx\n            sequence = processed_features_df.iloc[0:end_idx].values\n            # Pad with zeros at the beginning\n            pad_width = SEQUENCE_LENGTH - len(sequence)\n            if pad_width > 0:\n                # Pad with the first available frame (repetition) or zeros. \n                # Using zeros is safer if we assume missing history means 'nothing happened'\n                # But repetition might be better for continuity. Let's use zero padding for now as it's standard.\n                padding = np.zeros((pad_width, sequence.shape[1]))\n                sequence = np.vstack([padding, sequence])\n        else:\n            # Slice the sequence from the purely numerical dataframe\n            sequence = processed_features_df.iloc[start_idx:end_idx].values\n        \n        sequences.append(sequence)\n\n    return np.array(sequences)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0a1990c4-9d97-49d2-9001-289e641a755c","cell_type":"code","source":"def predict(test_df, test_input_df):\n    \"\"\"\n    Generates predictions for a single batch (play).\n    \"\"\"\n    # The gateway provides polars dataframes, convert them to pandas\n    test_df = test_df.to_pandas()\n    test_input_df = test_input_df.to_pandas()\n\n    # 1. Preprocess the data to create features for the model\n    features = preprocess_features(test_df, test_input_df)\n\n    if features.shape[0] == 0:\n        return pd.DataFrame([], columns=['x', 'y'])\n    \n    if scipy.sparse.issparse(features):\n        features = features.toarray()\n    # 2. Run inference\n    # Calling the model directly is often faster for inference than model.predict()\n    predictions_xy = model(features, training=False).numpy()\n\n    if scipy.sparse.issparse(predictions_xy):\n        preds = predictions_xy.toarray()\n\n    # 3. Format the predictions into the required DataFrame\n    return pd.DataFrame(predictions_xy, columns=['x', 'y'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c5d6e7f8","cell_type":"markdown","source":"### 7. Run Inference Server\nThis final cell sets up the Kaggle evaluation environment. It initializes the `NFLInferenceServer` with our `predict` function. The server will then either run in a live competition environment or use a local gateway for testing, depending on the environment variables.","metadata":{}},{"id":"5b2dded0-0c59-4bd2-9f1d-dc3d594ef267","cell_type":"code","source":"import kaggle_evaluation.nfl_inference_server\n\ninference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    output = inference_server.serve()\n    print(\"this is output\", output)\nelse:\n    output = inference_server.run_local_gateway(('/kaggle/input/nfl-big-data-bowl-2026-prediction/',))\n    print(\"this is output local\", output)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}