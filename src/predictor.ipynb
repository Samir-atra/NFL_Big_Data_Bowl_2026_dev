{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"sourceType":"competition"}],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a0b1c2d3","cell_type":"markdown","source":"### 1. Installs and Imports\n\nThis cell ensures the necessary libraries (`tensorflow`, `keras`) are up-to-date and then imports all required modules for data loading, preprocessing, model building, and training.","metadata":{}},{"id":"1afe4cc5","cell_type":"code","source":"!pip install --upgrade tensorflow keras\n!pip install --upgrade polars\n\nimport os\nimport pandas as pd\nimport tensorflow as tf\nimport jax.numpy as jnp\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport kaggle_evaluation.nfl_inference_server\nimport joblib\nimport scipy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:27:32.299007Z","iopub.execute_input":"2025-11-19T14:27:32.299264Z","iopub.status.idle":"2025-11-19T14:27:35.302585Z","shell.execute_reply.started":"2025-11-19T14:27:32.299248Z","shell.execute_reply":"2025-11-19T14:27:35.301659Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/site-packages (2.20.0)\nRequirement already satisfied: keras in /usr/local/lib/python3.12/site-packages (3.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.3.1)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/site-packages (from tensorflow) (25.9.23)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/site-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/site-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from tensorflow) (25.0)\nRequirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (6.33.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.32.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from tensorflow) (80.9.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (3.2.0)\nRequirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/site-packages (from tensorflow) (4.15.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.0.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/site-packages (from tensorflow) (1.76.0)\nRequirement already satisfied: tensorboard~=2.20.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.20.0)\nRequirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.3.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (3.15.1)\nRequirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/site-packages (from tensorflow) (0.5.3)\nRequirement already satisfied: rich in /usr/local/lib/python3.12/site-packages (from keras) (14.2.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.12/site-packages (from keras) (0.1.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.12/site-packages (from keras) (0.17.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich->keras) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich->keras) (2.19.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: polars in /usr/local/lib/python3.12/site-packages (1.35.2)\nRequirement already satisfied: polars-runtime-32==1.35.2 in /usr/local/lib/python3.12/site-packages (from polars) (1.35.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":11},{"id":"e1f2g3h4","cell_type":"markdown","source":"### 2. Data Loading and Preprocessing Functions\n\nThis section contains all the functions required to load, preprocess, and structure the data for the model. \n\n- `get_feature_label_specs`: A utility to inspect the shape of features and labels in a `tf.data.Dataset`.\n- `create_preprocessor`: Defines the feature transformation pipeline using `ColumnTransformer`. It handles scaling for numerical features, one-hot encoding for categorical features, and conversion for boolean features.\n- `height_to_inches`: A helper function to convert height from a string format to inches.\n- `_create_sequences_for_group`: This function takes data for a single player in a single play and transforms it into sequences of a fixed length (`SEQUENCE_LENGTH`), which is the required input format for an LSTM model.\n- `load_and_prepare_data`: The main data pipeline function. It reads the raw CSVs, merges them, performs feature engineering (e.g., calculating age), fits and applies the preprocessor, creates sequences using `_create_sequences_for_group`, and finally splits the data into training and validation sets, returning them as `tf.data.Dataset` objects.","metadata":{}},{"id":"i5j6k7l8","cell_type":"code","source":"def get_feature_label_specs(dataset):\n    \"\"\"\n    Gets the feature and label specifications from a TensorFlow Dataset.\n\n    Args:\n        dataset (tf.data.Dataset): The TensorFlow Dataset.\n\n    Returns:\n        tuple: A tuple containing the feature and label specifications.\n               (feature_spec, label_spec)\n    \"\"\"\n    element_spec = dataset.element_spec\n    return element_spec[0], element_spec[1]\n\ndef create_preprocessor(features_df: pd.DataFrame):\n    \"\"\"\n    Creates a preprocessor for the NFL Big Data Bowl 2026 prediction data.\n\n    Args:\n        features_df (pd.DataFrame): The dataframe with the features.\n\n    Returns:\n        ColumnTransformer: The preprocessor.\n    \"\"\"\n    # Identify column types from the dataframe\n    numerical_features = features_df.select_dtypes(include=np.number).columns.tolist()\n    categorical_features = features_df.select_dtypes(exclude=np.number).columns.tolist()\n\n    # Ensure boolean 'player_to_predict' is handled separately if it's not numeric\n    boolean_features = []\n    if 'player_to_predict' in categorical_features:\n        categorical_features.remove('player_to_predict')\n        boolean_features.append('player_to_predict')\n    elif 'player_to_predict' in numerical_features:\n        numerical_features.remove('player_to_predict')\n        boolean_features.append('player_to_predict')\n        \n    # Remove identifiers that should not be model features\n    ids_to_remove = ['game_id', 'play_id', 'frame_id']\n    for col in ids_to_remove:\n        if col in numerical_features:\n            numerical_features.remove(col)\n        if col in categorical_features:\n            categorical_features.remove(col)\n\n    boolean_features = ['player_to_predict']\n\n    numerical_transformer = StandardScaler()\n    categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n    boolean_transformer = FunctionTransformer(lambda x: x.fillna(0).astype(int))\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numerical_transformer, numerical_features),\n            ('cat', categorical_transformer, categorical_features),\n            ('bool', boolean_transformer, boolean_features)\n        ],\n        remainder='drop'\n    )\n\n    return preprocessor\n\ndef height_to_inches(height_str):\n    \"\"\"\n    Converts height string 'feet-inches' to inches.\n    \"\"\"\n    if isinstance(height_str, str):\n        feet, inches = map(int, height_str.split('-'))\n        return feet * 12 + inches\n    return np.nan\n\nSEQUENCE_LENGTH = 10\ndef _create_sequences_for_group(group_df: pd.DataFrame, sequence_length):\n    \"\"\"\n    Creates sequences of features and corresponding labels for a single player/play group.\n    \"\"\"\n    # The group is already sorted by frame_id from the previous step\n    num_frames = len(group_df)\n    if num_frames < sequence_length + 1:\n        return np.array([]), np.array([])\n\n    # Extract features and labels as numpy arrays\n    feature_cols = [col for col in group_df.columns if col not in ['game_id', 'play_id', 'nfl_id', 'frame_id', 'x_label', 'y_label']]\n    features_array = group_df[feature_cols].values\n    labels_array = group_df[['x_label', 'y_label']].values\n\n    sequences = []\n    labels = []\n\n    # Number of complete sequences that can be formed\n    num_sequences = num_frames - sequence_length\n\n    for i in range(num_sequences):\n        sequences.append(features_array[i : i + sequence_length])\n        labels.append(labels_array[i + sequence_length])\n\n    return np.array(sequences), np.array(labels)\n\ndef load_and_prepare_data(data_dir, test_size=0.2, random_state=42):\n    \"\"\"\n    Loads input and output data from CSV files in the specified directory,\n    merges them, preprocesses the features, splits them into training and \n    validation sets, and returns them as TensorFlow Datasets.\n    The data is prepared into sequences of SEQUENCE_LENGTH frames.\n\n    Args:\n        data_dir (str): The path to the directory containing the training data.\n        test_size (float): The proportion of the dataset to allocate to the validation set.\n        random_state (int): The seed for the random number generator used for the split.\n\n    Returns:\n        tuple: A tuple containing the training and validation TensorFlow Datasets,\n               and the preprocessor.\n               (train_dataset, val_dataset, preprocessor)\n    \"\"\"\n    input_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('input')])\n    output_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('output')])\n\n    input_dfs = [pd.read_csv(f) for f in input_files]\n    output_dfs = [pd.read_csv(f) for f in output_files]\n\n    input_df = pd.concat(input_dfs, ignore_index=True)\n    output_df = pd.concat(output_dfs, ignore_index=True)\n\n    merged_df = pd.merge(input_df, output_df, on=['game_id', 'play_id', 'nfl_id', 'frame_id'], suffixes=('', '_label'))\n\n    all_sequences = []\n    all_labels = []\n\n    # Use all columns from the original input dataframe as features, except for labels\n    feature_cols_for_model = [col for col in input_df.columns]\n\n    # Create a DataFrame with only the features that will be preprocessed\n    # This is what the preprocessor will be fitted on\n    features_for_preprocessor_fitting = merged_df[feature_cols_for_model]\n\n    preprocessor = create_preprocessor(features_for_preprocessor_fitting)\n    preprocessor.fit(features_for_preprocessor_fitting) # Fit the preprocessor here\n\n    # Apply preprocessing to the entire feature set\n    # This will return a sparse matrix, convert to dense array for sequence creation\n    processed_features_array = preprocessor.transform(features_for_preprocessor_fitting)\n    \n    # Create a DataFrame from the processed features to easily merge back with identifiers\n    processed_features_df = pd.DataFrame(processed_features_array, index=merged_df.index)\n    \n    # Add back identifiers needed for grouping and labels\n    processed_df = pd.concat([merged_df[['game_id', 'play_id', 'nfl_id', 'frame_id', 'x_label', 'y_label']], processed_features_df], axis=1)\n\n    # Sort by frame_id within each group to ensure correct sequence order\n    processed_df = processed_df.sort_values(by=['game_id', 'play_id', 'nfl_id', 'frame_id']).reset_index(drop=True)\n\n    all_sequences = []\n    all_labels = []\n\n    # Group by game, play, and player to create sequences\n    for (game_id, play_id, nfl_id), group_df in processed_df.groupby(['game_id', 'play_id', 'nfl_id']):\n        sequences, labels = _create_sequences_for_group(group_df, SEQUENCE_LENGTH)\n        # print(f\"Group: {game_id}, {play_id}, {nfl_id} - Sequences length: {len(sequences)}, Labels length: {len(labels)}\")\n        if sequences.size > 0 and labels.size > 0:\n            all_sequences.append(sequences)\n            all_labels.append(labels)\n\n    if not all_sequences:\n        raise ValueError(\"No sequences could be created. Please check data and SEQUENCE_LENGTH.\")\n\n    X = np.concatenate(all_sequences, axis=0)\n    y = np.concatenate(all_labels, axis=0)\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state)\n\n    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n\n    return train_dataset, val_dataset, preprocessor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:23:35.455097Z","iopub.execute_input":"2025-11-19T14:23:35.455367Z","iopub.status.idle":"2025-11-19T14:23:35.467490Z","shell.execute_reply.started":"2025-11-19T14:23:35.455350Z","shell.execute_reply":"2025-11-19T14:23:35.466625Z"}},"outputs":[],"execution_count":6},{"id":"m9n0o1p2","cell_type":"markdown","source":"### 3. Model Definition and Training Functions\n\nThis section defines the model architecture and the training loop.\n\n- `build_model`: Creates a simple Keras Sequential model with an LSTM layer followed by a Dense output layer. It's compiled with the Adam optimizer and Mean Squared Error (MSE) loss, suitable for this regression task.\n- `train_model`: A wrapper function that handles the training process. It shuffles and batches the datasets for efficiency and then calls `model.fit` to train the model.","metadata":{}},{"id":"3429fbe7","cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\n# from data_loader import load_and_prepare_data, SEQUENCE_LENGTH\n\ndef build_model(input_features, output_shape, lstm_units=64):\n    \"\"\"\n    Builds a sequential model with two LSTM layers.\n\n    Args:\n        input_features (int): The number of input features per timestep.\n        output_shape (int): The number of output units.\n        lstm_units (int): The number of units in the LSTM layers.\n\n    Returns:\n        keras.Model: The compiled Keras model.\n    \"\"\"\n    model = keras.Sequential([\n        layers.Input(shape=(SEQUENCE_LENGTH, input_features)),  # Input shape for a sequence of timesteps\n        layers.LSTM(lstm_units),\n        layers.Dense(output_shape)\n    ])\n\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n                  loss=tf.keras.losses.MeanSquaredError(),\n                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n    return model\n\ndef train_model(model, train_dataset, val_dataset, epochs, batch_size):\n    \"\"\"\n    Trains the Keras model.\n    \"\"\"\n    train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    if val_dataset:\n        val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    print(\"Starting model training...\")\n    history = model.fit(train_dataset,\n                        epochs=epochs,\n                        validation_data=val_dataset)\n    print(\"Model training finished.\")\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:21:43.584840Z","iopub.execute_input":"2025-11-19T14:21:43.585132Z","iopub.status.idle":"2025-11-19T14:21:43.590519Z","shell.execute_reply.started":"2025-11-19T14:21:43.585112Z","shell.execute_reply":"2025-11-19T14:21:43.589771Z"}},"outputs":[],"execution_count":4},{"id":"q3r4s5t6","cell_type":"markdown","source":"### 4. Main Training Execution\n\nThis is the main execution block of the notebook. It sets hyperparameters like the data directory, batch size, and number of epochs. It then calls the functions defined above to:\n1. Load and prepare the data.\n2. Determine the input and output shapes for the model from the dataset.\n3. Build the model.\n4. Train the model.","metadata":{}},{"id":"d2455736","cell_type":"code","source":"# def main():\n\"\"\"\nMain function to load data, build, and train the model.\n\"\"\"\nprediction_data_dir = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n\nbatch_size = 32\nepochs = 3\n\ntrain_ds, val_ds, preprocessor = load_and_prepare_data(prediction_data_dir)\n\nif train_ds.cardinality().numpy() == 0:\n    print(\"No training data generated. Please check data loading and feature engineering.\")\n    # return\n\n# Detect and initialize hardware strategy\ntpu_resolver = None\ntry:\n    tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('TPU found with resolver: ', tpu_resolver.master())\nexcept ValueError:\n    print(\"Could not initialize TPU resolver. Falling back to other checks.\")\n\nif tpu_resolver:\n    tf.config.experimental_connect_to_cluster(tpu_resolver)\n    tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n    strategy = tf.distribute.TPUStrategy(tpu_resolver)\n    print(\"Running on TPU\")\nelse:\n    # If no TPU is found, check for GPUs\n    gpus = tf.config.list_physical_devices('GPU')\n    if len(gpus) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print(f'Running on {len(gpus)} GPU(s).')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Running on CPU.')\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n# Get the input and output shapes from the dataset specs\nfeature_spec, label_spec = train_ds.element_spec\ninput_features = feature_spec.shape[1] # Now shape is (SEQUENCE_LENGTH, input_features)\noutput_shape = label_spec.shape[0]\n\n# Build and compile the model within the strategy scope to run on TPU\nwith strategy.scope():\n    model = build_model(input_features, output_shape)\n\nmodel.summary()\n\ntrain_model(model, train_ds, val_ds, epochs, batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:23:42.893722Z","iopub.execute_input":"2025-11-19T14:23:42.893982Z","iopub.status.idle":"2025-11-19T14:26:52.101232Z","shell.execute_reply.started":"2025-11-19T14:23:42.893964Z","shell.execute_reply":"2025-11-19T14:26:52.100161Z"}},"outputs":[{"name":"stderr","text":"2025-11-19 14:24:53.450758: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"Could not initialize TPU resolver. Falling back to other checks.\nRunning on CPU.\nREPLICAS:  1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m582,144\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">582,144</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m582,274\u001b[0m (2.22 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">582,274</span> (2.22 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m582,274\u001b[0m (2.22 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">582,274</span> (2.22 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Starting model training...\nEpoch 1/3\n\u001b[1m3476/3476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - loss: 308.4175 - mean_absolute_error: 11.1129 - val_loss: 34.2967 - val_mean_absolute_error: 4.3716\nEpoch 2/3\n\u001b[1m3476/3476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 23.3029 - mean_absolute_error: 3.5646 - val_loss: 18.6037 - val_mean_absolute_error: 3.1417\nEpoch 3/3\n\u001b[1m3476/3476\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 15.7945 - mean_absolute_error: 2.8973 - val_loss: 14.8946 - val_mean_absolute_error: 2.8172\nModel training finished.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x78afc77fe750>"},"metadata":{}}],"execution_count":7},{"id":"u7v8w9x0","cell_type":"markdown","source":"### 5. Save Artifacts\n\nAfter training is complete, this cell saves the two essential artifacts for inference: the trained Keras model (`nfl_model.h5`) and the fitted `preprocessor` object (`preprocessor.joblib`). These files are required by the `submission.ipynb` notebook to make predictions.","metadata":{}},{"id":"d656f4e3-f3d0-489f-bf46-ac450b023485","cell_type":"code","source":"model_save_path = 'nfl_model.h5'\npreprocessor_save_path = 'preprocessor.joblib'\n\nmodel.save(model_save_path)\njoblib.dump(preprocessor, preprocessor_save_path)\n\nprint(f\"Model saved to {model_save_path}\")\nprint(f\"Preprocessor saved to {preprocessor_save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:03:18.803739Z","iopub.execute_input":"2025-11-16T12:03:18.803975Z","iopub.status.idle":"2025-11-16T12:03:18.837130Z","shell.execute_reply.started":"2025-11-16T12:03:18.803956Z","shell.execute_reply":"2025-11-16T12:03:18.836124Z"}},"outputs":[{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"Model saved to nfl_model.h5\nPreprocessor saved to preprocessor.joblib\n","output_type":"stream"}],"execution_count":5},{"id":"y1z2a3b4","cell_type":"markdown","source":"### 6. Inference Functions (for submission)\nThis section contains the functions that will be used in the `submission.ipynb` notebook. They are included here for completeness and to ensure the entire pipeline is defined in one place before being split for submission.\n\n- `load_artifacts`: Loads the saved model and preprocessor.\n- `preprocess_features`: Replicates the feature engineering and sequence creation for the test data.\n- `predict`: The main prediction function that ties preprocessing and model inference together.","metadata":{}},{"id":"5e77717a-a133-41fc-a94f-c06cae579250","cell_type":"code","source":"MODEL_PATH = 'nfl_model.h5'\nPREPROCESSOR_PATH = 'preprocessor.joblib'\n\ndef load_artifacts():\n    \"\"\"\n    Loads the trained Keras model and the preprocessor from disk.\n    Raises FileNotFoundError if either artifact is missing.\n    \"\"\"\n    if not os.path.exists(MODEL_PATH):\n        raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}. Please train the model first by running predictor.py.\")\n    if not os.path.exists(PREPROCESSOR_PATH):\n        raise FileNotFoundError(f\"Preprocessor file not found at {PREPROCESSOR_PATH}. Please train the model first.\")\n\n    print(f\"Loading model from {MODEL_PATH}\")\n    model = tf.keras.models.load_model(MODEL_PATH)\n    \n    print(f\"Loading preprocessor from {PREPROCESSOR_PATH}\")\n    preprocessor = joblib.load(PREPROCESSOR_PATH)\n    \n    return model, preprocessor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:03:18.837819Z","iopub.execute_input":"2025-11-16T12:03:18.838004Z","iopub.status.idle":"2025-11-16T12:03:18.842025Z","shell.execute_reply.started":"2025-11-16T12:03:18.837987Z","shell.execute_reply":"2025-11-16T12:03:18.841170Z"}},"outputs":[],"execution_count":6},{"id":"0899e0e5-0a35-43b9-98b5-6e643c88e63f","cell_type":"code","source":"# Load the model globally to avoid reloading it for each batch.\n# model, preprocessor = load_artifacts()\n\ndef preprocess_features(test_df, test_input_df):\n    \"\"\"\n    Preprocesses the raw input dataframes into a format the model expects.\n    This function replicates the feature engineering and sequence creation from\n    the training pipeline (`data_loader.py`).\n    \n    Args:\n        test_df (pd.DataFrame): The dataframe with the rows to predict.\n        test_input_df (pd.DataFrame): The dataframe with the input features for the play.\n\n    Returns:\n        np.array: A 3D array of shape (num_predictions, SEQUENCE_LENGTH, num_features)\n                  ready to be fed into the LSTM model.\n    \"\"\"\n    num_predictions = len(test_df)\n    if num_predictions == 0:\n        return np.array([])\n\n    # Combine input data for the entire play. `test_input_df` contains frame 0 (the context),\n    # and `test_df` contains the frames we need to predict for.\n    play_df = pd.concat([test_input_df, test_df], ignore_index=True)\n    play_df = play_df.sort_values(by=['nfl_id', 'frame_id']).reset_index(drop=True)\n\n    # 1. Recreate the exact same features as in training\n    play_df['height_inches'] = play_df['player_height'].apply(height_to_inches)\n    game_date_str = play_df['game_id'].astype(str).str[:8]\n    game_date = pd.to_datetime(game_date_str, format='%Y%m%d')\n    player_birth_date = pd.to_datetime(play_df['player_birth_date'])\n    play_df['age'] = (game_date - player_birth_date).dt.days / 365.25\n\n    # 2. Apply the pre-fitted preprocessor\n    feature_cols = preprocessor.feature_names_in_\n    processed_features_array = preprocessor.transform(play_df[feature_cols])\n    processed_features_df = pd.DataFrame(processed_features_array, index=play_df.index)\n\n    # 3. Create sequences for each row in the original `test_df` (each row to predict)\n    processed_df_with_ids = pd.concat([play_df[['nfl_id', 'frame_id']], processed_features_df], axis=1)\n    sequences = []\n    for _, row_to_predict in test_df.iterrows():\n        player_id = row_to_predict['nfl_id']\n        frame_id = row_to_predict['frame_id']\n        \n        # Find the player's data and the exact frame we need to predict\n        player_data_with_ids = processed_df_with_ids[processed_df_with_ids['nfl_id'] == player_id]\n        prediction_frame_index = player_data_with_ids[player_data_with_ids['frame_id'] == frame_id].index[0]\n        \n        # The sequence consists of the `SEQUENCE_LENGTH` frames *before* the prediction frame\n        start_idx = prediction_frame_index - SEQUENCE_LENGTH\n        end_idx = prediction_frame_index\n        \n        # Slice the sequence from the purely numerical dataframe\n        sequence = processed_features_df.iloc[start_idx:end_idx].values\n        sequences.append(sequence)\n\n    return np.array(sequences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:26:58.081782Z","iopub.execute_input":"2025-11-19T14:26:58.082094Z","iopub.status.idle":"2025-11-19T14:26:58.088237Z","shell.execute_reply.started":"2025-11-19T14:26:58.082061Z","shell.execute_reply":"2025-11-19T14:26:58.087387Z"}},"outputs":[],"execution_count":8},{"id":"0a1990c4-9d97-49d2-9001-289e641a755c","cell_type":"code","source":"def predict(test_df, test_input_df):\n    \"\"\"\n    Generates predictions for a single batch (play).\n    \"\"\"\n    # The gateway provides polars dataframes, convert them to pandas\n    test_df = test_df.to_pandas()\n    test_input_df = test_input_df.to_pandas()\n\n    # 1. Preprocess the data to create features for the model\n    features = preprocess_features(test_df, test_input_df)\n\n    if features.shape[0] == 0:\n        return pd.DataFrame([], columns=['x', 'y'])\n    \n    if scipy.sparse.issparse(features):\n        features = features.toarray()\n    # 2. Run inference\n    # Calling the model directly is often faster for inference than model.predict()\n    predictions_xy = model(features, training=False).numpy()\n\n    if scipy.sparse.issparse(predictions_xy):\n        preds = predictions_xy.toarray()\n\n    # 3. Format the predictions into the required DataFrame\n    return pd.DataFrame(predictions_xy, columns=['x', 'y'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:27:01.768982Z","iopub.execute_input":"2025-11-19T14:27:01.769322Z","iopub.status.idle":"2025-11-19T14:27:01.773749Z","shell.execute_reply.started":"2025-11-19T14:27:01.769304Z","shell.execute_reply":"2025-11-19T14:27:01.772910Z"}},"outputs":[],"execution_count":9},{"id":"c5d6e7f8","cell_type":"markdown","source":"### 7. Run Inference Server\nThis final cell sets up the Kaggle evaluation environment. It initializes the `NFLInferenceServer` with our `predict` function. The server will then either run in a live competition environment or use a local gateway for testing, depending on the environment variables.","metadata":{}},{"id":"5b2dded0-0c59-4bd2-9f1d-dc3d594ef267","cell_type":"code","source":"import kaggle_evaluation.nfl_inference_server\n\ninference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(('/kaggle/input/nfl-big-data-bowl-2026-prediction/',))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:27:47.254873Z","iopub.execute_input":"2025-11-19T14:27:47.255196Z","iopub.status.idle":"2025-11-19T14:27:47.782585Z","shell.execute_reply.started":"2025-11-19T14:27:47.255174Z","shell.execute_reply":"2025-11-19T14:27:47.781640Z"}},"outputs":[{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mGatewayRuntimeError\u001b[39m                       Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     inference_server.serve()\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[43minference_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_local_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/kaggle/input/nfl-big-data-bowl-2026-prediction/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[36mFile \u001b[39m\u001b[32m/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/core/templates.py:108\u001b[39m, in \u001b[36mInferenceServer.run_local_gateway\u001b[39m\u001b[34m(self, data_paths, file_share_dir, *args, **kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mself\u001b[39m.gateway.run()\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m.server.stop(\u001b[32m0\u001b[39m)\n","\u001b[36mFile \u001b[39m\u001b[32m/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/core/templates.py:106\u001b[39m, in \u001b[36mInferenceServer.run_local_gateway\u001b[39m\u001b[34m(self, data_paths, file_share_dir, *args, **kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    105\u001b[39m     \u001b[38;5;28mself\u001b[39m.gateway = \u001b[38;5;28mself\u001b[39m._get_gateway_for_test(data_paths, file_share_dir, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[36mFile \u001b[39m\u001b[32m/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/core/base_gateway.py:151\u001b[39m, in \u001b[36mBaseGateway.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m.write_result(error)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error:\n\u001b[32m    150\u001b[39m     \u001b[38;5;66;03m# For local testing\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n","\u001b[36mFile \u001b[39m\u001b[32m/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/core/base_gateway.py:132\u001b[39m, in \u001b[36mBaseGateway.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    131\u001b[39m     \u001b[38;5;28mself\u001b[39m.unpack_data_paths()\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     predictions, row_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_all_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28mself\u001b[39m.write_submission(predictions, row_ids)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GatewayRuntimeError \u001b[38;5;28;01mas\u001b[39;00m gre:\n","\u001b[36mFile \u001b[39m\u001b[32m/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/core/base_gateway.py:110\u001b[39m, in \u001b[36mBaseGateway.get_all_predictions\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m predictions = \u001b[38;5;28mself\u001b[39m.predict(*data_batch)\n\u001b[32m    109\u001b[39m \u001b[38;5;28mself\u001b[39m.competition_agnostic_validation(predictions, row_ids)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompetition_specific_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m all_predictions.append(predictions)\n\u001b[32m    112\u001b[39m all_row_ids.append(row_ids)\n","\u001b[36mFile \u001b[39m\u001b[32m/kaggle/input/nfl-big-data-bowl-2026-prediction/kaggle_evaluation/nfl_gateway.py:81\u001b[39m, in \u001b[36mNFLGateway.competition_specific_validation\u001b[39m\u001b[34m(self, prediction, row_ids, data_batch)\u001b[39m\n\u001b[32m     79\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m GatewayRuntimeError(GatewayRuntimeErrorType.INVALID_SUBMISSION, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m must be numeric.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pred[col].isna().any():\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m GatewayRuntimeError(GatewayRuntimeErrorType.INVALID_SUBMISSION, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m contains NaNs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(pred[expected].values).all():\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GatewayRuntimeError(GatewayRuntimeErrorType.INVALID_SUBMISSION, \u001b[33m'\u001b[39m\u001b[33mPrediction contains non-finite values.\u001b[39m\u001b[33m'\u001b[39m)\n","\u001b[31mGatewayRuntimeError\u001b[39m: (<GatewayRuntimeErrorType.INVALID_SUBMISSION: 6>, \"Column 'x' contains NaNs.\")"],"ename":"GatewayRuntimeError","evalue":"(<GatewayRuntimeErrorType.INVALID_SUBMISSION: 6>, \"Column 'x' contains NaNs.\")","output_type":"error"}],"execution_count":12}]}