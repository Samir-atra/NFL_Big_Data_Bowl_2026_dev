{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b1c2d3",
   "metadata": {},
   "source": [
    "### 1. Installs and Imports\n",
    "\n",
    "This cell ensures the necessary libraries (`tensorflow`, `keras`) are up-to-date and then imports all required modules for data loading, preprocessing, model building, and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1afe4cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:51:15.069812Z",
     "iopub.status.busy": "2025-11-20T05:51:15.069586Z",
     "iopub.status.idle": "2025-11-20T05:51:57.104249Z",
     "shell.execute_reply": "2025-11-20T05:51:57.103115Z",
     "shell.execute_reply.started": "2025-11-20T05:51:15.069794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/site-packages (2.20.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.12/site-packages (3.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /usr/local/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (6.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (2.3.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/site-packages (from keras) (14.2.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/site-packages (from keras) (0.17.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/site-packages (from rich->keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/site-packages (from rich->keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting polars\n",
      "  Downloading polars-1.35.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting polars-runtime-32==1.35.2 (from polars)\n",
      "  Downloading polars_runtime_32-1.35.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Downloading polars-1.35.2-py3-none-any.whl (783 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m783.6/783.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading polars_runtime_32-1.35.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.3/41.3 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: polars-runtime-32, polars\n",
      "Successfully installed polars-1.35.2 polars-runtime-32-1.35.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow keras\n",
    "!pip install --upgrade polars\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import jax.numpy as jnp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import kaggle_evaluation.nfl_inference_server\n",
    "import joblib\n",
    "import scipy\n",
    "\n",
    "# os.environ['KAGGLE_IS_COMPETITION_RERUN'] = '1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2g3h4",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Preprocessing Functions\n",
    "\n",
    "This section contains all the functions required to load, preprocess, and structure the data for the model. \n",
    "\n",
    "- `get_feature_label_specs`: A utility to inspect the shape of features and labels in a `tf.data.Dataset`.\n",
    "- `create_preprocessor`: Defines the feature transformation pipeline using `ColumnTransformer`. It handles scaling for numerical features, one-hot encoding for categorical features, and conversion for boolean features.\n",
    "- `height_to_inches`: A helper function to convert height from a string format to inches.\n",
    "- `_create_sequences_for_group`: This function takes data for a single player in a single play and transforms it into sequences of a fixed length (`SEQUENCE_LENGTH`), which is the required input format for an LSTM model.\n",
    "- `load_and_prepare_data`: The main data pipeline function. It reads the raw CSVs, merges them, performs feature engineering (e.g., calculating age), fits and applies the preprocessor, creates sequences using `_create_sequences_for_group`, and finally splits the data into training and validation sets, returning them as `tf.data.Dataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "i5j6k7l8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:52:02.342291Z",
     "iopub.status.busy": "2025-11-20T05:52:02.341732Z",
     "iopub.status.idle": "2025-11-20T05:52:02.355052Z",
     "shell.execute_reply": "2025-11-20T05:52:02.354214Z",
     "shell.execute_reply.started": "2025-11-20T05:52:02.342269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_feature_label_specs(dataset):\n",
    "    \"\"\"\n",
    "    Gets the feature and label specifications from a TensorFlow Dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (tf.data.Dataset): The TensorFlow Dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the feature and label specifications.\n",
    "               (feature_spec, label_spec)\n",
    "    \"\"\"\n",
    "    element_spec = dataset.element_spec\n",
    "    return element_spec[0], element_spec[1]\n",
    "\n",
    "def create_preprocessor(features_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Creates a preprocessor for the NFL Big Data Bowl 2026 prediction data.\n",
    "\n",
    "    Args:\n",
    "        features_df (pd.DataFrame): The dataframe with the features.\n",
    "\n",
    "    Returns:\n",
    "        ColumnTransformer: The preprocessor.\n",
    "    \"\"\"\n",
    "    # Identify column types from the dataframe\n",
    "    numerical_features = features_df.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_features = features_df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "    # Ensure boolean 'player_to_predict' is handled separately if it's not numeric\n",
    "    boolean_features = []\n",
    "    if 'player_to_predict' in categorical_features:\n",
    "        categorical_features.remove('player_to_predict')\n",
    "        boolean_features.append('player_to_predict')\n",
    "    elif 'player_to_predict' in numerical_features:\n",
    "        numerical_features.remove('player_to_predict')\n",
    "        boolean_features.append('player_to_predict')\n",
    "        \n",
    "    # Remove identifiers that should not be model features\n",
    "    ids_to_remove = ['game_id', 'play_id', 'frame_id']\n",
    "    for col in ids_to_remove:\n",
    "        if col in numerical_features:\n",
    "            numerical_features.remove(col)\n",
    "        if col in categorical_features:\n",
    "            categorical_features.remove(col)\n",
    "\n",
    "    boolean_features = ['player_to_predict']\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    boolean_transformer = FunctionTransformer(lambda x: x.fillna(0).astype(int))\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('bool', boolean_transformer, boolean_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "def height_to_inches(height_str):\n",
    "    \"\"\"\n",
    "    Converts height string 'feet-inches' to inches.\n",
    "    \"\"\"\n",
    "    if isinstance(height_str, str):\n",
    "        feet, inches = map(int, height_str.split('-'))\n",
    "        return feet * 12 + inches\n",
    "    return np.nan\n",
    "\n",
    "SEQUENCE_LENGTH = 10\n",
    "def _create_sequences_for_group(group_df: pd.DataFrame, sequence_length):\n",
    "    \"\"\"\n",
    "    Creates sequences of features and corresponding labels for a single player/play group.\n",
    "    \"\"\"\n",
    "    # The group is already sorted by frame_id from the previous step\n",
    "    num_frames = len(group_df)\n",
    "    if num_frames < sequence_length + 1:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # Extract features and labels as numpy arrays\n",
    "    feature_cols = [col for col in group_df.columns if col not in ['game_id', 'play_id', 'nfl_id', 'frame_id', 'x_label', 'y_label']]\n",
    "    features_array = group_df[feature_cols].values\n",
    "    labels_array = group_df[['x_label', 'y_label']].values\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # Number of complete sequences that can be formed\n",
    "    num_sequences = num_frames - sequence_length\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        sequences.append(features_array[i : i + sequence_length])\n",
    "        labels.append(labels_array[i + sequence_length])\n",
    "\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "def load_and_prepare_data(data_dir, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Loads input and output data from CSV files in the specified directory,\n",
    "    merges them, preprocesses the features, splits them into training and \n",
    "    validation sets, and returns them as TensorFlow Datasets.\n",
    "    The data is prepared into sequences of SEQUENCE_LENGTH frames.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The path to the directory containing the training data.\n",
    "        test_size (float): The proportion of the dataset to allocate to the validation set.\n",
    "        random_state (int): The seed for the random number generator used for the split.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training and validation TensorFlow Datasets,\n",
    "               and the preprocessor.\n",
    "               (train_dataset, val_dataset, preprocessor)\n",
    "    \"\"\"\n",
    "    input_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('input')])\n",
    "    output_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('output')])\n",
    "\n",
    "    input_dfs = [pd.read_csv(f) for f in input_files]\n",
    "    output_dfs = [pd.read_csv(f) for f in output_files]\n",
    "\n",
    "    input_df = pd.concat(input_dfs, ignore_index=True)\n",
    "    output_df = pd.concat(output_dfs, ignore_index=True)\n",
    "\n",
    "    merged_df = pd.merge(input_df, output_df, on=['game_id', 'play_id', 'nfl_id', 'frame_id'], suffixes=('', '_label'))\n",
    "\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Use all columns from the original input dataframe as features, except for labels\n",
    "    feature_cols_for_model = [col for col in input_df.columns]\n",
    "\n",
    "    # Create a DataFrame with only the features that will be preprocessed\n",
    "    # This is what the preprocessor will be fitted on\n",
    "    features_for_preprocessor_fitting = merged_df[feature_cols_for_model]\n",
    "\n",
    "    preprocessor = create_preprocessor(features_for_preprocessor_fitting)\n",
    "    preprocessor.fit(features_for_preprocessor_fitting) # Fit the preprocessor here\n",
    "\n",
    "    # Apply preprocessing to the entire feature set\n",
    "    # This will return a sparse matrix, convert to dense array for sequence creation\n",
    "    processed_features_array = preprocessor.transform(features_for_preprocessor_fitting)\n",
    "    \n",
    "    # Create a DataFrame from the processed features to easily merge back with identifiers\n",
    "    processed_features_df = pd.DataFrame(processed_features_array, index=merged_df.index)\n",
    "    \n",
    "    # Add back identifiers needed for grouping and labels\n",
    "    processed_df = pd.concat([merged_df[['game_id', 'play_id', 'nfl_id', 'frame_id', 'x_label', 'y_label']], processed_features_df], axis=1)\n",
    "\n",
    "    # Sort by frame_id within each group to ensure correct sequence order\n",
    "    processed_df = processed_df.sort_values(by=['game_id', 'play_id', 'nfl_id', 'frame_id']).reset_index(drop=True)\n",
    "\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Group by game, play, and player to create sequences\n",
    "    for (game_id, play_id, nfl_id), group_df in processed_df.groupby(['game_id', 'play_id', 'nfl_id']):\n",
    "        sequences, labels = _create_sequences_for_group(group_df, SEQUENCE_LENGTH)\n",
    "        # print(f\"Group: {game_id}, {play_id}, {nfl_id} - Sequences length: {len(sequences)}, Labels length: {len(labels)}\")\n",
    "        if sequences.size > 0 and labels.size > 0:\n",
    "            all_sequences.append(sequences)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    if not all_sequences:\n",
    "        raise ValueError(\"No sequences could be created. Please check data and SEQUENCE_LENGTH.\")\n",
    "\n",
    "    X = np.concatenate(all_sequences, axis=0)\n",
    "    y = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "    return train_dataset, val_dataset, preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m9n0o1p2",
   "metadata": {},
   "source": [
    "### 3. Model Definition and Training Functions\n",
    "\n",
    "This section defines the model architecture and the training loop.\n",
    "\n",
    "- `build_model`: Creates a simple Keras Sequential model with an LSTM layer followed by a Dense output layer. It's compiled with the Adam optimizer and Mean Squared Error (MSE) loss, suitable for this regression task.\n",
    "- `train_model`: A wrapper function that handles the training process. It shuffles and batches the datasets for efficiency and then calls `model.fit` to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3429fbe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:52:08.431728Z",
     "iopub.status.busy": "2025-11-20T05:52:08.431455Z",
     "iopub.status.idle": "2025-11-20T05:52:08.436831Z",
     "shell.execute_reply": "2025-11-20T05:52:08.436138Z",
     "shell.execute_reply.started": "2025-11-20T05:52:08.431709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "# from data_loader import load_and_prepare_data, SEQUENCE_LENGTH\n",
    "\n",
    "def build_model(input_features, output_shape, lstm_units=64):\n",
    "    \"\"\"\n",
    "    Builds a sequential model with two LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        input_features (int): The number of input features per timestep.\n",
    "        output_shape (int): The number of output units.\n",
    "        lstm_units (int): The number of units in the LSTM layers.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(SEQUENCE_LENGTH, input_features)),  # Input shape for a sequence of timesteps\n",
    "        layers.LSTM(lstm_units),\n",
    "        layers.Dense(output_shape)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, epochs, batch_size):\n",
    "    \"\"\"\n",
    "    Trains the Keras model.\n",
    "    \"\"\"\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    if val_dataset:\n",
    "        val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    print(\"Starting model training...\")\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset)\n",
    "    print(\"Model training finished.\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q3r4s5t6",
   "metadata": {},
   "source": [
    "### 4. Main Training Execution\n",
    "\n",
    "This is the main execution block of the notebook. It sets hyperparameters like the data directory, batch size, and number of epochs. It then calls the functions defined above to:\n",
    "1. Load and prepare the data.\n",
    "2. Determine the input and output shapes for the model from the dataset.\n",
    "3. Build the model.\n",
    "4. Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2455736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:52:12.047197Z",
     "iopub.status.busy": "2025-11-20T05:52:12.046904Z",
     "iopub.status.idle": "2025-11-20T05:55:31.548268Z",
     "shell.execute_reply": "2025-11-20T05:55:31.547250Z",
     "shell.execute_reply.started": "2025-11-20T05:52:12.047181Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 05:53:30.895462: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not initialize TPU resolver. Falling back to other checks.\n",
      "Running on CPU.\n",
      "REPLICAS:  1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">582,144</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚       \u001b[38;5;34m582,144\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              â”‚           \u001b[38;5;34m130\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">582,274</span> (2.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m582,274\u001b[0m (2.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">582,274</span> (2.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m582,274\u001b[0m (2.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Epoch 1/3\n",
      "\u001b[1m3476/3476\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 9ms/step - loss: 325.8424 - mean_absolute_error: 11.4878 - val_loss: 35.6895 - val_mean_absolute_error: 4.4342\n",
      "Epoch 2/3\n",
      "\u001b[1m3476/3476\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 24.0757 - mean_absolute_error: 3.6178 - val_loss: 18.1090 - val_mean_absolute_error: 3.1135\n",
      "Epoch 3/3\n",
      "\u001b[1m3476/3476\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 15.8271 - mean_absolute_error: 2.8925 - val_loss: 14.3638 - val_mean_absolute_error: 2.7533\n",
      "Model training finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7bdecb6140b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def main():\n",
    "\"\"\"\n",
    "Main function to load data, build, and train the model.\n",
    "\"\"\"\n",
    "prediction_data_dir = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "train_ds, val_ds, preprocessor = load_and_prepare_data(prediction_data_dir)\n",
    "\n",
    "if train_ds.cardinality().numpy() == 0:\n",
    "    print(\"No training data generated. Please check data loading and feature engineering.\")\n",
    "    # return\n",
    "\n",
    "# Detect and initialize hardware strategy\n",
    "tpu_resolver = None\n",
    "try:\n",
    "    tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('TPU found with resolver: ', tpu_resolver.master())\n",
    "except ValueError:\n",
    "    print(\"Could not initialize TPU resolver. Falling back to other checks.\")\n",
    "\n",
    "if tpu_resolver:\n",
    "    tf.config.experimental_connect_to_cluster(tpu_resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu_resolver)\n",
    "    print(\"Running on TPU\")\n",
    "else:\n",
    "    # If no TPU is found, check for GPUs\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if len(gpus) > 0:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print(f'Running on {len(gpus)} GPU(s).')\n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        print('Running on CPU.')\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "# Get the input and output shapes from the dataset specs\n",
    "feature_spec, label_spec = train_ds.element_spec\n",
    "input_features = feature_spec.shape[1] # Now shape is (SEQUENCE_LENGTH, input_features)\n",
    "output_shape = label_spec.shape[0]\n",
    "\n",
    "# Build and compile the model within the strategy scope to run on TPU\n",
    "with strategy.scope():\n",
    "    model = build_model(input_features, output_shape)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "train_model(model, train_ds, val_ds, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cache_explanation",
   "metadata": {},
   "source": [
    "### 4.5 Save Preprocessed Data Cache (OPTIONAL)\\n\n",
    "\\n\n",
    "**Run this cell ONCE after the first data processing to save the cache.**\\n\n",
    "This saves the preprocessed `X_train`, `X_val`, `y_train`, `y_val` arrays to disk.\\n\n",
    "\\n\n",
    "**Benefits:**\\n\n",
    "- âš¡ Loads in 5-10 seconds instead of 3-5 minutes\\n\n",
    "- ğŸ’¾ Cache size: ~1-1.5 GB\\n\n",
    "- ğŸ”„ Reusable across notebook sessions\\n\n",
    "\\n\n",
    "**Instructions:**\\n\n",
    "1. Run the training cell above (cell 4) to get `train_ds`, `val_ds`, `preprocessor`\\n\n",
    "2. Run this cell to save the cache\\n\n",
    "3. Download the `/kaggle/working/training_cache/` folder from Kaggle\\n\n",
    "4. Upload it as a Kaggle dataset\\n\n",
    "5. Use the loading cell below in future runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_training_cache",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\\n\n",
    "# SAVE TRAINING DATA CACHE\\n\n",
    "# Run this cell ONCE to generate the cache files\\n\n",
    "# ============================================================\\n\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "CACHE_DIR = '/kaggle/working/training_cache'  # Kaggle working directory\\n\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Converting TensorFlow datasets to numpy arrays...\")\n",
    "\n",
    "# Convert train dataset to arrays\\n\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "for features, labels in train_ds:\n",
    "    X_train_list.append(features.numpy())\n",
    "    y_train_list.append(labels.numpy())\n",
    "\n",
    "# Convert validation dataset to arrays\\n\n",
    "X_val_list = []\n",
    "y_val_list = []\n",
    "for features, labels in val_ds:\n",
    "    X_val_list.append(features.numpy())\n",
    "    y_val_list.append(labels.numpy())\n",
    "\n",
    "# Stack into single arrays\\n\n",
    "X_train = np.vstack(X_train_list)\n",
    "y_train = np.vstack(y_train_list)\n",
    "X_val = np.vstack(X_val_list)\n",
    "y_val = np.vstack(y_val_list)\n",
    "\n",
    "print(f\"âœ… Conversion complete!\")\n",
    "print(f\"   X_train shape: {X_train.shape}\")\n",
    "print(f\"   X_val shape: {X_val.shape}\")\n",
    "print(f\"   y_train shape: {y_train.shape}\")\n",
    "print(f\"   y_val shape: {y_val.shape}\")\n",
    "\n",
    "# Save arrays\n",
    "print(f\"\\nğŸ’¾ Saving cache to {CACHE_DIR}...\")\n",
    "np.save(f'{CACHE_DIR}/X_train.npy', X_train)\n",
    "np.save(f'{CACHE_DIR}/X_val.npy', X_val)\n",
    "np.save(f'{CACHE_DIR}/y_train.npy', y_train)\n",
    "np.save(f'{CACHE_DIR}/y_val.npy', y_val)\n",
    "joblib.dump(preprocessor, f'{CACHE_DIR}/preprocessor.joblib')\n",
    "\n",
    "# Save metadata\\n\n",
    "metadata = {\n",
    "    'created': datetime.now().isoformat(),\n",
    "    'X_train_shape': X_train.shape,\n",
    "    'X_val_shape': X_val.shape,\n",
    "    'y_train_shape': y_train.shape,\n",
    "    'y_val_shape': y_val.shape,\n",
    "    'sequence_length': SEQUENCE_LENGTH\n",
    "}\n",
    "\n",
    "with open(f'{CACHE_DIR}/metadata.txt', 'w') as f:\n",
    "    for key, value in metadata.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "# Calculate total size\n",
    "total_size = sum(\n",
    "    os.path.getsize(f'{CACHE_DIR}/{fname}') \n",
    "    for fname in ['X_train.npy', 'X_val.npy', 'y_train.npy', 'y_val.npy', 'preprocessor.joblib']\n",
    ")\n",
    "\n",
    "print(f\"âœ… Cache saved successfully!\")\n",
    "print(f\"   Location: {CACHE_DIR}\")\n",
    "print(f\"   Total size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"ğŸ“¥ NEXT STEPS:\")\n",
    "print(f\"   1. Download the '{CACHE_DIR}' folder from Kaggle\")\n",
    "print(f\"   2. Upload it as a new Kaggle dataset\")\n",
    "print(f\"   3. Add that dataset to your notebook\")\n",
    "print(f\"   4. Use the loading cell below to load from cache\")\n",
    "\n",
    "# Clean up to free memory\n",
    "del X_train_list, y_train_list, X_val_list, y_val_list\n",
    "del X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_cache_explanation",
   "metadata": {},
   "source": [
    "### 4.6 Load Preprocessed Data from Cache (OPTIONAL)\\n\n",
    "\\n\n",
    "**Use this cell instead of cell 4 to load from cache.**\\n\n",
    "\\n\n",
    "**Prerequisites:**\\n\n",
    "1. You've run the save cache cell above at least once\\n\n",
    "2. You've uploaded the cache folder as a Kaggle dataset\\n\n",
    "3. You've added that dataset to this notebook\\n\n",
    "\\n\n",
    "**Update the CACHE_INPUT_PATH below to point to your cache dataset!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_training_cache",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\\n\n",
    "# LOAD TRAINING DATA FROM CACHE\\n\n",
    "# Use this cell INSTEAD of the training execution cell (cell 4)\\n\n",
    "# when you have cached data available\\n\n",
    "# ============================================================\\n\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "\n",
    "# IMPORTANT: Update this path to your cache dataset!\\n\n",
    "# After uploading cache as a dataset, it will be at:\\n\n",
    "# /kaggle/input/your-cache-dataset-name/training_cache/\\n\n",
    "CACHE_INPUT_PATH = '/kaggle/input/nfl-training-cache/training_cache'\n",
    "\n",
    "# Check if cache exists\\n\n",
    "if not os.path.exists(CACHE_INPUT_PATH):\n",
    "    print(f\"âŒ Cache not found at {CACHE_INPUT_PATH}\")\n",
    "    print(f\"   Please update CACHE_INPUT_PATH or run the data processing cell instead.\")\n",
    "    raise FileNotFoundError(f\"Cache directory not found: {CACHE_INPUT_PATH}\")\n",
    "\n",
    "print(f\"ğŸ“‚ Loading preprocessed data from cache...\")\n",
    "print(f\"   Location: {CACHE_INPUT_PATH}\")\n",
    "\n",
    "# Load numpy arrays\\n\n",
    "X_train = np.load(f'{CACHE_INPUT_PATH}/X_train.npy')\n",
    "X_val = np.load(f'{CACHE_INPUT_PATH}/X_val.npy')\n",
    "y_train = np.load(f'{CACHE_INPUT_PATH}/y_train.npy')\n",
    "y_val = np.load(f'{CACHE_INPUT_PATH}/y_val.npy')\n",
    "preprocessor = joblib.load(f'{CACHE_INPUT_PATH}/preprocessor.joblib')\n",
    "\n",
    "print(f\" Cache loaded successfully!\")\n",
    "print(f\"   X_train shape: {X_train.shape}\")\n",
    "print(f\"   X_val shape: {X_val.shape}\")\n",
    "print(f\"   y_train shape: {y_train.shape}\")\n",
    "print(f\"   y_val shape: {y_val.shape}\")\n",
    "\n",
    "# Create TensorFlow datasets from cached arrays\\n\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "print(f\" TensorFlow datasets created!\")\n",
    "print(f\"   Training samples: {len(X_train):,}\")\n",
    "print(f\"   Validation samples: {len(X_val):,}\")\n",
    "print(f\" Ready for model training! Continue to the model building cell.\")\n",
    "\n",
    "# Continue with rest of training (hardware detection, model building, etc.)\\n\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "if train_ds.cardinality().numpy() == 0:\n",
    "    print(\"No training data generated. Please check data loading.\")\n",
    "\n",
    "# Detect and initialize hardware strategy\\n\n",
    "tpu_resolver = None\n",
    "try:\n",
    "    tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('TPU found with resolver: ', tpu_resolver.master())\n",
    "except ValueError:\n",
    "    print(\"Could not initialize TPU resolver. Falling back to other checks.\")\n",
    "\n",
    "if tpu_resolver:\n",
    "    tf.config.experimental_connect_to_cluster(tpu_resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu_resolver)\n",
    "    print(\"Running on TPU\")\n",
    "else:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if len(gpus) > 0:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print(f'Running on {len(gpus)} GPU(s).')\n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        print('Running on CPU.')\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "# Get the input and output shapes from the dataset specs\\n\n",
    "feature_spec, label_spec = train_ds.element_spec\n",
    "input_features = feature_spec.shape[1]\n",
    "output_shape = label_spec.shape[0]\n",
    "\n",
    "# Build and compile the model within the strategy scope\\n\n",
    "with strategy.scope():\n",
    "    model = build_model(input_features, output_shape)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "train_model(model, train_ds, val_ds, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u7v8w9x0",
   "metadata": {},
   "source": [
    "### 5. Save Artifacts\n",
    "\n",
    "After training is complete, this cell saves the two essential artifacts for inference: the trained Keras model (`nfl_model.h5`) and the fitted `preprocessor` object (`preprocessor.joblib`). These files are required by the `submission.ipynb` notebook to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d656f4e3-f3d0-489f-bf46-ac450b023485",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T12:03:18.803975Z",
     "iopub.status.busy": "2025-11-16T12:03:18.803739Z",
     "iopub.status.idle": "2025-11-16T12:03:18.837130Z",
     "shell.execute_reply": "2025-11-16T12:03:18.836124Z",
     "shell.execute_reply.started": "2025-11-16T12:03:18.803956Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to nfl_model.h5\n",
      "Preprocessor saved to preprocessor.joblib\n"
     ]
    }
   ],
   "source": [
    "model_save_path = 'nfl_model.h5'\n",
    "preprocessor_save_path = 'preprocessor.joblib'\n",
    "\n",
    "model.save(model_save_path)\n",
    "joblib.dump(preprocessor, preprocessor_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "print(f\"Preprocessor saved to {preprocessor_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y1z2a3b4",
   "metadata": {},
   "source": [
    "### 6. Inference Functions (for submission)\n",
    "This section contains the functions that will be used in the `submission.ipynb` notebook. They are included here for completeness and to ensure the entire pipeline is defined in one place before being split for submission.\n",
    "\n",
    "- `load_artifacts`: Loads the saved model and preprocessor.\n",
    "- `preprocess_features`: Replicates the feature engineering and sequence creation for the test data.\n",
    "- `predict`: The main prediction function that ties preprocessing and model inference together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e77717a-a133-41fc-a94f-c06cae579250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-16T12:03:18.838004Z",
     "iopub.status.busy": "2025-11-16T12:03:18.837819Z",
     "iopub.status.idle": "2025-11-16T12:03:18.842025Z",
     "shell.execute_reply": "2025-11-16T12:03:18.841170Z",
     "shell.execute_reply.started": "2025-11-16T12:03:18.837987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'nfl_model.h5'\n",
    "PREPROCESSOR_PATH = 'preprocessor.joblib'\n",
    "\n",
    "def load_artifacts():\n",
    "    \"\"\"\n",
    "    Loads the trained Keras model and the preprocessor from disk.\n",
    "    Raises FileNotFoundError if either artifact is missing.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}. Please train the model first by running predictor.py.\")\n",
    "    if not os.path.exists(PREPROCESSOR_PATH):\n",
    "        raise FileNotFoundError(f\"Preprocessor file not found at {PREPROCESSOR_PATH}. Please train the model first.\")\n",
    "\n",
    "    print(f\"Loading model from {MODEL_PATH}\")\n",
    "    model = tf.keras.models.load_model(MODEL_PATH)\n",
    "    \n",
    "    print(f\"Loading preprocessor from {PREPROCESSOR_PATH}\")\n",
    "    preprocessor = joblib.load(PREPROCESSOR_PATH)\n",
    "    \n",
    "    return model, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0899e0e5-0a35-43b9-98b5-6e643c88e63f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:55:37.058803Z",
     "iopub.status.busy": "2025-11-20T05:55:37.058524Z",
     "iopub.status.idle": "2025-11-20T05:55:37.065839Z",
     "shell.execute_reply": "2025-11-20T05:55:37.064951Z",
     "shell.execute_reply.started": "2025-11-20T05:55:37.058783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the model globally to avoid reloading it for each batch.\n",
    "# model, preprocessor = load_artifacts()\n",
    "\n",
    "def preprocess_features(test_df, test_input_df):\n",
    "    \"\"\"\n",
    "    Preprocesses the raw input dataframes into a format the model expects.\n",
    "    This function replicates the feature engineering and sequence creation from\n",
    "    the training pipeline (`data_loader.py`).\n",
    "    \n",
    "    Args:\n",
    "        test_df (pd.DataFrame): The dataframe with the rows to predict.\n",
    "        test_input_df (pd.DataFrame): The dataframe with the input features for the play.\n",
    "\n",
    "    Returns:\n",
    "        np.array: A 3D array of shape (num_predictions, SEQUENCE_LENGTH, num_features)\n",
    "                  ready to be fed into the LSTM model.\n",
    "    \"\"\"\n",
    "    num_predictions = len(test_df)\n",
    "    if num_predictions == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # Combine input data for the entire play. `test_input_df` contains frame 0 (the context),\n",
    "    # and `test_df` contains the frames we need to predict for.\n",
    "    play_df = pd.concat([test_input_df, test_df], ignore_index=True)\n",
    "    play_df = play_df.sort_values(by=['nfl_id', 'frame_id']).reset_index(drop=True)\n",
    "\n",
    "    # 1. Recreate the exact same features as in training\n",
    "    play_df['height_inches'] = play_df['player_height'].apply(height_to_inches)\n",
    "    game_date_str = play_df['game_id'].astype(str).str[:8]\n",
    "    game_date = pd.to_datetime(game_date_str, format='%Y%m%d')\n",
    "    player_birth_date = pd.to_datetime(play_df['player_birth_date'])\n",
    "    play_df['age'] = (game_date - player_birth_date).dt.days / 365.25\n",
    "\n",
    "    # 2. Apply the pre-fitted preprocessor\n",
    "    feature_cols = preprocessor.feature_names_in_\n",
    "    processed_features_array = preprocessor.transform(play_df[feature_cols])\n",
    "    processed_features_df = pd.DataFrame(processed_features_array, index=play_df.index)\n",
    "\n",
    "    # 3. Create sequences for each row in the original `test_df` (each row to predict)\n",
    "    processed_df_with_ids = pd.concat([play_df[['nfl_id', 'frame_id']], processed_features_df], axis=1)\n",
    "    sequences = []\n",
    "    for _, row_to_predict in test_df.iterrows():\n",
    "        player_id = row_to_predict['nfl_id']\n",
    "        frame_id = row_to_predict['frame_id']\n",
    "        \n",
    "        # Find the player's data and the exact frame we need to predict\n",
    "        player_data_with_ids = processed_df_with_ids[processed_df_with_ids['nfl_id'] == player_id]\n",
    "        prediction_frame_index = player_data_with_ids[player_data_with_ids['frame_id'] == frame_id].index[0]\n",
    "        \n",
    "        # The sequence consists of the `SEQUENCE_LENGTH` frames *before* the prediction frame\n",
    "        start_idx = prediction_frame_index - SEQUENCE_LENGTH\n",
    "        end_idx = prediction_frame_index\n",
    "        \n",
    "        if start_idx < 0:\n",
    "            # If we don't have enough history, pad with the first frame or zeros\n",
    "            # Here we slice from 0 to end_idx\n",
    "            sequence = processed_features_df.iloc[0:end_idx].values\n",
    "            # Pad with zeros at the beginning\n",
    "            pad_width = SEQUENCE_LENGTH - len(sequence)\n",
    "            if pad_width > 0:\n",
    "                # Pad with the first available frame (repetition) or zeros. \n",
    "                # Using zeros is safer if we assume missing history means 'nothing happened'\n",
    "                # But repetition might be better for continuity. Let's use zero padding for now as it's standard.\n",
    "                padding = np.zeros((pad_width, sequence.shape[1]))\n",
    "                sequence = np.vstack([padding, sequence])\n",
    "        else:\n",
    "            # Slice the sequence from the purely numerical dataframe\n",
    "            sequence = processed_features_df.iloc[start_idx:end_idx].values\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a1990c4-9d97-49d2-9001-289e641a755c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:55:41.728285Z",
     "iopub.status.busy": "2025-11-20T05:55:41.728052Z",
     "iopub.status.idle": "2025-11-20T05:55:41.732873Z",
     "shell.execute_reply": "2025-11-20T05:55:41.731948Z",
     "shell.execute_reply.started": "2025-11-20T05:55:41.728268Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(test_df, test_input_df):\n",
    "    \"\"\"\n",
    "    Generates predictions for a single batch (play).\n",
    "    \"\"\"\n",
    "    # The gateway provides polars dataframes, convert them to pandas\n",
    "    test_df = test_df.to_pandas()\n",
    "    test_input_df = test_input_df.to_pandas()\n",
    "\n",
    "    # 1. Preprocess the data to create features for the model\n",
    "    features = preprocess_features(test_df, test_input_df)\n",
    "\n",
    "    if features.shape[0] == 0:\n",
    "        return pd.DataFrame([], columns=['x', 'y'])\n",
    "    \n",
    "    if scipy.sparse.issparse(features):\n",
    "        features = features.toarray()\n",
    "    # 2. Run inference\n",
    "    # Calling the model directly is often faster for inference than model.predict()\n",
    "    predictions_xy = model(features, training=False).numpy()\n",
    "\n",
    "    if scipy.sparse.issparse(predictions_xy):\n",
    "        preds = predictions_xy.toarray()\n",
    "\n",
    "    # 3. Format the predictions into the required DataFrame\n",
    "    return pd.DataFrame(predictions_xy, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "### 7. Run Inference Server\n",
    "This final cell sets up the Kaggle evaluation environment. It initializes the `NFLInferenceServer` with our `predict` function. The server will then either run in a live competition environment or use a local gateway for testing, depending on the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b2dded0-0c59-4bd2-9f1d-dc3d594ef267",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T05:55:46.047670Z",
     "iopub.status.busy": "2025-11-20T05:55:46.047418Z",
     "iopub.status.idle": "2025-11-20T05:56:10.153808Z",
     "shell.execute_reply": "2025-11-20T05:56:10.152549Z",
     "shell.execute_reply.started": "2025-11-20T05:55:46.047653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is output local None\n"
     ]
    }
   ],
   "source": [
    "import kaggle_evaluation.nfl_inference_server\n",
    "\n",
    "inference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    output = inference_server.serve()\n",
    "    print(\"this is output\", output)\n",
    "else:\n",
    "    output = inference_server.run_local_gateway(('/kaggle/input/nfl-big-data-bowl-2026-prediction/',))\n",
    "    print(\"this is output local\", output)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpuV5e8",
   "dataSources": [
    {
     "databundleVersionId": 14210809,
     "sourceId": 114239,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31194,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
