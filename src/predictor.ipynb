{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b1c2d3",
   "metadata": {},
   "source": [
    "### 1. Installs and Imports\n",
    "\n",
    "This cell ensures the necessary libraries (`tensorflow`, `keras`) are up-to-date and then imports all required modules for data loading, preprocessing, model building, and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe4cc5",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow keras\n",
    "!pip install --upgrade polars\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import jax.numpy as jnp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import kaggle_evaluation.nfl_inference_server\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2g3h4",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Preprocessing Functions\n",
    "\n",
    "This section contains all the functions required to load, preprocess, and structure the data for the model. \n",
    "\n",
    "- `get_feature_label_specs`: A utility to inspect the shape of features and labels in a `tf.data.Dataset`.\n",
    "- `create_preprocessor`: Defines the feature transformation pipeline using `ColumnTransformer`. It handles scaling for numerical features, one-hot encoding for categorical features, and conversion for boolean features.\n",
    "- `height_to_inches`: A helper function to convert height from a string format to inches.\n",
    "- `_create_sequences_for_group`: This function takes data for a single player in a single play and transforms it into sequences of a fixed length (`SEQUENCE_LENGTH`), which is the required input format for an LSTM model.\n",
    "- `load_and_prepare_data`: The main data pipeline function. It reads the raw CSVs, merges them, performs feature engineering (e.g., calculating age), fits and applies the preprocessor, creates sequences using `_create_sequences_for_group`, and finally splits the data into training and validation sets, returning them as `tf.data.Dataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i5j6k7l8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_label_specs(dataset):\n",
    "    \"\"\"\n",
    "    Gets the feature and label specifications from a TensorFlow Dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (tf.data.Dataset): The TensorFlow Dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the feature and label specifications.\n",
    "               (feature_spec, label_spec)\n",
    "    \"\"\"\n",
    "    element_spec = dataset.element_spec\n",
    "    return element_spec[0], element_spec[1]\n",
    "\n",
    "def create_preprocessor(features_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Creates a preprocessor for the NFL Big Data Bowl 2026 prediction data.\n",
    "\n",
    "    Args:\n",
    "        features_df (pd.DataFrame): The dataframe with the features.\n",
    "\n",
    "    Returns:\n",
    "        ColumnTransformer: The preprocessor.\n",
    "    \"\"\"\n",
    "    # Identify column types from the dataframe\n",
    "    numerical_features = features_df.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_features = features_df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "    # Ensure boolean 'player_to_predict' is handled separately if it's not numeric\n",
    "    boolean_features = []\n",
    "    if 'player_to_predict' in categorical_features:\n",
    "        categorical_features.remove('player_to_predict')\n",
    "        boolean_features.append('player_to_predict')\n",
    "    elif 'player_to_predict' in numerical_features:\n",
    "        numerical_features.remove('player_to_predict')\n",
    "        boolean_features.append('player_to_predict')\n",
    "        \n",
    "    # Remove identifiers that should not be model features\n",
    "    ids_to_remove = ['game_id', 'play_id', 'frame_id']\n",
    "    for col in ids_to_remove:\n",
    "        if col in numerical_features:\n",
    "            numerical_features.remove(col)\n",
    "        if col in categorical_features:\n",
    "            categorical_features.remove(col)\n",
    "\n",
    "    boolean_features = ['player_to_predict']\n",
    "\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "    boolean_transformer = FunctionTransformer(lambda x: x.fillna(0).astype(int))\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('bool', boolean_transformer, boolean_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "def height_to_inches(height_str):\n",
    "    \"\"\"\n",
    "    Converts height string 'feet-inches' to inches.\n",
    "    \"\"\"\n",
    "    if isinstance(height_str, str):\n",
    "        feet, inches = map(int, height_str.split('-'))\n",
    "        return feet * 12 + inches\n",
    "    return np.nan\n",
    "\n",
    "SEQUENCE_LENGTH = 10\n",
    "def _create_sequences_for_group(group_df: pd.DataFrame, sequence_length):\n",
    "    \"\"\"\n",
    "    Creates sequences of features and corresponding labels for a single player/play group.\n",
    "    \"\"\"\n",
    "    # The group is already sorted by frame_id from the previous step\n",
    "    num_frames = len(group_df)\n",
    "    if num_frames < sequence_length + 1:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # Extract features and labels as numpy arrays\n",
    "    feature_cols = [col for col in group_df.columns if col not in ['game_id', 'play_id', 'nfl_id', 'frame_id', 'x_label', 'y_label']]\n",
    "    features_array = group_df[feature_cols].values\n",
    "    labels_array = group_df[['x_label', 'y_label']].values\n",
    "\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # Number of complete sequences that can be formed\n",
    "    num_sequences = num_frames - sequence_length\n",
    "\n",
    "    for i in range(num_sequences):\n",
    "        sequences.append(features_array[i : i + sequence_length])\n",
    "        labels.append(labels_array[i + sequence_length])\n",
    "\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "def load_and_prepare_data(data_dir, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Loads input and output data from CSV files in the specified directory,\n",
    "    merges them, preprocesses the features, splits them into training and \n",
    "    validation sets, and returns them as TensorFlow Datasets.\n",
    "    The data is prepared into sequences of SEQUENCE_LENGTH frames.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The path to the directory containing the training data.\n",
    "        test_size (float): The proportion of the dataset to allocate to the validation set.\n",
    "        random_state (int): The seed for the random number generator used for the split.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training and validation TensorFlow Datasets,\n",
    "               and the preprocessor.\n",
    "               (train_dataset, val_dataset, preprocessor)\n",
    "    \"\"\"\n",
    "    input_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('input')])\n",
    "    output_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.startswith('output')])\n",
    "\n",
    "    input_dfs = [pd.read_csv(f) for f in input_files]\n",
    "    output_dfs = [pd.read_csv(f) for f in output_files]\n",
    "\n",
    "    input_df = pd.concat(input_dfs, ignore_index=True)\n",
    "    output_df = pd.concat(output_dfs, ignore_index=True)\n",
    "\n",
    "    merged_df = pd.merge(input_df, output_df, on=['game_id', 'play_id', 'nfl_id', 'frame_id'], suffixes=('', '_label'))\n",
    "\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Use all columns from the original input dataframe as features, except for labels\n",
    "    feature_cols_for_model = [col for col in input_df.columns]\n",
    "\n",
    "    # Create a DataFrame with only the features that will be preprocessed\n",
    "    # This is what the preprocessor will be fitted on\n",
    "    features_for_preprocessor_fitting = merged_df[feature_cols_for_model]\n",
    "\n",
    "    preprocessor = create_preprocessor(features_for_preprocessor_fitting)\n",
    "    preprocessor.fit(features_for_preprocessor_fitting) # Fit the preprocessor here\n",
    "\n",
    "    # Apply preprocessing to the entire feature set\n",
    "    # This will return a sparse matrix, convert to dense array for sequence creation\n",
    "    processed_features_array = preprocessor.transform(features_for_preprocessor_fitting).toarray()\n",
    "    \n",
    "    # Create a DataFrame from the processed features to easily merge back with identifiers\n",
    "    processed_features_df = pd.DataFrame(processed_features_array, index=merged_df.index)\n",
    "    \n",
    "    # Add back identifiers needed for grouping and labels\n",
    "    processed_df = pd.concat([merged_df[['game_id', 'play_id', 'nfl_id', 'frame_id', 'x_label', 'y_label']], processed_features_df], axis=1)\n",
    "\n",
    "    # Sort by frame_id within each group to ensure correct sequence order\n",
    "    processed_df = processed_df.sort_values(by=['game_id', 'play_id', 'nfl_id', 'frame_id']).reset_index(drop=True)\n",
    "\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Group by game, play, and player to create sequences\n",
    "    for (game_id, play_id, nfl_id), group_df in processed_df.groupby(['game_id', 'play_id', 'nfl_id']):\n",
    "        sequences, labels = _create_sequences_for_group(group_df, SEQUENCE_LENGTH)\n",
    "        # print(f\"Group: {game_id}, {play_id}, {nfl_id} - Sequences length: {len(sequences)}, Labels length: {len(labels)}\")\n",
    "        if sequences.size > 0 and labels.size > 0:\n",
    "            all_sequences.append(sequences)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    if not all_sequences:\n",
    "        raise ValueError(\"No sequences could be created. Please check data and SEQUENCE_LENGTH.\")\n",
    "\n",
    "    X = np.concatenate(all_sequences, axis=0)\n",
    "    y = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "    return train_dataset, val_dataset, preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m9n0o1p2",
   "metadata": {},
   "source": [
    "### 3. Model Definition and Training Functions\n",
    "\n",
    "This section defines the model architecture and the training loop.\n",
    "\n",
    "- `build_model`: Creates a simple Keras Sequential model with an LSTM layer followed by a Dense output layer. It's compiled with the Adam optimizer and Mean Squared Error (MSE) loss, suitable for this regression task.\n",
    "- `train_model`: A wrapper function that handles the training process. It shuffles and batches the datasets for efficiency and then calls `model.fit` to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429fbe7",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "# from data_loader import load_and_prepare_data, SEQUENCE_LENGTH\n",
    "\n",
    "def build_model(input_features, output_shape, lstm_units=64):\n",
    "    \"\"\"\n",
    "    Builds a sequential model with two LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        input_features (int): The number of input features per timestep.\n",
    "        output_shape (int): The number of output units.\n",
    "        lstm_units (int): The number of units in the LSTM layers.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(SEQUENCE_LENGTH, input_features)),  # Input shape for a sequence of timesteps\n",
    "        layers.LSTM(lstm_units),\n",
    "        layers.Dense(output_shape)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, epochs, batch_size):\n",
    "    \"\"\"\n",
    "    Trains the Keras model.\n",
    "    \"\"\"\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    if val_dataset:\n",
    "        val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    print(\"Starting model training...\")\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_dataset)\n",
    "    print(\"Model training finished.\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q3r4s5t6",
   "metadata": {},
   "source": [
    "### 4. Main Training Execution\n",
    "\n",
    "This is the main execution block of the notebook. It sets hyperparameters like the data directory, batch size, and number of epochs. It then calls the functions defined above to:\n",
    "1. Load and prepare the data.\n",
    "2. Determine the input and output shapes for the model from the dataset.\n",
    "3. Build the model.\n",
    "4. Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2455736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "\"\"\"\n",
    "Main function to load data, build, and train the model.\n",
    "\"\"\"\n",
    "prediction_data_dir = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "train_ds, val_ds, preprocessor = load_and_prepare_data(prediction_data_dir)\n",
    "\n",
    "if train_ds.cardinality().numpy() == 0:\n",
    "    print(\"No training data generated. Please check data loading and feature engineering.\")\n",
    "    # return\n",
    "\n",
    "# Detect and initialize hardware strategy\n",
    "tpu_resolver = None\n",
    "try:\n",
    "    tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('TPU found with resolver: ', tpu_resolver.master())\n",
    "except ValueError:\n",
    "    print(\"Could not initialize TPU resolver. Falling back to other checks.\")\n",
    "\n",
    "if tpu_resolver:\n",
    "    tf.config.experimental_connect_to_cluster(tpu_resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu_resolver)\n",
    "    print(\"Running on TPU\")\n",
    "else:\n",
    "    # If no TPU is found, check for GPUs\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if len(gpus) > 0:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print(f'Running on {len(gpus)} GPU(s).')\n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        print('Running on CPU.')\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "# Get the input and output shapes from the dataset specs\n",
    "feature_spec, label_spec = train_ds.element_spec\n",
    "input_features = feature_spec.shape[1] # Now shape is (SEQUENCE_LENGTH, input_features)\n",
    "output_shape = label_spec.shape[0]\n",
    "\n",
    "# Build and compile the model within the strategy scope to run on TPU\n",
    "with strategy.scope():\n",
    "    model = build_model(input_features, output_shape)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "train_model(model, train_ds, val_ds, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u7v8w9x0",
   "metadata": {},
   "source": [
    "### 5. Save Artifacts\n",
    "\n",
    "After training is complete, this cell saves the two essential artifacts for inference: the trained Keras model (`nfl_model.h5`) and the fitted `preprocessor` object (`preprocessor.joblib`). These files are required by the `submission.ipynb` notebook to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d656f4e3-f3d0-489f-bf46-ac450b023485",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_save_path = 'nfl_model.h5'\n",
    "preprocessor_save_path = 'preprocessor.joblib'\n",
    "\n",
    "model.save(model_save_path)\n",
    "joblib.dump(preprocessor, preprocessor_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "print(f\"Preprocessor saved to {preprocessor_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y1z2a3b4",
   "metadata": {},
   "source": [
    "### 6. Inference Functions (for submission)\n",
    "This section contains the functions that will be used in the `submission.ipynb` notebook. They are included here for completeness and to ensure the entire pipeline is defined in one place before being split for submission.\n",
    "\n",
    "- `load_artifacts`: Loads the saved model and preprocessor.\n",
    "- `preprocess_features`: Replicates the feature engineering and sequence creation for the test data.\n",
    "- `predict`: The main prediction function that ties preprocessing and model inference together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e77717a-a133-41fc-a94f-c06cae579250",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'nfl_model.h5'\n",
    "PREPROCESSOR_PATH = 'preprocessor.joblib'\n",
    "\n",
    "def load_artifacts():\n",
    "    \"\"\"\n",
    "    Loads the trained Keras model and the preprocessor from disk.\n",
    "    Raises FileNotFoundError if either artifact is missing.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}. Please train the model first by running predictor.py.\")\n",
    "    if not os.path.exists(PREPROCESSOR_PATH):\n",
    "        raise FileNotFoundError(f\"Preprocessor file not found at {PREPROCESSOR_PATH}. Please train the model first.\")\n",
    "\n",
    "    print(f\"Loading model from {MODEL_PATH}\")\n",
    "    model = tf.keras.models.load_model(MODEL_PATH)\n",
    "    \n",
    "    print(f\"Loading preprocessor from {PREPROCESSOR_PATH}\")\n",
    "    preprocessor = joblib.load(PREPROCESSOR_PATH)\n",
    "    \n",
    "    return model, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899e0e5-0a35-43b9-98b5-6e643c88e63f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the model globally to avoid reloading it for each batch.\n",
    "model, preprocessor = load_artifacts()\n",
    "\n",
    "def preprocess_features(test_df, test_input_df):\n",
    "    \"\"\"\n",
    "    Preprocesses the raw input dataframes into a format the model expects.\n",
    "    This function replicates the feature engineering and sequence creation from\n",
    "    the training pipeline (`data_loader.py`).\n",
    "    \n",
    "    Args:\n",
    "        test_df (pd.DataFrame): The dataframe with the rows to predict.\n",
    "        test_input_df (pd.DataFrame): The dataframe with the input features for the play.\n",
    "\n",
    "    Returns:\n",
    "        np.array: A 3D array of shape (num_predictions, SEQUENCE_LENGTH, num_features)\n",
    "                  ready to be fed into the LSTM model.\n",
    "    \"\"\"\n",
    "    num_predictions = len(test_df)\n",
    "    if num_predictions == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # Combine input data for the entire play. `test_input_df` contains frame 0 (the context),\n",
    "    # and `test_df` contains the frames we need to predict for.\n",
    "    play_df = pd.concat([test_input_df, test_df], ignore_index=True)\n",
    "    play_df = play_df.sort_values(by=['nfl_id', 'frame_id']).reset_index(drop=True)\n",
    "\n",
    "    # 1. Recreate the exact same features as in training\n",
    "    play_df['height_inches'] = play_df['player_height'].apply(height_to_inches)\n",
    "    game_date_str = play_df['game_id'].astype(str).str[:8]\n",
    "    game_date = pd.to_datetime(game_date_str, format='%Y%m%d')\n",
    "    player_birth_date = pd.to_datetime(play_df['player_birth_date'])\n",
    "    play_df['age'] = (game_date - player_birth_date).dt.days / 365.25\n",
    "\n",
    "    # 2. Apply the pre-fitted preprocessor\n",
    "    feature_cols = preprocessor.feature_names_in_\n",
    "    processed_features_array = preprocessor.transform(play_df[feature_cols])\n",
    "    processed_features_df = pd.DataFrame(processed_features_array, index=play_df.index)\n",
    "\n",
    "    # 3. Create sequences for each row in the original `test_df` (each row to predict)\n",
    "    processed_df_with_ids = pd.concat([play_df[['nfl_id', 'frame_id']], processed_features_df], axis=1)\n",
    "    sequences = []\n",
    "    for _, row_to_predict in test_df.iterrows():\n",
    "        player_id = row_to_predict['nfl_id']\n",
    "        frame_id = row_to_predict['frame_id']\n",
    "        \n",
    "        # Find the player's data and the exact frame we need to predict\n",
    "        player_data_with_ids = processed_df_with_ids[processed_df_with_ids['nfl_id'] == player_id]\n",
    "        prediction_frame_index = player_data_with_ids[player_data_with_ids['frame_id'] == frame_id].index[0]\n",
    "        \n",
    "        # The sequence consists of the `SEQUENCE_LENGTH` frames *before* the prediction frame\n",
    "        start_idx = prediction_frame_index - SEQUENCE_LENGTH\n",
    "        end_idx = prediction_frame_index\n",
    "        \n",
    "        # Slice the sequence from the purely numerical dataframe\n",
    "        sequence = processed_features_df.iloc[start_idx:end_idx].values\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1990c4-9d97-49d2-9001-289e641a755c",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(test_df, test_input_df):\n",
    "    \"\"\"\n",
    "    Generates predictions for a single batch (play).\n",
    "    \"\"\"\n",
    "    # The gateway provides polars dataframes, convert them to pandas\n",
    "    test_df = test_df.to_pandas()\n",
    "    test_input_df = test_input_df.to_pandas()\n",
    "\n",
    "    # 1. Preprocess the data to create features for the model\n",
    "    features = preprocess_features(test_df, test_input_df)\n",
    "\n",
    "    if features.shape[0] == 0:\n",
    "        return pd.DataFrame([], columns=['x', 'y'])\n",
    "\n",
    "    # 2. Run inference\n",
    "    # Calling the model directly is often faster for inference than model.predict()\n",
    "    predictions_xy = model(features, training=False).numpy()\n",
    "\n",
    "    # 3. Format the predictions into the required DataFrame\n",
    "    return pd.DataFrame(predictions_xy, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8",
   "metadata": {},
   "source": [
    "### 7. Run Inference Server\n",
    "This final cell sets up the Kaggle evaluation environment. It initializes the `NFLInferenceServer` with our `predict` function. The server will then either run in a live competition environment or use a local gateway for testing, depending on the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2dded0-0c59-4bd2-9f1d-dc3d594ef267",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import kaggle_evaluation.nfl_inference_server\n",
    "\n",
    "inference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/nfl-big-data-bowl-2026-prediction/',))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
