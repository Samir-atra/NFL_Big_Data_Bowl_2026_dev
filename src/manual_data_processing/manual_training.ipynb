{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerator strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Accelerator Strategy Setup\n",
    "\n",
    "This cell configures TensorFlow to automatically detect and utilize the best available hardware accelerator:\n",
    "\n",
    "- **TPU**: Google's Tensor Processing Unit (Kaggle/Colab)\n",
    "- **Multi-GPU**: Uses `MirroredStrategy` for distributed training\n",
    "- **Single GPU/CPU**: Falls back to default strategy\n",
    "\n",
    "### Key Components:\n",
    "| Strategy | Use Case | Expected Speedup |\n",
    "|----------|----------|-----------------|\n",
    "| TPUStrategy | Cloud TPU pods | 8-16x |\n",
    "| MirroredStrategy | Multi-GPU systems | 2-4x per GPU |\n",
    "| Default | Single GPU/CPU | Baseline |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T09:44:18.108799Z",
     "iopub.status.busy": "2025-11-29T09:44:18.108386Z",
     "iopub.status.idle": "2025-11-29T09:44:36.343798Z",
     "shell.execute_reply": "2025-11-29T09:44:36.342983Z",
     "shell.execute_reply.started": "2025-11-29T09:44:18.108774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Detect hardware\n",
    "try:\n",
    "    # Check for TPU\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    # Check for GPU(s)\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if len(gpus) > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print(f'Running on {len(gpus)} GPUs')\n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        print('Running on single GPU or CPU')\n",
    "\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "# Configure for Kaggle\n",
    "if os.path.exists('/kaggle'):\n",
    "    print(\"Running in Kaggle environment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Unsupervised Data Loader\n",
    "\n",
    "This cell defines the data loading infrastructure for **unsupervised pre-training**. Unlike supervised learning, we use ALL player data (not just `player_to_predict=True`) to maximize the amount of training data for representation learning.\n",
    "\n",
    "### Key Classes:\n",
    "\n",
    "#### `UnsupervisedNFLDataLoader`\n",
    "Loads and preprocesses NFL tracking data from CSV files using **Polars** for efficient data handling.\n",
    "\n",
    "**Features:**\n",
    "- âœ… Supports loading from multiple directories\n",
    "- âœ… Flexible filtering (labeled/unlabeled data)\n",
    "- âœ… Automatic feature type conversion (strings â†’ floats)\n",
    "- âœ… Optional Z-score normalization\n",
    "- âœ… Groups data into sequences by (game_id, play_id, nfl_id)\n",
    "\n",
    "#### `UnsupervisedNFLSequence`\n",
    "A Keras `Sequence` generator that handles batching and padding for variable-length sequences.\n",
    "\n",
    "**Supported Tasks:**\n",
    "| Task | Input | Output | Use Case |\n",
    "|------|-------|--------|----------|\n",
    "| `autoencoder` | Sequence | Same Sequence | Reconstruction learning |\n",
    "| `next_step` | Sequence[:-n] | Sequence[-n:] | Temporal prediction |\n",
    "\n",
    "### Data Flow:\n",
    "```\n",
    "CSV Files â†’ UnsupervisedNFLDataLoader â†’ NumPy Arrays â†’ UnsupervisedNFLSequence â†’ Model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T16:01:16.018174Z",
     "iopub.status.busy": "2025-11-30T16:01:16.017417Z",
     "iopub.status.idle": "2025-11-30T16:02:57.065829Z",
     "shell.execute_reply": "2025-11-30T16:02:57.064975Z",
     "shell.execute_reply.started": "2025-11-30T16:01:16.018146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install polars\n",
    "# !pip install keras-tuner\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "class UnsupervisedNFLDataLoader:\n",
    "    \"\"\"Loads NFL data for unsupervised learning (no trajectory labels needed).\n",
    "    \n",
    "    This loader processes ALL player sequences (player_to_predict=True and False)\n",
    "    to maximize the amount of training data for representation learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_sequences = None\n",
    "        \n",
    "    def load_files(self, directories, include_labeled=True, include_unlabeled=True, normalize=False):\n",
    "        \"\"\"Load input files from specified directories.\n",
    "        \n",
    "        Args:\n",
    "            directories (list): List of directory paths to load from\n",
    "            include_labeled (bool): Include player_to_predict=True sequences\n",
    "            include_unlabeled (bool): Include player_to_predict=False sequences\n",
    "            normalize (bool): Whether to normalize features (Z-score)\n",
    "        \"\"\"\n",
    "        input_dfs = []\n",
    "        \n",
    "        print(f\"Loading unsupervised data from {len(directories)} directories...\")\n",
    "        print(f\"Include labeled: {include_labeled}, Include unlabeled: {include_unlabeled}\")\n",
    "        \n",
    "        for d in directories:\n",
    "            if not os.path.exists(d):\n",
    "                print(f\"Warning: Directory not found: {d}\")\n",
    "                continue\n",
    "                \n",
    "            input_files = sorted([f for f in os.listdir(d) if f.startswith('input') and f.endswith('.csv')])\n",
    "            print(f\"  Found {len(input_files)} input files in {d}\")\n",
    "            \n",
    "            for f in input_files:\n",
    "                try:\n",
    "                    df = pl.read_csv(os.path.join(d, f), infer_schema_length=10000)\n",
    "                    \n",
    "                    initial_rows = len(df)\n",
    "                    \n",
    "                    # Filter based on player_to_predict flag\n",
    "                    if \"player_to_predict\" in df.columns:\n",
    "                        if include_labeled and not include_unlabeled:\n",
    "                            # Only labeled\n",
    "                            if df[\"player_to_predict\"].dtype == pl.Boolean:\n",
    "                                df = df.filter(pl.col(\"player_to_predict\") == True)\n",
    "                            else:\n",
    "                                df = df.filter(pl.col(\"player_to_predict\").cast(pl.Utf8).str.to_lowercase() == \"true\")\n",
    "                        elif include_unlabeled and not include_labeled:\n",
    "                            # Only unlabeled\n",
    "                            if df[\"player_to_predict\"].dtype == pl.Boolean:\n",
    "                                df = df.filter(pl.col(\"player_to_predict\") == False)\n",
    "                            else:\n",
    "                                df = df.filter(pl.col(\"player_to_predict\").cast(pl.Utf8).str.to_lowercase() == \"false\")\n",
    "                        # If both True, include all (no filtering)\n",
    "                    \n",
    "                    if len(df) > 0:\n",
    "                        input_dfs.append(df)\n",
    "                        print(f\"    {f}: {initial_rows} -> {len(df)} rows\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {f}: {e}\")\n",
    "        \n",
    "        if not input_dfs:\n",
    "            print(\"No data found.\")\n",
    "            self.input_sequences = pl.DataFrame()\n",
    "            return\n",
    "        \n",
    "        # Concatenate all dataframes\n",
    "        print(\"Concatenating dataframes...\")\n",
    "        full_input = pl.concat(input_dfs, how=\"vertical_relaxed\")\n",
    "        \n",
    "        # Deduplicate\n",
    "        full_input = full_input.unique(subset=[\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n",
    "        \n",
    "        # Process features\n",
    "        print(\"Processing features...\")\n",
    "        id_cols = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"player_to_predict\", \"time\"]\n",
    "        feature_cols = [c for c in full_input.columns if c not in id_cols]\n",
    "        \n",
    "        expressions = []\n",
    "        for col in feature_cols:\n",
    "            if full_input[col].dtype == pl.Utf8:\n",
    "                expr = (\n",
    "                    pl.when(pl.col(col).str.to_lowercase() == \"true\").then(1.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"false\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"left\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"right\").then(1.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"defense\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"offense\").then(1.0)\n",
    "                    .otherwise(\n",
    "                        pl.col(col).cast(pl.Float64, strict=False).fill_null(\n",
    "                            pl.col(col).hash() % 10000\n",
    "                        )\n",
    "                    ).cast(pl.Float64).alias(col)\n",
    "                )\n",
    "                expressions.append(expr)\n",
    "            else:\n",
    "                expressions.append(pl.col(col).cast(pl.Float64).alias(col))\n",
    "        \n",
    "        full_input = full_input.with_columns(expressions)\n",
    "        \n",
    "        # Sort by frame_id\n",
    "        if \"frame_id\" in full_input.columns:\n",
    "            full_input = full_input.sort([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n",
    "            \n",
    "        # Normalize features if requested\n",
    "        if normalize:\n",
    "            print(\"Normalizing features (Z-score)...\")\n",
    "            norm_exprs = []\n",
    "            for col in feature_cols:\n",
    "                # Check for constant columns to avoid division by zero\n",
    "                n_unique = full_input.select(pl.col(col).n_unique()).item()\n",
    "                if n_unique > 1:\n",
    "                    norm_exprs.append(\n",
    "                        ((pl.col(col) - pl.col(col).mean()) / (pl.col(col).std() + 1e-8)).alias(col)\n",
    "                    )\n",
    "            \n",
    "            if norm_exprs:\n",
    "                full_input = full_input.with_columns(norm_exprs)\n",
    "        \n",
    "        # Group into sequences\n",
    "        agg_exprs = [pl.col(c) for c in feature_cols]\n",
    "        self.input_sequences = full_input.group_by(\n",
    "            [\"game_id\", \"play_id\", \"nfl_id\"], \n",
    "            maintain_order=True\n",
    "        ).agg(agg_exprs)\n",
    "        \n",
    "        print(f\"Total sequences: {len(self.input_sequences)}\")\n",
    "        \n",
    "        # Debug sequence lengths\n",
    "        if len(self.input_sequences) > 0:\n",
    "            lengths = self.input_sequences.select(pl.col(feature_cols[0]).list.len()).to_series()\n",
    "            print(f\"Sequence length stats: min={lengths.min()}, max={lengths.max()}, mean={lengths.mean():.2f}\")\n",
    "\n",
    "        \n",
    "    def get_sequences(self):\n",
    "        \"\"\"Convert sequences to numpy arrays.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Array of input sequences (object array)\n",
    "        \"\"\"\n",
    "        if self.input_sequences is None or self.input_sequences.is_empty():\n",
    "            return np.array([])\n",
    "        \n",
    "        print(\"Converting to NumPy arrays...\")\n",
    "        \n",
    "        # Get feature columns (exclude keys)\n",
    "        input_cols = [c for c in self.input_sequences.columns \n",
    "                     if c not in [\"game_id\", \"play_id\", \"nfl_id\"]]\n",
    "        \n",
    "        # Convert to sequences\n",
    "        input_col_indices = [self.input_sequences.columns.index(c) for c in input_cols]\n",
    "        rows = self.input_sequences.iter_rows()\n",
    "        \n",
    "        X_list = []\n",
    "        for row in rows:\n",
    "            feature_seqs = [row[i] for i in input_col_indices]\n",
    "            # Convert to numpy array to ensure consistent shape (T, F)\n",
    "            X_seq = np.array(list(zip(*feature_seqs)), dtype=np.float32)\n",
    "            if len(X_seq) > 0:\n",
    "                X_list.append(X_seq)\n",
    "        \n",
    "        X = np.array(X_list, dtype=object)\n",
    "        print(f\"Loaded {len(X)} sequences\")\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "class UnsupervisedNFLSequence(Sequence):\n",
    "    \"\"\"Keras Sequence for unsupervised learning on NFL data.\n",
    "    \n",
    "    For autoencoder: input and output are the same (reconstruction)\n",
    "    For next-step prediction: input is sequence[:-n], output is sequence[n:]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, batch_size=32, maxlen=None, shuffle=True, \n",
    "                 task='autoencoder', prediction_steps=1):\n",
    "        \"\"\"Initialize the sequence.\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequences\n",
    "            batch_size: Batch size\n",
    "            maxlen: Maximum sequence length (auto-detect if None)\n",
    "            shuffle: Whether to shuffle\n",
    "            task: 'autoencoder' or 'next_step'\n",
    "            prediction_steps: For next_step, how many steps ahead to predict\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.task = task\n",
    "        self.prediction_steps = prediction_steps\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        \n",
    "        # Determine max length\n",
    "        if maxlen is None:\n",
    "            self.maxlen = max(len(seq) for seq in X)\n",
    "        else:\n",
    "            self.maxlen = maxlen\n",
    "        \n",
    "        print(f\"UnsupervisedNFLSequence initialized:\")\n",
    "        print(f\"  Samples: {len(self.X)}\")\n",
    "        print(f\"  Batch size: {batch_size}\")\n",
    "        print(f\"  Max length: {self.maxlen}\")\n",
    "        print(f\"  Task: {task}\")\n",
    "        \n",
    "        if self.task == 'next_step':\n",
    "            if self.maxlen <= self.prediction_steps:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid configuration: maxlen ({self.maxlen}) must be greater than \"\n",
    "                    f\"prediction_steps ({self.prediction_steps}) for 'next_step' task.\\n\"\n",
    "                    f\"Please increase maxlen or decrease prediction_steps.\"\n",
    "                )\n",
    "            \n",
    "            # Ensure we don't produce 0-length sequences even if maxlen is technically larger but close\n",
    "            effective_len = self.maxlen - self.prediction_steps\n",
    "            if effective_len < 1:\n",
    "                 raise ValueError(f\"Effective sequence length (maxlen - prediction_steps) is {effective_len}, must be >= 1.\")\n",
    "\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        # Filter out empty sequences to prevent pad_sequences errors\n",
    "        batch_X = [self.X[i] for i in batch_indices if len(self.X[i]) > 0]\n",
    "        \n",
    "        if not batch_X:\n",
    "            # Return dummy batch with correct shape to avoid crashing the training loop\n",
    "            # Shape: (0, maxlen, features)\n",
    "            if len(self.X) > 0 and len(self.X[0]) > 0:\n",
    "                 feat_dim = self.X[0].shape[1]\n",
    "                 return np.zeros((0, self.maxlen, feat_dim)), np.zeros((0, self.maxlen, feat_dim))\n",
    "            return np.array([]), np.array([])\n",
    "        \n",
    "        if self.task == 'autoencoder':\n",
    "            # Input and output are the same (reconstruction task)\n",
    "            X_padded = pad_sequences(\n",
    "                batch_X,\n",
    "                maxlen=self.maxlen,\n",
    "                dtype='float32',\n",
    "                padding='post',\n",
    "                truncating='post',\n",
    "                value=0.0\n",
    "            )\n",
    "            return X_padded, X_padded\n",
    "            \n",
    "        elif self.task == 'next_step':\n",
    "            # Input: sequence up to -prediction_steps\n",
    "            # Output: last prediction_steps frames\n",
    "            batch_X_input = []\n",
    "            batch_y_output = []\n",
    "            \n",
    "            for seq in batch_X:\n",
    "                if len(seq) > self.prediction_steps:\n",
    "                    batch_X_input.append(seq[:-self.prediction_steps])\n",
    "                    batch_y_output.append(seq[-self.prediction_steps:])\n",
    "                else:\n",
    "                    # If sequence too short, use full sequence for both\n",
    "                    batch_X_input.append(seq)\n",
    "                    batch_y_output.append(seq)\n",
    "            \n",
    "            X_padded = pad_sequences(\n",
    "                batch_X_input,\n",
    "                maxlen=self.maxlen - self.prediction_steps,\n",
    "                dtype='float32',\n",
    "                padding='post',\n",
    "                truncating='post',\n",
    "                value=0.0\n",
    "            )\n",
    "            \n",
    "            y_padded = pad_sequences(\n",
    "                batch_y_output,\n",
    "                maxlen=self.prediction_steps,\n",
    "                dtype='float32',\n",
    "                padding='post',\n",
    "                truncating='post',\n",
    "                value=0.0\n",
    "            )\n",
    "            \n",
    "            return X_padded, y_padded\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the loader\n",
    "    PREDICTION_TRAIN_DIR = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n",
    "    \n",
    "    print(\"=== Testing Unsupervised Data Loader ===\\n\")\n",
    "    \n",
    "    # Test 1: Load only unlabeled data\n",
    "    print(\"Test 1: Loading UNLABELED data only\")\n",
    "    loader = UnsupervisedNFLDataLoader()\n",
    "    loader.load_files([PREDICTION_TRAIN_DIR], include_labeled=False, include_unlabeled=True)\n",
    "    X_unlabeled = loader.get_sequences()\n",
    "    print(f\"Unlabeled sequences: {len(X_unlabeled)}\\n\")\n",
    "    \n",
    "    # Test 2: Load ALL data\n",
    "    print(\"Test 2: Loading ALL data (labeled + unlabeled)\")\n",
    "    loader_all = UnsupervisedNFLDataLoader()\n",
    "    loader_all.load_files([PREDICTION_TRAIN_DIR], include_labeled=True, include_unlabeled=True)\n",
    "    X_all = loader_all.get_sequences()\n",
    "    print(f\"Total sequences: {len(X_all)}\\n\")\n",
    "    \n",
    "    if len(X_all) > 0:\n",
    "        print(f\"Sample sequence length: {len(X_all[0])}\")\n",
    "        print(f\"Sample features per timestep: {len(X_all[0][0])}\")\n",
    "        \n",
    "        # Test sequence generators\n",
    "        print(\"\\n=== Testing Sequence Generators ===\")\n",
    "        \n",
    "        print(\"\\nAutoencoder sequence:\")\n",
    "        ae_seq = UnsupervisedNFLSequence(X_all[:1000], batch_size=32, task='autoencoder')\n",
    "        x_batch, y_batch = ae_seq[0]\n",
    "        print(f\"Input shape: {x_batch.shape}\")\n",
    "        print(f\"Output shape: {y_batch.shape}\")\n",
    "        print(f\"Are input and output same? {np.array_equal(x_batch, y_batch)}\")\n",
    "        \n",
    "        print(\"\\nNext-step prediction sequence:\")\n",
    "        ns_seq = UnsupervisedNFLSequence(X_all[:1000], batch_size=32, task='next_step', prediction_steps=5)\n",
    "        x_batch, y_batch = ns_seq[0]\n",
    "        print(f\"Input shape: {x_batch.shape}\")\n",
    "        print(f\"Output shape: {y_batch.shape}\")\n",
    "\n",
    "    # Test 3: Load with normalization\n",
    "    print(\"\\nTest 3: Loading with NORMALIZATION\")\n",
    "    loader_norm = UnsupervisedNFLDataLoader()\n",
    "    loader_norm.load_files([PREDICTION_TRAIN_DIR], include_labeled=True, include_unlabeled=True, normalize=True)\n",
    "    X_norm = loader_norm.get_sequences()\n",
    "    \n",
    "    if len(X_norm) > 0:\n",
    "        print(f\"Sample normalized sequence (first feature of first step): {X_norm[0][0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised models architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Unsupervised Model Architectures\n",
    "\n",
    "This cell defines neural network architectures for **unsupervised representation learning** on temporal sequences.\n",
    "\n",
    "### Architecture 1: LSTM Autoencoder\n",
    "\n",
    "An encoder-decoder architecture that learns compressed representations of player movement sequences.\n",
    "\n",
    "```\n",
    "Input Sequence â†’ Encoder (LSTM stack) â†’ Latent Vector â†’ Decoder (LSTM stack) â†’ Reconstructed Sequence\n",
    "     (T, F)           [256, 128]           (L,)              [128, 256]              (T, F)\n",
    "```\n",
    "\n",
    "**Learning Objective:** Minimize reconstruction error (MSE) between input and output sequences.\n",
    "\n",
    "**Transfer Learning:** The trained encoder extracts meaningful features that can initialize supervised models.\n",
    "\n",
    "### Architecture 2: Next-Step Predictor\n",
    "\n",
    "A sequence-to-sequence model for self-supervised temporal prediction.\n",
    "\n",
    "```\n",
    "Input Sequence[:-N] â†’ Encoder (LSTM stack) â†’ Context Vector â†’ Decoder â†’ Predicted Sequence[-N:]\n",
    "      (T-N, F)             [256, 128]            (H,)         (LSTM)         (N, F)\n",
    "```\n",
    "\n",
    "**Learning Objective:** Predict future N timesteps from past observations.\n",
    "\n",
    "### Training Callbacks\n",
    "\n",
    "| Callback | Purpose | Configuration |\n",
    "|----------|---------|---------------|\n",
    "| `EarlyStopping` | Prevent overfitting | patience=10, monitor='val_loss' |\n",
    "| `ModelCheckpoint` | Save best model | save_best_only=True |\n",
    "| `ReduceLROnPlateau` | Adaptive learning rate | factor=0.5, patience=5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:26:19.331079Z",
     "iopub.status.busy": "2025-12-01T05:26:19.330767Z",
     "iopub.status.idle": "2025-12-01T05:26:19.645062Z",
     "shell.execute_reply": "2025-12-01T05:26:19.644295Z",
     "shell.execute_reply.started": "2025-12-01T05:26:19.331056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class LSTMAutoencoder:\n",
    "    \"\"\"LSTM Autoencoder for unsupervised representation learning on NFL sequences.\n",
    "    \n",
    "    The encoder learns to compress player movement sequences into a latent representation,\n",
    "    and the decoder reconstructs the original sequence. The encoder can then be used\n",
    "    to initialize supervised models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, latent_dim=128, lstm_units=[512, 256, 128, 64, 32]):\n",
    "        \"\"\"Initialize the LSTM Autoencoder.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of input (timesteps, features)\n",
    "            latent_dim: Dimension of latent representation\n",
    "            lstm_units: List of LSTM units for encoder layers\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.autoencoder = None\n",
    "        \n",
    "    def build_encoder(self):\n",
    "        \"\"\"Build the encoder network.\"\"\"\n",
    "        inputs = layers.Input(shape=self.input_shape, name='encoder_input')\n",
    "        \n",
    "        x = inputs\n",
    "        # Stack LSTM layers\n",
    "        for i, units in enumerate(self.lstm_units[:-1]):\n",
    "            x = layers.LSTM(\n",
    "                units, \n",
    "                return_sequences=True,\n",
    "                name=f'encoder_lstm_{i+1}'\n",
    "            )(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Last LSTM layer doesn't return sequences\n",
    "        x = layers.LSTM(\n",
    "            self.lstm_units[-1],\n",
    "            return_sequences=False,\n",
    "            name=f'encoder_lstm_{len(self.lstm_units)}'\n",
    "        )(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Latent representation\n",
    "        latent = layers.Dense(self.latent_dim, activation='relu', name='latent')(x)\n",
    "        \n",
    "        self.encoder = Model(inputs, latent, name='encoder')\n",
    "        return self.encoder\n",
    "    \n",
    "    def build_decoder(self):\n",
    "        \"\"\"Build the decoder network.\"\"\"\n",
    "        # Decoder input is the latent vector\n",
    "        latent_inputs = layers.Input(shape=(self.latent_dim,), name='decoder_input')\n",
    "        \n",
    "        # Repeat the latent vector for each timestep\n",
    "        x = layers.RepeatVector(self.input_shape[0])(latent_inputs)\n",
    "        \n",
    "        # Stack LSTM layers in reverse\n",
    "        for i, units in enumerate(reversed(self.lstm_units)):\n",
    "            x = layers.LSTM(\n",
    "                units,\n",
    "                return_sequences=True,\n",
    "                name=f'decoder_lstm_{i+1}'\n",
    "            )(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Output layer to reconstruct features\n",
    "        outputs = layers.TimeDistributed(\n",
    "            layers.Dense(self.input_shape[1], activation='linear'),\n",
    "            name='reconstruction'\n",
    "        )(x)\n",
    "        \n",
    "        self.decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "        return self.decoder\n",
    "    \n",
    "    def build_autoencoder(self):\n",
    "        \"\"\"Build the complete autoencoder.\"\"\"\n",
    "        if self.encoder is None:\n",
    "            self.build_encoder()\n",
    "        if self.decoder is None:\n",
    "            self.build_decoder()\n",
    "        \n",
    "        # Connect encoder and decoder\n",
    "        inputs = layers.Input(shape=self.input_shape, name='autoencoder_input')\n",
    "        latent = self.encoder(inputs)\n",
    "        outputs = self.decoder(latent)\n",
    "        \n",
    "        self.autoencoder = Model(inputs, outputs, name='autoencoder')\n",
    "        return self.autoencoder\n",
    "    \n",
    "    def compile(self, learning_rate=0.001):\n",
    "        \"\"\"Compile the autoencoder.\"\"\"\n",
    "        if self.autoencoder is None:\n",
    "            self.build_autoencoder()\n",
    "        \n",
    "        self.autoencoder.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "    def get_summary(self):\n",
    "        \"\"\"Print model summaries.\"\"\"\n",
    "        if self.autoencoder:\n",
    "            print(\"\\n=== Autoencoder Summary ===\")\n",
    "            self.autoencoder.summary()\n",
    "        if self.encoder:\n",
    "            print(\"\\n=== Encoder Summary ===\")\n",
    "            self.encoder.summary()\n",
    "        if self.decoder:\n",
    "            print(\"\\n=== Decoder Summary ===\")\n",
    "            self.decoder.summary()\n",
    "\n",
    "\n",
    "class NextStepPredictor:\n",
    "    \"\"\"LSTM model for self-supervised next-step prediction.\n",
    "    \n",
    "    Predicts future timesteps given past timesteps, which can be used\n",
    "    as a pre-training task for the supervised trajectory prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, output_steps=5, lstm_units=[256, 128], output_features=None):\n",
    "        \"\"\"Initialize the next-step predictor.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of input (timesteps, features)\n",
    "            output_steps: Number of future steps to predict\n",
    "            lstm_units: List of LSTM units\n",
    "            output_features: Number of output features (if None, same as input features)\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_steps = output_steps\n",
    "        self.lstm_units = lstm_units\n",
    "        self.output_features = output_features or input_shape[1]\n",
    "        self.model = None\n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\"Build the next-step prediction model.\"\"\"\n",
    "        inputs = layers.Input(shape=self.input_shape, name='input')\n",
    "        \n",
    "        x = inputs\n",
    "        # Stack LSTM layers\n",
    "        for i, units in enumerate(self.lstm_units):\n",
    "            return_seq = (i < len(self.lstm_units) - 1)\n",
    "            x = layers.LSTM(\n",
    "                units,\n",
    "                return_sequences=return_seq,\n",
    "                name=f'lstm_{i+1}'\n",
    "            )(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Prediction head\n",
    "        # Expand to output_steps timesteps\n",
    "        x = layers.RepeatVector(self.output_steps)(x)\n",
    "        x = layers.LSTM(128, return_sequences=True, name='prediction_lstm')(x)\n",
    "        \n",
    "        # Output for each timestep\n",
    "        outputs = layers.TimeDistributed(\n",
    "            layers.Dense(self.output_features, activation='linear'),\n",
    "            name='predictions'\n",
    "        )(x)\n",
    "        \n",
    "        self.model = Model(inputs, outputs, name='next_step_predictor')\n",
    "        return self.model\n",
    "    \n",
    "    def compile(self, learning_rate=0.001):\n",
    "        \"\"\"Compile the model.\"\"\"\n",
    "        if self.model is None:\n",
    "            self.build()\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Print model summary.\"\"\"\n",
    "        if self.model:\n",
    "            self.model.summary()\n",
    "\n",
    "\n",
    "def create_training_callbacks(model_path, patience=10):\n",
    "    \"\"\"Create standard callbacks for training.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to save best model\n",
    "        patience: Patience for early stopping\n",
    "        \n",
    "    Returns:\n",
    "        List of callbacks\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            model_path,\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "def transfer_encoder_weights(pretrained_encoder, supervised_model, freeze_encoder=False):\n",
    "    \"\"\"Transfer weights from pretrained encoder to supervised model.\n",
    "    \n",
    "    Args:\n",
    "        pretrained_encoder: The pretrained encoder model\n",
    "        supervised_model: The supervised model to transfer weights to\n",
    "        freeze_encoder: Whether to freeze the transferred weights\n",
    "        \n",
    "    Returns:\n",
    "        The supervised model with transferred weights\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Transferring Encoder Weights ===\")\n",
    "    \n",
    "    # Get encoder layers from pretrained model\n",
    "    encoder_layer_names = [layer.name for layer in pretrained_encoder.layers]\n",
    "    \n",
    "    # Transfer weights to matching layers in supervised model\n",
    "    transferred_count = 0\n",
    "    for layer in supervised_model.layers:\n",
    "        if layer.name in encoder_layer_names:\n",
    "            try:\n",
    "                pretrained_layer = pretrained_encoder.get_layer(layer.name)\n",
    "                layer.set_weights(pretrained_layer.get_weights())\n",
    "                \n",
    "                if freeze_encoder:\n",
    "                    layer.trainable = False\n",
    "                \n",
    "                transferred_count += 1\n",
    "                print(f\"Transferred weights for layer: {layer.name} (frozen={freeze_encoder})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not transfer weights for {layer.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nTransferred weights for {transferred_count} layers\")\n",
    "    return supervised_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Testing Unsupervised Models ===\\n\")\n",
    "    \n",
    "    # Test parameters\n",
    "    timesteps = 28\n",
    "    features = 18\n",
    "    latent_dim = 64\n",
    "    \n",
    "    print(\"1. Testing LSTM Autoencoder\")\n",
    "    print(\"-\" * 50)\n",
    "    ae = LSTMAutoencoder(\n",
    "        input_shape=(timesteps, features),\n",
    "        latent_dim=latent_dim,\n",
    "        lstm_units=[128, 64]\n",
    "    )\n",
    "    ae.build_autoencoder()\n",
    "    ae.compile()\n",
    "    ae.get_summary()\n",
    "    \n",
    "    print(\"\\n2. Testing Next-Step Predictor\")\n",
    "    print(\"-\" * 50)\n",
    "    predictor = NextStepPredictor(\n",
    "        input_shape=(timesteps, features),\n",
    "        output_steps=5,\n",
    "        lstm_units=[128, 64],\n",
    "        output_features=features\n",
    "    )\n",
    "    predictor.build()\n",
    "    predictor.compile()\n",
    "    predictor.get_summary()\n",
    "    \n",
    "    # Test with dummy data\n",
    "    print(\"\\n3. Testing with dummy data\")\n",
    "    print(\"-\" * 50)\n",
    "    dummy_input = tf.random.normal((32, timesteps, features))\n",
    "    \n",
    "    print(\"Autoencoder forward pass:\")\n",
    "    ae_output = ae.autoencoder(dummy_input)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {ae_output.shape}\")\n",
    "    \n",
    "    print(\"\\nNext-step predictor forward pass:\")\n",
    "    ns_output = predictor.model(dummy_input)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {ns_output.shape}\")\n",
    "    \n",
    "    print(\"\\nEncoder output (latent representation):\")\n",
    "    latent = ae.encoder(dummy_input)\n",
    "    print(f\"Latent shape: {latent.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unsupervised training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‹ï¸ Unsupervised Pre-training Execution\n",
    "\n",
    "This cell executes the unsupervised pre-training pipeline:\n",
    "\n",
    "### Training Pipeline:\n",
    "\n",
    "1. **Data Loading**: Load all NFL player sequences from training directories\n",
    "2. **Train/Val Split**: 80/20 split for training and validation\n",
    "3. **Sequence Generation**: Create batched, padded data generators\n",
    "4. **Model Training**: Train autoencoder for sequence reconstruction\n",
    "5. **Save Artifacts**: Export trained model and encoder weights\n",
    "\n",
    "### Training Configuration:\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|-----------|-------|-----------|\n",
    "| Batch Size | 64 | Balance memory usage and gradient stability |\n",
    "| Max Length | 10 | Truncate/pad sequences to uniform length |\n",
    "| Epochs | 100 | With early stopping for convergence |\n",
    "| Task | autoencoder | Learn general sequence representations |\n",
    "\n",
    "### Output Artifacts:\n",
    "- `autoencoder_{timestamp}.keras` - Full trained model\n",
    "- `autoencoder_{timestamp}_encoder.keras` - Encoder for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T11:03:01.534662Z",
     "iopub.status.busy": "2025-11-30T11:03:01.534328Z",
     "iopub.status.idle": "2025-11-30T11:05:38.713296Z",
     "shell.execute_reply": "2025-11-30T11:05:38.712027Z",
     "shell.execute_reply.started": "2025-11-30T11:03:01.534635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unsupervised Pre-training Script for NFL Player Trajectory Prediction\n",
    "\n",
    "This script performs unsupervised pre-training using LSTM autoencoders on all available\n",
    "NFL player sequences (both labeled and unlabeled). The pretrained encoder can then be\n",
    "used to initialize supervised models for better performance.\n",
    "\n",
    "Usage:\n",
    "    python unsupervised_pretraining.py --task autoencoder --epochs 50\n",
    "    python unsupervised_pretraining.py --task next_step --epochs 50\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import layers, metrics, models, losses\n",
    "\n",
    "\n",
    "\n",
    "# Add parent directory to path\n",
    "# sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "# from unsupervised_data_loader import UnsupervisedNFLDataLoader, UnsupervisedNFLSequence\n",
    "# from unsupervised_models import (\n",
    "#     LSTMAutoencoder, \n",
    "#     NextStepPredictor, \n",
    "#     create_training_callbacks\n",
    "# )\n",
    "\n",
    "\n",
    "def train_autoencoder(train_seq, val_seq, epochs=100, latent_dim=128, model_save_path='autoencoder.keras'):\n",
    "    \"\"\"Train LSTM autoencoder for representation learning.\n",
    "    \n",
    "    Args:\n",
    "        train_seq: Training data sequence\n",
    "        val_seq: Validation data sequence\n",
    "        epochs: Number of training epochs\n",
    "        latent_dim: Dimension of latent space\n",
    "        model_save_path: Path to save the trained model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING LSTM AUTOENCODER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get input shape from first batch\n",
    "    x_sample, _ = train_seq[0]\n",
    "    input_shape = (x_sample.shape[1], x_sample.shape[2])\n",
    "    \n",
    "    print(f\"\\nInput shape: {input_shape}\")\n",
    "    print(f\"Latent dimension: {latent_dim}\")\n",
    "    \n",
    "    # Build autoencoder\n",
    "    # ae = LSTMAutoencoder(\n",
    "    #     input_shape=input_shape,\n",
    "    #     latent_dim=latent_dim,\n",
    "    #     lstm_units=[512, 256, 128, 64, 32]\n",
    "    # )\n",
    "    # ae.build_autoencoder()\n",
    "    # ae.compile(learning_rate=0.0001)\n",
    "    \n",
    "    # print(\"\\n\" + \"-\"*70)\n",
    "    # ae.get_summary()\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks = create_training_callbacks(model_save_path, patience=10)\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    pretrained_ae = keras.models.load_model('/kaggle/working/best_hyperband_unsupervised_model_second_run.keras')\n",
    "    \n",
    "    pretrained_ae.summary()\n",
    "\n",
    "    cosine_decay = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.0005780335476190155,\n",
    "    decay_steps=140000,\n",
    "    alpha=1e-5,\n",
    "    )\n",
    "\n",
    "    pretrained_ae.compile(\n",
    "        optimizer=keras.optimizers.AdamW(learning_rate=cosine_decay, global_clipnorm=1.0),\n",
    "        loss=losses.MeanSquaredError(),\n",
    "        metrics=[metrics.MeanSquaredError(), metrics.MeanAbsoluteError()]\n",
    "    )\n",
    "    \n",
    "\n",
    "    history = pretrained_ae.fit(\n",
    "        train_seq,\n",
    "        validation_data=val_seq,\n",
    "        epochs=epochs,\n",
    "        shuffle=False,\n",
    "        initial_epoch=20,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "    print(f\"Model saved to: {model_save_path}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Save encoder separately\n",
    "    encoder_path = model_save_path.replace('.keras', '_encoder.keras')\n",
    "    ae.encoder.save(encoder_path)\n",
    "    print(f\"Encoder saved to: {encoder_path}\")\n",
    "    \n",
    "    return ae, history\n",
    "\n",
    "\n",
    "def train_next_step_predictor(train_seq, val_seq, epochs=50, prediction_steps=5, \n",
    "                               model_save_path='next_step_predictor.keras'):\n",
    "    \"\"\"Train next-step predictor for self-supervised learning.\n",
    "    \n",
    "    Args:\n",
    "        train_seq: Training data sequence\n",
    "        val_seq: Validation data sequence\n",
    "        epochs: Number of training epochs\n",
    "        prediction_steps: Number of steps to predict ahead\n",
    "        model_save_path: Path to save the trained model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING NEXT-STEP PREDICTOR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get input shape from first batch\n",
    "    x_sample, y_sample = train_seq[0]\n",
    "    input_shape = (x_sample.shape[1], x_sample.shape[2])\n",
    "    output_features = y_sample.shape[2]\n",
    "    \n",
    "    print(f\"\\nInput shape: {input_shape}\")\n",
    "    print(f\"Output steps: {prediction_steps}\")\n",
    "    print(f\"Output features: {output_features}\")\n",
    "    \n",
    "    # Build model\n",
    "    predictor = NextStepPredictor(\n",
    "        input_shape=input_shape,\n",
    "        output_steps=prediction_steps,\n",
    "        lstm_units=[256, 128],\n",
    "        output_features=output_features\n",
    "    )\n",
    "    predictor.build()\n",
    "    predictor.compile(learning_rate=0.001)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    predictor.get_summary()\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks = create_training_callbacks(model_save_path, patience=10)\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    history = predictor.model.fit(\n",
    "        train_seq,\n",
    "        validation_data=val_seq,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "    print(f\"Model saved to: {model_save_path}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return predictor, history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    PREDICTION_TRAIN_DIR = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n",
    "    ANALYTICS_TRAIN_DIR = '/kaggle/input/nfl-big-data-bowl-2026-analytics/114239_nfl_competition_files_published_analytics_final/train'\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"UNSUPERVISED PRE-TRAINING FOR NFL PLAYER TRAJECTORY PREDICTION\")\n",
    "    print(\"=\"*70)\n",
    "    # print(f\"\\nTask: {args.task}\")\n",
    "    # print(f\"Epochs: {args.epochs}\")\n",
    "    # print(f\"Batch size: {args.batch_size}\")\n",
    "    # print(f\"Include labeled: {args.include_labeled}\")\n",
    "    # print(f\"Include unlabeled: {args.include_unlabeled}\")\n",
    "    # print(f\"Validation split: {args.val_split}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    loader = UnsupervisedNFLDataLoader()\n",
    "    loader.load_files(\n",
    "        [PREDICTION_TRAIN_DIR, ANALYTICS_TRAIN_DIR],\n",
    "        include_labeled=True,\n",
    "        include_unlabeled=True\n",
    "    )\n",
    "    X = loader.get_sequences()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"ERROR: No data loaded!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nTotal sequences loaded: {len(X)}\")\n",
    "    print(f\"Sample sequence length: {len(X[0])}\")\n",
    "    print(f\"Sample features: {len(X[0][0])}\")\n",
    "    \n",
    "    # Split into train/val\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_val = train_test_split(\n",
    "        X, \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining sequences: {len(X_train)}\")\n",
    "    print(f\"Validation sequences: {len(X_val)}\")\n",
    "    \n",
    "    # Create data sequences based on task\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CREATING DATA GENERATORS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    train_seq = UnsupervisedNFLSequence(\n",
    "        X_train,\n",
    "        batch_size=64,\n",
    "        maxlen=10,\n",
    "        shuffle=False,\n",
    "        task=\"autoencoder\",\n",
    "        prediction_steps=10\n",
    "    )\n",
    "    \n",
    "    val_seq = UnsupervisedNFLSequence(\n",
    "        X_val,\n",
    "        batch_size=64,\n",
    "        maxlen=10,\n",
    "        shuffle=False,\n",
    "        task=\"autoencoder\",\n",
    "        prediction_steps=10\n",
    "    )\n",
    "    \n",
    "    # Generate timestamp for model name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Train based on task\n",
    "    model_path = os.path.join(\"/kaggle/working/\", f'autoencoder_{timestamp}.keras')\n",
    "    model, history = train_autoencoder(\n",
    "        train_seq, \n",
    "        val_seq, \n",
    "        epochs=100,\n",
    "        latent_dim=256,\n",
    "        model_save_path=model_path\n",
    "    )\n",
    "    \n",
    "    # model_path = os.path.join(args.output_dir, f'next_step_{timestamp}.keras')\n",
    "    # model, history = train_next_step_predictor(\n",
    "    #     train_seq,\n",
    "    #     val_seq,\n",
    "    #     epochs=args.epochs,\n",
    "    #     prediction_steps=args.prediction_steps,\n",
    "    #     model_save_path=model_path\n",
    "    # )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "    print(f\"\\nModel saved to: {model_path}\")\n",
    "    \n",
    "    encoder_path = model_path.replace('.keras', '_encoder.keras')\n",
    "    print(f\"Encoder saved to: {encoder_path}\")\n",
    "    print(\"\\nTo use the pretrained encoder in your supervised model:\")\n",
    "    print(f\"  from tensorflow import keras\")\n",
    "    print(f\"  from unsupervised_models import transfer_encoder_weights\")\n",
    "    print(f\"  pretrained_encoder = keras.models.load_model('{encoder_path}')\")\n",
    "    print(f\"  supervised_model = transfer_encoder_weights(pretrained_encoder, supervised_model)\")\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"DONE!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised model fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Supervised Model Fine-tuning\n",
    "\n",
    "This cell implements the **second stage** of the two-stage training process: fine-tuning a supervised trajectory prediction model using pre-trained encoder weights.\n",
    "\n",
    "### Two-Stage Training Strategy:\n",
    "\n",
    "```\n",
    "Stage 1 (Unsupervised)          Stage 2 (Supervised)\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”           â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "ALL player sequences      â†’     Load pre-trained encoder\n",
    "        â†“                               â†“\n",
    "Autoencoder training            Initialize supervised model\n",
    "        â†“                               â†“\n",
    "Learn representations           Fine-tune on labeled data\n",
    "        â†“                               â†“\n",
    "Save encoder weights            Predict (x, y) trajectories\n",
    "```\n",
    "\n",
    "### Model Architecture:\n",
    "\n",
    "```python\n",
    "Input (10, 18)  # 10 timesteps, 18 features\n",
    "    â†“\n",
    "[Pre-trained Encoder Layers]  # Transferred from autoencoder\n",
    "    â†“\n",
    "Latent LSTM (256 units, return_sequences=True)\n",
    "    â†“\n",
    "TimeDistributed Dense(2)  # Output (x, y) coordinates\n",
    "    â†“\n",
    "Output (10, 2)  # 10 timesteps, 2 coordinates\n",
    "```\n",
    "\n",
    "### Transfer Learning Benefits:\n",
    "- âœ… Better initialization than random weights\n",
    "- âœ… Faster convergence on labeled data\n",
    "- âœ… Improved generalization from pre-training\n",
    "- âœ… Can leverage unlimited unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-01T05:51:34.035Z",
     "iopub.execute_input": "2025-12-01T05:39:12.436451Z",
     "iopub.status.busy": "2025-12-01T05:39:12.435645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, metrics, models, losses\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the manual_data_processing directory to the path\n",
    "# sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'manual_data_processing'))\n",
    "\n",
    "# from csv_to_numpy import NFLDataLoader, create_tf_datasets\n",
    "\n",
    "def build_seq2seq_model(input_seq_length, input_features, output_seq_length, output_features, lstm_units=256):\n",
    "    \"\"\"\n",
    "    Builds a sequence-to-sequence model with LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        input_seq_length (int): The length of input sequences (time steps).\n",
    "        input_features (int): The number of input features per timestep.\n",
    "        output_seq_length (int): The length of output sequences (time steps).\n",
    "        output_features (int): The number of output features per timestep.\n",
    "        lstm_units (int): The number of units in the LSTM layers.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "    dropout_rate = 0.2\n",
    "    SEED = 42\n",
    "\n",
    "    encoder_inputs = layers.Input(shape=(input_seq_length, input_features), name='encoder_inputs')\n",
    "    \n",
    "    enc_lstm_1 = layers.LSTM(lstm_units, return_sequences=True, name='enc_lstm_1')(encoder_inputs)\n",
    "    \n",
    "    enc_lstm_2 = layers.LSTM(lstm_units, return_sequences=True, name='enc_lstm_2')(enc_lstm_1)\n",
    "    \n",
    "    enc_lstm_3 = layers.LSTM(lstm_units, return_sequences=True, name='enc_lstm_3')(enc_lstm_2)\n",
    "    \n",
    "    enc_lstm_4 = layers.LSTM(lstm_units, return_sequences=True, name='enc_lstm_4')(enc_lstm_3)\n",
    "    \n",
    "    enc_lstm_5 = layers.LSTM(lstm_units, return_sequences=True, name='enc_lstm_5')(enc_lstm_4)\n",
    "    \n",
    "    latent = layers.LSTM(lstm_units, return_sequences=True, name='latent')(enc_lstm_5)\n",
    "\n",
    "    outputs = layers.TimeDistributed(\n",
    "        layers.Dense(2, activation='linear'),\n",
    "        name='trajectory_output'\n",
    "    )(latent)\n",
    "    \n",
    "    model = models.Model(inputs=encoder_inputs, outputs=outputs, name='encoder')\n",
    "\n",
    "    cosine_decay = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.003054455019851368,\n",
    "    decay_steps=11500,\n",
    "    alpha=1e-5,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.AdamW(learning_rate=cosine_decay, global_clipnorm=1.0),\n",
    "        loss=losses.MeanSquaredError(),\n",
    "        metrics=[metrics.MeanSquaredError(), metrics.MeanAbsoluteError()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, train_sequence, val_sequence, epochs=100, callbacks=None):\n",
    "    \"\"\"\n",
    "    Trains the Keras model using Keras Sequence objects.\n",
    "    \n",
    "    Args:\n",
    "        model: The Keras model to train\n",
    "        train_sequence: Training data sequence (NFLDataSequence)\n",
    "        val_sequence: Validation data sequence (NFLDataSequence)\n",
    "        epochs (int): Number of training epochs\n",
    "        callbacks: List of Keras callbacks\n",
    "    \n",
    "    Returns:\n",
    "        history: Training history object\n",
    "    \"\"\"\n",
    "    pretrained_encoder = keras.models.load_model('/kaggle/working/best_hyperband_unsupervised_model_fifth_run.keras')\n",
    "    pretrained_encoder.summary()\n",
    "    supervised_model = transfer_encoder_weights(pretrained_encoder, model)\n",
    "    print(\"pretrained encoder\")\n",
    "    pretrained_encoder.summary()\n",
    "    print(\"supervised model\")\n",
    "    supervised_model.summary()\n",
    "    if callbacks is None:\n",
    "        callbacks = []\n",
    "    \n",
    "    # Add early stopping and model checkpoint callbacks\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    callbacks.extend([early_stopping, model_checkpoint])\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    history = supervised_model.fit(\n",
    "        train_sequence,\n",
    "        epochs=epochs,\n",
    "        initial_epoch=24,\n",
    "        shuffle=False,\n",
    "        validation_data=val_sequence,\n",
    "        callbacks=model_checkpoint,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Model training finished.\")\n",
    "    return history\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data, build, and train the model.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    train_dir = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "    test_size = 0.2\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"NFL Big Data Bowl 2026 - Predictor Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n[1/4] Loading data from CSV files...\")\n",
    "    loader = NFLDataLoader(train_dir)\n",
    "    X, y = loader.get_aligned_data()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No data loaded. Please check the data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  Total sequences: {len(X)}\")\n",
    "    print(f\"  Sample input sequence length: {len(X[0])}\")\n",
    "    print(f\"  Sample output sequence length: {len(y[0])}\")\n",
    "    print(f\"  Input features per timestep: {len(X[0][0]) if len(X[0]) > 0 else 0}\")\n",
    "    print(f\"  Output features per timestep: {len(y[0][0]) if len(y[0]) > 0 else 0}\")\n",
    "    \n",
    "    # Create Keras Sequences with padding\n",
    "    print(f\"\\n[2/4] Creating training and validation sequences (test_size={test_size})...\")\n",
    "    train_seq, val_seq = create_tf_datasets(X, y, test_size=test_size, batch_size=batch_size)\n",
    "    \n",
    "    if train_seq is None:\n",
    "        print(\"Error: Failed to create training sequences.\")\n",
    "        return\n",
    "    \n",
    "    # Get one batch to determine shapes\n",
    "    x_sample, y_sample = train_seq[0]\n",
    "    input_seq_length = x_sample.shape[1]\n",
    "    input_features = x_sample.shape[2]\n",
    "    output_seq_length = y_sample.shape[1]\n",
    "    output_features = y_sample.shape[2]\n",
    "    \n",
    "    print(f\"\\nSequence Shapes:\")\n",
    "    print(f\"  Input: (batch_size, {input_seq_length}, {input_features})\")\n",
    "    print(f\"  Output: (batch_size, {output_seq_length}, {output_features})\")\n",
    "    \n",
    "    # Build model\n",
    "    print(f\"\\n[3/4] Building sequence-to-sequence model...\")\n",
    "    model = build_seq2seq_model(\n",
    "        input_seq_length=input_seq_length,\n",
    "        input_features=input_features,\n",
    "        output_seq_length=output_seq_length,\n",
    "        output_features=output_features,\n",
    "        lstm_units=256\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\n[4/4] Training model for {epochs} epochs...\")\n",
    "    history = train_model(model, train_seq, val_seq, epochs=epochs)\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = 'nfl_predictor_final.keras'\n",
    "    model.save(final_model_path)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Final model saved to: {final_model_path}\")\n",
    "    print(f\"Best model saved to: best_model.keras\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Print training summary\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"  Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"  Final training MAE: {history.history['mae'][-1]:.4f}\")\n",
    "    print(f\"  Final validation MAE: {history.history['val_mae'][-1]:.4f}\")\n",
    "    print(f\"  Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Supervised Data Loader\n",
    "\n",
    "This cell defines the data loading infrastructure for **supervised training** with labeled trajectory data.\n",
    "\n",
    "### Key Differences from Unsupervised Loader:\n",
    "\n",
    "| Aspect | Unsupervised | Supervised |\n",
    "|--------|--------------|------------|\n",
    "| Data Scope | ALL players | `player_to_predict=True` only |\n",
    "| Target | Same sequence (reconstruction) | Future (x, y) coordinates |\n",
    "| Files Used | Input files only | Input AND Output files |\n",
    "| Alignment | Not required | Input â†” Output alignment required |\n",
    "\n",
    "### Data Processing Pipeline:\n",
    "\n",
    "```\n",
    "Input CSV Files              Output CSV Files\n",
    "      â†“                            â†“\n",
    "  Filter by                   Extract (x, y)\n",
    "player_to_predict                  â†“\n",
    "      â†“                            â†“\n",
    "Feature Processing           Group by keys\n",
    "      â†“                            â†“\n",
    "Group by keys           â†â”€â”€ Inner Join â”€â”€â†’\n",
    "      â†“\n",
    "Aligned (X, y) pairs\n",
    "      â†“\n",
    "NFLDataSequence (Keras)\n",
    "```\n",
    "\n",
    "### Feature Engineering:\n",
    "- Birth date â†’ Player age (years)\n",
    "- Boolean strings â†’ 0.0/1.0\n",
    "- Categorical strings â†’ Hash values\n",
    "- All features â†’ Float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T05:24:22.844285Z",
     "iopub.status.busy": "2025-12-01T05:24:22.843560Z",
     "iopub.status.idle": "2025-12-01T05:24:56.925349Z",
     "shell.execute_reply": "2025-12-01T05:24:56.924297Z",
     "shell.execute_reply.started": "2025-12-01T05:24:22.844259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class NFLDataLoader:\n",
    "    \"\"\"\n",
    "    Loads and processes NFL Big Data Bowl 2026 data from CSV files using Polars.\n",
    "    Filters input data for 'player_to_predict' == True and aligns with output data.\n",
    "    \n",
    "    Selected Input Features: ['x', 'y', 's', 'a', 'dir', 'o']\n",
    "    Selected Output Features: ['x', 'y']\n",
    "    \"\"\"\n",
    "    def __init__(self, train_dir):\n",
    "        self.train_dir = train_dir\n",
    "        self.input_df = None\n",
    "        self.output_df = None\n",
    "        self.stats = {}\n",
    "\n",
    "    def process_features(self, df):\n",
    "        \"\"\"\n",
    "        Processes features using Polars expressions.\n",
    "        Converts strings/booleans to floats and hashes categorical strings.\n",
    "        \"\"\"\n",
    "        # Define ID columns to exclude from feature processing\n",
    "        id_cols = ['game_id', 'play_id', 'nfl_id', 'frame_id', 'player_to_predict', 'time']\n",
    "        \n",
    "        # Identify feature columns\n",
    "        feature_cols = [col for col in df.columns if col not in id_cols]\n",
    "        \n",
    "        expressions = []\n",
    "        for col in feature_cols:\n",
    "            # Check if column is string type\n",
    "            if df[col].dtype == pl.Utf8:\n",
    "                expr = (\n",
    "                    pl.when(pl.col(col).str.to_lowercase() == \"true\").then(1.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"false\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"left\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"right\").then(1.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"defense\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"offense\").then(1.0)\n",
    "                    .otherwise(\n",
    "                        # Try to cast to float, if fails (null), use hash\n",
    "                        # Use seed=42 for deterministic hashing\n",
    "                        pl.col(col).cast(pl.Float64, strict=False).fill_null(\n",
    "                            pl.col(col).hash(seed=42).mod(10000).cast(pl.Float64)\n",
    "                        )\n",
    "                    ).cast(pl.Float64).alias(col)\n",
    "                )\n",
    "                expressions.append(expr)\n",
    "            else:\n",
    "                # Cast numeric columns to Float64\n",
    "                expressions.append(pl.col(col).cast(pl.Float64).alias(col))\n",
    "                \n",
    "        return expressions\n",
    "\n",
    "    def load_input_files(self):\n",
    "        \"\"\"\n",
    "        Loads and filters input CSV files using Polars.\n",
    "        Filters for 'player_to_predict' == True.\n",
    "        \"\"\"\n",
    "        input_files = sorted([f for f in os.listdir(self.train_dir) if f.startswith('input') and f.endswith('.csv')])\n",
    "        print(f\"Loading and filtering {len(input_files)} Input files...\")\n",
    "        \n",
    "        dfs = []\n",
    "        for f in input_files:\n",
    "            try:\n",
    "                path = os.path.join(self.train_dir, f)\n",
    "                # Read CSV\n",
    "                df = pl.read_csv(path, infer_schema_length=10000)\n",
    "                \n",
    "                # Filter for player_to_predict == True\n",
    "                if \"player_to_predict\" in df.columns:\n",
    "                    if df[\"player_to_predict\"].dtype == pl.Boolean:\n",
    "                        df = df.filter(pl.col(\"player_to_predict\") == True)\n",
    "                    else:\n",
    "                        df = df.filter(pl.col(\"player_to_predict\").cast(pl.Utf8).str.to_lowercase() == \"true\")\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {f}: {e}\")\n",
    "                \n",
    "        if not dfs:\n",
    "            print(\"No input data found.\")\n",
    "            self.input_df = pl.DataFrame()\n",
    "            return\n",
    "\n",
    "        # Concatenate all input dataframes\n",
    "        self.input_df = pl.concat(dfs, how=\"vertical_relaxed\")\n",
    "        \n",
    "        # Process features\n",
    "        print(\"Processing input features...\")\n",
    "        feature_exprs = self.process_features(self.input_df)\n",
    "        \n",
    "        # Keep ID columns and processed features\n",
    "        id_cols = ['game_id', 'play_id', 'nfl_id', 'frame_id']\n",
    "        # Ensure we only select columns that exist\n",
    "        existing_id_cols = [c for c in id_cols if c in self.input_df.columns]\n",
    "        \n",
    "        self.input_df = self.input_df.select(\n",
    "            [pl.col(c) for c in existing_id_cols] + feature_exprs\n",
    "        )\n",
    "\n",
    "    def load_output_files(self):\n",
    "        \"\"\"\n",
    "        Loads output CSV files using Polars.\n",
    "        Selects specific features: ['x', 'y'].\n",
    "        \"\"\"\n",
    "        output_files = sorted([f for f in os.listdir(self.train_dir) if f.startswith('output') and f.endswith('.csv')])\n",
    "        print(f\"Loading {len(output_files)} Output files...\")\n",
    "        \n",
    "        dfs = []\n",
    "        features_to_keep = ['x', 'y']\n",
    "        \n",
    "        for f in output_files:\n",
    "            try:\n",
    "                path = os.path.join(self.train_dir, f)\n",
    "                df = pl.read_csv(path, infer_schema_length=10000)\n",
    "                \n",
    "                # Select IDs and target features\n",
    "                cols_to_select = ['game_id', 'play_id', 'nfl_id', 'frame_id'] + features_to_keep\n",
    "                # Ensure all columns exist\n",
    "                existing_cols = [c for c in cols_to_select if c in df.columns]\n",
    "                \n",
    "                if len(existing_cols) == len(cols_to_select):\n",
    "                    dfs.append(df.select(existing_cols))\n",
    "                else:\n",
    "                    print(f\"Missing columns in {f}. Found: {existing_cols}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {f}: {e}\")\n",
    "\n",
    "        if not dfs:\n",
    "            print(\"No output data found.\")\n",
    "            self.output_df = pl.DataFrame()\n",
    "            return\n",
    "\n",
    "        self.output_df = pl.concat(dfs, how=\"vertical_relaxed\")\n",
    "        \n",
    "        # Cast targets to float\n",
    "        self.output_df = self.output_df.with_columns([\n",
    "            pl.col(c).cast(pl.Float64) for c in features_to_keep\n",
    "        ])\n",
    "\n",
    "    def normalize_data(self, df, feature_cols):\n",
    "        \"\"\"\n",
    "        Normalizes the dataframe features (Z-score).\n",
    "        \"\"\"\n",
    "        print(\"Normalizing features...\")\n",
    "        stats = {}\n",
    "        exprs = []\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            mean = df.select(pl.col(col).mean()).item()\n",
    "            std = df.select(pl.col(col).std()).item()\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            if std == 0 or std is None:\n",
    "                std = 1.0\n",
    "            if mean is None:\n",
    "                mean = 0.0\n",
    "                \n",
    "            stats[col] = {'mean': mean, 'std': std}\n",
    "            exprs.append(((pl.col(col) - mean) / std).alias(col))\n",
    "            \n",
    "        return df.with_columns(exprs), stats\n",
    "\n",
    "    def get_aligned_data(self, normalize=False):\n",
    "        \"\"\"\n",
    "        Aligns input and output sequences and returns NumPy arrays.\n",
    "        Returns:\n",
    "            X (np.ndarray): Input sequences\n",
    "            y (np.ndarray): Output sequences\n",
    "        \"\"\"\n",
    "        self.load_input_files()\n",
    "        self.load_output_files()\n",
    "        \n",
    "        if self.input_df is None or self.input_df.is_empty() or \\\n",
    "           self.output_df is None or self.output_df.is_empty():\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "        print(\"Aligning Input and Output sequences...\")\n",
    "        \n",
    "        # Rename output columns to avoid collision with input columns\n",
    "        output_features = ['x', 'y']\n",
    "        rename_map = {c: f\"target_{c}\" for c in output_features}\n",
    "        self.output_df = self.output_df.rename(rename_map)\n",
    "        \n",
    "        # Join input and output on IDs\n",
    "        joined_df = self.input_df.join(\n",
    "            self.output_df,\n",
    "            on=['game_id', 'play_id', 'nfl_id', 'frame_id'],\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        # Sort by frame_id to ensure temporal order\n",
    "        joined_df = joined_df.sort(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "        \n",
    "        # Identify feature columns (from input) and target columns (from output)\n",
    "        id_cols = {'game_id', 'play_id', 'nfl_id', 'frame_id'}\n",
    "        input_cols = [c for c in self.input_df.columns if c not in id_cols]\n",
    "        target_cols = [f\"target_{c}\" for c in output_features]\n",
    "        \n",
    "        # Normalize if requested\n",
    "        if normalize:\n",
    "            # Normalize input features\n",
    "            normalized_df, self.stats = self.normalize_data(joined_df, input_cols)\n",
    "            joined_df = normalized_df\n",
    "            \n",
    "        # Group by sequence\n",
    "        print(\"Grouping sequences...\")\n",
    "        \n",
    "        # Aggregation expressions\n",
    "        input_agg = [pl.col(c) for c in input_cols]\n",
    "        target_agg = [pl.col(c) for c in target_cols]\n",
    "        \n",
    "        sequences_df = joined_df.group_by(['game_id', 'play_id', 'nfl_id'], maintain_order=True).agg(\n",
    "            input_agg + target_agg\n",
    "        )\n",
    "        \n",
    "        print(f\"Total Unique Sequences: {len(sequences_df)}\")\n",
    "        \n",
    "        # Convert to NumPy object arrays\n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        \n",
    "        # Iterate rows and stack features\n",
    "        rows = sequences_df.iter_rows(named=True)\n",
    "        for row in rows:\n",
    "            # Check sequence length\n",
    "            seq_len = len(row[input_cols[0]])\n",
    "            if seq_len == 0:\n",
    "                continue\n",
    "                \n",
    "            # Stack features: (seq_len, n_features)\n",
    "            input_seq = np.column_stack([row[c] for c in input_cols])\n",
    "            target_seq = np.column_stack([row[c] for c in target_cols])\n",
    "            \n",
    "            X_list.append(input_seq)\n",
    "            y_list.append(target_seq)\n",
    "            \n",
    "        X = np.array(X_list, dtype=object)\n",
    "        y = np.array(y_list, dtype=object)\n",
    "        \n",
    "        print(f\"Initial X shape: {X.shape}\")\n",
    "        print(f\"Initial y shape: {y.shape}\")\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "\n",
    "class NFLDataSequence(Sequence):\n",
    "    \"\"\"\n",
    "    Keras Sequence for NFL data with automatic padding of variable-length sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size=64, maxlen_x=10, maxlen_y=10, shuffle=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (list/array): Input sequences (list of 2D arrays)\n",
    "            y (list/array): Output sequences (list of 2D arrays)\n",
    "            batch_size (int): Batch size\n",
    "            maxlen_x (int, optional): Maximum length for input sequences.\n",
    "            maxlen_y (int, optional): Maximum length for output sequences.\n",
    "            shuffle (bool): Whether to shuffle data at the end of each epoch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        \n",
    "        # Determine max lengths if not provided\n",
    "        if maxlen_x is None:\n",
    "            self.maxlen_x = max(len(seq) for seq in X)\n",
    "        else:\n",
    "            self.maxlen_x = maxlen_x\n",
    "            \n",
    "        if maxlen_y is None:\n",
    "            self.maxlen_y = max(len(seq) for seq in y)\n",
    "        else:\n",
    "            self.maxlen_y = maxlen_y\n",
    "        \n",
    "        print(f\"NFLDataSequence initialized: {len(self.X)} samples, batch_size={batch_size}\")\n",
    "        print(f\"Max sequence lengths - X: {self.maxlen_x}, y: {self.maxlen_y}\")\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches per epoch\"\"\"\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generate one batch of data\n",
    "        \"\"\"\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        # Get batch data\n",
    "        batch_X = [self.X[i] for i in batch_indices]\n",
    "        batch_y = [self.y[i] for i in batch_indices]\n",
    "        \n",
    "        # Pad sequences\n",
    "        # pad_sequences handles list of 2D arrays correctly\n",
    "        X_padded = pad_sequences(\n",
    "            batch_X, \n",
    "            maxlen=self.maxlen_x, \n",
    "            dtype='float32',\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0.0\n",
    "        )\n",
    "        \n",
    "        y_padded = pad_sequences(\n",
    "            batch_y,\n",
    "            maxlen=self.maxlen_y,\n",
    "            dtype='float32',\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0.0\n",
    "        )\n",
    "        \n",
    "        return X_padded, y_padded\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffle indices after each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "def create_tf_datasets(X, y, test_size=0.2, batch_size=64, maxlen_x=10, maxlen_y=10):\n",
    "    \"\"\"\n",
    "    Splits X and y into training and validation sets and creates Keras Sequence datasets.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Keras Sequence Datasets with Padding ---\")\n",
    "    \n",
    "    try:\n",
    "        # Convert object arrays to lists for splitting\n",
    "        X_list = X.tolist()\n",
    "        y_list = y.tolist()\n",
    "        \n",
    "        # Split into train and validation\n",
    "        print(f\"Splitting data (test_size={test_size})...\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_list, y_list, \n",
    "            test_size=test_size, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"Train size: {len(X_train)}\")\n",
    "        print(f\"Val size: {len(X_val)}\")\n",
    "        \n",
    "        # Create Sequence objects\n",
    "        print(\"Creating Training Sequence...\")\n",
    "        train_sequence = NFLDataSequence(\n",
    "            X_train, y_train, \n",
    "            batch_size=batch_size,\n",
    "            maxlen_x=maxlen_x,\n",
    "            maxlen_y=maxlen_y,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        print(\"Creating Validation Sequence...\")\n",
    "        val_sequence = NFLDataSequence(\n",
    "            X_val, y_val,\n",
    "            batch_size=batch_size,\n",
    "            maxlen_x=train_sequence.maxlen_x,  # Use same max lengths as training\n",
    "            maxlen_y=train_sequence.maxlen_y,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        print(\"Sequences created successfully.\")\n",
    "        print(f\"Training batches per epoch: {len(train_sequence)}\")\n",
    "        print(f\"Validation batches per epoch: {len(val_sequence)}\")\n",
    "        \n",
    "        return train_sequence, val_sequence\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Keras sequences: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TRAIN_DIR = '/home/samer/Desktop/competitions/NFL_Big_Data_Bowl_2026_dev/nfl-big-data-bowl-2026-prediction/train/'\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(TRAIN_DIR):\n",
    "        print(f\"Warning: Directory {TRAIN_DIR} does not exist. Using current directory for testing.\")\n",
    "        TRAIN_DIR = \".\"\n",
    "\n",
    "    loader = NFLDataLoader(TRAIN_DIR)\n",
    "    # Enable normalization\n",
    "    X, y = loader.get_aligned_data(normalize=True)\n",
    "\n",
    "    print(\"\\n--- Final Data Shapes ---\")\n",
    "    print(f\"X (Input) Shape: {X.shape}\")\n",
    "    print(f\"y (Output) Shape: {y.shape}\")\n",
    "\n",
    "    if len(X) > 0:\n",
    "        print(f\"Sample Input Sequence Length: {len(X[0])}\")\n",
    "        print(f\"Sample Output Sequence Length: {len(y[0])}\")\n",
    "        print(f\"Input Features: {X[0].shape[1]}\")\n",
    "        print(f\"Output Features: {y[0].shape[1]}\")\n",
    "\n",
    "    # Create Keras Sequences with padding\n",
    "    if len(X) > 0:\n",
    "        train_seq, val_seq = create_tf_datasets(X, y, batch_size=32)\n",
    "        \n",
    "        if train_seq:\n",
    "            print(\"\\nVerifying Sequence Element:\")\n",
    "            # Get one batch to verify shapes\n",
    "            x_batch, y_batch = train_seq[0]\n",
    "            print(f\"Batch X shape: {x_batch.shape}\")\n",
    "            print(f\"Batch y shape: {y_batch.shape}\")\n",
    "            print(f\"Max sequence lengths - X: {train_seq.maxlen_x}, y: {train_seq.maxlen_y}\")\n",
    "\n",
    "    print(\"\\nData loading, alignment, and sequence creation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Hyperparameter Tuning with Keras Tuner\n",
    "\n",
    "This cell implements **Hyperband tuning** to find optimal model hyperparameters.\n",
    "\n",
    "### Hyperband Algorithm:\n",
    "\n",
    "Hyperband is an efficient hyperparameter optimization algorithm that:\n",
    "1. Samples many configurations with small budgets\n",
    "2. Progressively allocates more resources to promising configurations\n",
    "3. Eliminates poorly performing configurations early\n",
    "\n",
    "### Tunable Hyperparameters:\n",
    "\n",
    "| Parameter | Search Space | Purpose |\n",
    "|-----------|--------------|---------|\n",
    "| `lstm_units` | [64, 128, 256, 512] | Encoder capacity |\n",
    "| `num_layers` | [2, 3, 4, 5, 6] | Model depth |\n",
    "| `learning_rate` | [1e-4, 1e-2] (log) | Optimization speed |\n",
    "| `dropout` | [0.1, 0.5] | Regularization strength |\n",
    "\n",
    "### Tuner Configuration:\n",
    "\n",
    "```python\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',      # Minimize validation loss\n",
    "    max_epochs=50,             # Maximum epochs per trial\n",
    "    factor=3,                  # Reduction factor per bracket\n",
    "    directory='tuner_results', # Save trial results\n",
    "    project_name='nfl_tuning'  # Project identifier\n",
    ")\n",
    "```\n",
    "\n",
    "### Best Practices:\n",
    "- âš ï¸ Run on GPU for faster iteration\n",
    "- ðŸ“Š Monitor TensorBoard for training curves\n",
    "- ðŸ’¾ Save best model configuration for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-26T04:52:12.825Z",
     "iopub.execute_input": "2025-11-26T04:32:10.631764Z",
     "iopub.status.busy": "2025-11-26T04:32:10.631345Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import sys\n",
    "import keras_tuner\n",
    "\n",
    "# Add the manual_data_processing directory to the path\n",
    "# sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'manual_data_processing'))\n",
    "\n",
    "# from csv_to_numpy import NFLDataLoader, create_tf_datasets\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Builds a compiled Keras LSTM model with hyperparameters to be experimented on.\n",
    "\n",
    "    This function defines the architecture of the LSTM model for sequence-to-sequence prediction.\n",
    "    It incorporates hyperparameter search spaces for key model parameters like learning rate,\n",
    "    number of LSTM units, kernel regularization, and activation functions.\n",
    "\n",
    "    Args:\n",
    "        hp (keras_tuner.HyperParameters): An instance of Keras Tuner's HyperParameters class,\n",
    "                                          used to define the search space for hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Keras LSTM model with hyperparameters set by Keras Tuner.\n",
    "    \"\"\"\n",
    "    k_init = keras.initializers.RandomNormal(mean=1503.17, \n",
    "    stddev=2755.38, \n",
    "    seed=42)\n",
    "    SEED = 42\n",
    "    # Define hyperparameter search spaces for tuning\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-7, max_value=1e-3, sampling=\"log\")\n",
    "    layer_u = hp.Int(\"lu\", min_value=160, max_value=512, step=8)\n",
    "    kernel_r = hp.Float(\"kr\", min_value=1e-10, max_value=1e-5, sampling=\"log\")\n",
    "    acti_f = hp.Choice(\"af\", [\"relu\", \"selu\"])\n",
    "    last_acti_f = hp.Choice(\"laf\", [\"sigmoid\", \"linear\"])\n",
    "    weight_d = hp.Float(\"wd\", min_value=1e-10, max_value=0.0009, sampling=\"log\")\n",
    "\n",
    "    # Define the model structure using Keras Sequential API\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        keras.layers.Input(shape=(input_seq_length, input_features)),\n",
    "        \n",
    "        # Encoder LSTM layers\n",
    "        keras.layers.LSTM(\n",
    "            units=layer_u,\n",
    "            return_sequences=True,\n",
    "            activation=acti_f,\n",
    "            kernel_regularizer=keras.regularizers.l2(kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=layer_u,\n",
    "            return_sequences=True,\n",
    "            activation=acti_f,\n",
    "            kernel_regularizer=keras.regularizers.l2(kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=layer_u,  \n",
    "            return_sequences=True,\n",
    "            activation=acti_f,\n",
    "            kernel_regularizer=keras.regularizers.l2(kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=layer_u,\n",
    "            return_sequences=True,\n",
    "            activation=acti_f,\n",
    "            kernel_regularizer=keras.regularizers.l2(kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=layer_u,\n",
    "            return_sequences=True,\n",
    "            activation=acti_f,\n",
    "            kernel_regularizer=keras.regularizers.l2(kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        # Crop or slice to match output sequence length\n",
    "        # layers.Lambda(lambda x: x[:, :output_seq_length, :]),\n",
    "        # TimeDistributed dense layer for output features\n",
    "        layers.TimeDistributed(\n",
    "            keras.layers.Dense(units=32, activation=last_acti_f)\n",
    "        ),\n",
    "        layers.TimeDistributed(\n",
    "            keras.layers.Dense(units=output_features, activation=last_acti_f)\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # Compile the model with a tunable optimizer and metrics\n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            global_clipnorm=1,\n",
    "            amsgrad=False,\n",
    "            weight_decay=weight_d, # Tunable weight decay\n",
    "        ),\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError(), tf.keras.metrics.MeanSquaredError(), tf.keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def experimenting(training_dataset, validation_data):\n",
    "    \"\"\"\n",
    "    Runs Keras Tuner experiments for the LSTM model using the RandomSearch algorithm.\n",
    "\n",
    "    This function initializes a `RandomSearch` tuner with the `build_model` function,\n",
    "    configures the search objective (minimizing validation loss), and then executes\n",
    "    the hyperparameter search across the defined search spaces. It prints summaries\n",
    "    of the search space and the results.\n",
    "\n",
    "    Args:\n",
    "        training_dataset: NFLDataSequence object for training data\n",
    "        validation_data: NFLDataSequence object for validation data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    hp = keras_tuner.HyperParameters()\n",
    "    \n",
    "    # Get a batch from the sequence to determine shapes\n",
    "    x_batch, y_batch = training_dataset[0]\n",
    "    global input_features, input_seq_length, output_seq_length, output_features\n",
    "    input_seq_length =1# x_batch.shape[1]\n",
    "    input_features = x_batch.shape[2]\n",
    "    output_seq_length = 1#y_batch.shape[1]\n",
    "    output_features = y_batch.shape[2]\n",
    "    \n",
    "    print(f\"\\nDetected shapes:\")\n",
    "    print(f\"  Input: ({input_seq_length}, {input_features})\")\n",
    "    print(f\"  Output: ({output_seq_length}, {output_features})\")\n",
    "    \n",
    "    build_model(hp) # Instantiate a dummy model to build the search space\n",
    "\n",
    "    # Initialize Keras Tuner's RandomSearch algorithm\n",
    "    tuner = keras_tuner.RandomSearch(\n",
    "        hypermodel=build_model,\n",
    "        max_trials=20, # Maximum number of hyperparameter combinations to try\n",
    "        objective=keras_tuner.Objective(\"val_root_mean_squared_error\", \"min\"),   # Objective is to minimize validation loss\n",
    "        executions_per_trial=1, # Number of models to train for each trial (1 for efficiency)\n",
    "        overwrite=True, # Overwrite previous results in the directory\n",
    "        directory=os.getenv(\"KERAS_TUNER_EXPERIMENTS_DIR\", \"./tuner_results\"), # Directory to save experiment logs and checkpoints\n",
    "        project_name=\"nfl_prediction\", # Name of the Keras Tuner project\n",
    "        seed = 42,\n",
    "        max_consecutive_failed_trials=5,\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary() # Print a summary of the hyperparameter search space\n",
    "\n",
    "    # NFLDataSequence is already batched, no need to call batch() again\n",
    "    # Run the hyperparameter search experiments\n",
    "    tuner.search(\n",
    "        training_dataset, \n",
    "        validation_data=validation_data, \n",
    "        epochs=5\n",
    "    )\n",
    "\n",
    "    tuner.results_summary() # Print a summary of the best performing trials\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dir = '/home/samer/Desktop/competitions/NFL_Big_Data_Bowl_2026_dev/nfl-big-data-bowl-2026-prediction/train'\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    test_size = 0.2\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"NFL Big Data Bowl 2026 - Predictor Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n[1/4] Loading data from CSV files...\")\n",
    "    loader = NFLDataLoader(train_dir)\n",
    "    X, y = loader.get_aligned_data()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No data loaded. Please check the data directory.\")\n",
    "    \n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  Total sequences: {len(X)}\")\n",
    "    print(f\"  Sample input sequence length: {len(X[0])}\")\n",
    "    print(f\"  Sample output sequence length: {len(y[0])}\")\n",
    "    print(f\"  Input features per timestep: {len(X[0][0]) if len(X[0]) > 0 else 0}\")\n",
    "    print(f\"  Output features per timestep: {len(y[0][0]) if len(y[0]) > 0 else 0}\")\n",
    "    \n",
    "    # Create Keras Sequences with padding\n",
    "    print(f\"\\n[2/4] Creating training and validation sequences (test_size={test_size})...\")\n",
    "    train_seq, val_seq = create_tf_datasets(X, y, test_size=test_size, batch_size=batch_size)\n",
    "    \n",
    "    # Run the hyperparameter experimentation\n",
    "    experimenting(train_seq, val_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Hyperband training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T16:03:01.483648Z",
     "iopub.status.busy": "2025-11-30T16:03:01.483049Z",
     "iopub.status.idle": "2025-11-30T16:03:01.494182Z",
     "shell.execute_reply": "2025-11-30T16:03:01.493360Z",
     "shell.execute_reply.started": "2025-11-30T16:03:01.483614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers, losses, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_seq2seq_model(hp, \n",
    "                        input_seq_length, input_features,\n",
    "                        output_seq_length, output_features):\n",
    "    \"\"\"\n",
    "    Returns a compiled Keras model.\n",
    "    hp â€“ HyperParameters object supplied by Kerasâ€‘Tuner.\n",
    "    \"\"\"\n",
    "    # ---------- Hyperâ€‘parameters ----------\n",
    "    # Number of LSTM layers (encoder + decoder)\n",
    "    n_encoder_layers = hp.Int('enc_layers', 2, 6, step=1)\n",
    "    n_decoder_layers = hp.Int('dec_layers', 2, 6, step=1)\n",
    "\n",
    "    # LSTM units per layer (same for all layers for simplicity)\n",
    "    lstm_units = hp.Choice('lstm_units', [64, 128, 256, 384, 512])\n",
    "\n",
    "    # Kernel regularizer\n",
    "    # kernel_reg = hp.Float('kernel_reg', 1e-8, 1e-4, sampling='log')\n",
    "    \n",
    "    # Dropout rate\n",
    "    dropout_rate = hp.Float('dropout', 0.0, 0.3, step=0.05)\n",
    "\n",
    "    # Learningâ€‘rate schedule\n",
    "    init_lr = hp.Float('init_lr', 1e-5, 5e-3, sampling='log')\n",
    "    \n",
    "    # ---------- Model ----------\n",
    "    # Encoder\n",
    "    encoder_inputs = layers.Input(shape=(input_seq_length, input_features),\n",
    "                                  name='encoder_inputs')\n",
    "    x = encoder_inputs\n",
    "    for i in range(n_encoder_layers):\n",
    "        # Residual LSTM block\n",
    "        lstm_out = layers.LSTM(lstm_units,\n",
    "                               return_sequences=True,\n",
    "                               seed=42,\n",
    "                               dropout=dropout_rate,\n",
    "                               # kernel_regularizer=kernel_reg,\n",
    "                               name=f'enc_lstm_{i+1}')(x)\n",
    "        # lstm_out = layers.Dropout(dropout_rate,\n",
    "        #                           name=f'enc_dropout_{i+1}')(lstm_out)\n",
    "        # # Add residual connection (if dimensions match)\n",
    "        # if lstm_out.shape[-1] == x.shape[-1]:\n",
    "        #     lstm_out = layers.Add(name=f'enc_res_{i+1}')([x, lstm_out])\n",
    "        # # Normalise\n",
    "        # lstm_out = layers.LayerNormalization(name=f'enc_norm_{i+1}')(lstm_out)\n",
    "        x = lstm_out\n",
    "\n",
    "    # Grab the final hidden state as the latent vector\n",
    "    latent = layers.LSTM(lstm_units,\n",
    "                         return_sequences=False,\n",
    "                         seed=42,\n",
    "                         name='latent')(x)\n",
    "\n",
    "    # Decoder â€“ repeat latent vector for each output timestep\n",
    "    decoder_inputs = layers.RepeatVector(output_seq_length,\n",
    "                                         name='repeat_latent')(latent)\n",
    "    y = decoder_inputs\n",
    "    for i in range(n_decoder_layers):\n",
    "        lstm_out = layers.LSTM(lstm_units,\n",
    "                               return_sequences=True,\n",
    "                               seed=42,\n",
    "                               dropout=dropout_rate,\n",
    "                               # kernel_regularizer=kernel_reg,\n",
    "                               name=f'dec_lstm_{i+1}')(y)\n",
    "        # lstm_out = layers.Dropout(dropout_rate,\n",
    "        #                           name=f'dec_dropout_{i+1}')(lstm_out)\n",
    "        # # Residual connection (again only when shapes match)\n",
    "        # if lstm_out.shape[-1] == y.shape[-1]:\n",
    "        #     lstm_out = layers.Add(name=f'dec_res_{i+1}')([y, lstm_out])\n",
    "        # lstm_out = layers.LayerNormalization(name=f'dec_norm_{i+1}')(lstm_out)\n",
    "        y = lstm_out\n",
    "\n",
    "    # Final TimeDistributed dense layer\n",
    "    decoder_outputs = layers.TimeDistributed(\n",
    "        layers.Dense(output_features, activation='linear'),\n",
    "        name='decoder_output')(y)\n",
    "\n",
    "    model = models.Model(inputs=encoder_inputs, outputs=decoder_outputs,\n",
    "                         name='tunable_seq2seq')\n",
    "\n",
    "    # ---------- Learningâ€‘rate schedule ----------\n",
    "    # Simplified to just CosineDecay to avoid TypeError\n",
    "    total_steps = hp.Int('total_steps', 1, 100000, step=100)\n",
    "    learning_rate = optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=total_steps,\n",
    "        alpha=1e-5)\n",
    "\n",
    "    optimizer = optimizers.AdamW(learning_rate=learning_rate, global_clipnorm=1.0)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=losses.MeanSquaredError(),\n",
    "                  metrics=[metrics.MeanSquaredError(), metrics.MeanAbsoluteError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Hyperband Search Execution\n",
    "\n",
    "This cell defines the `tuner_search` function that orchestrates the Hyperband hyperparameter optimization process.\n",
    "\n",
    "### Function Overview:\n",
    "\n",
    "**Purpose:** Automates the search for optimal model hyperparameters using Keras Tuner's Hyperband algorithm.\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Define Hypermodel**: Wraps `build_seq2seq_model` with hyperparameter choices\n",
    "2. **Initialize Tuner**: Creates Hyperband tuner with search configuration\n",
    "3. **Execute Search**: Runs trials with early stopping\n",
    "4. **Extract Best Model**: Retrieves the top-performing configuration\n",
    "5. **Retrain**: Trains the best model for full epochs\n",
    "\n",
    "### Hyperband Configuration:\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `max_trials` | 30 | Total number of configurations to try |\n",
    "| `epochs_per_trial` | 5 | Initial epochs for each trial |\n",
    "| `factor` | 3 | Reduction factor between brackets |\n",
    "| `objective` | 'val_loss' | Metric to minimize |\n",
    "\n",
    "### Early Stopping:\n",
    "```python\n",
    "EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,           # Stop if no improvement for 3 epochs\n",
    "    restore_best_weights=True\n",
    ")\n",
    "```\n",
    "\n",
    "**Output:** Returns the best model and its training history for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T16:03:05.524842Z",
     "iopub.status.busy": "2025-11-30T16:03:05.524279Z",
     "iopub.status.idle": "2025-11-30T16:03:05.531624Z",
     "shell.execute_reply": "2025-11-30T16:03:05.530847Z",
     "shell.execute_reply.started": "2025-11-30T16:03:05.524817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "def tuner_search(train_seq, val_seq,\n",
    "                 input_seq_len, input_feat,\n",
    "                 output_seq_len, output_feat,\n",
    "                 max_trials=30, epochs_per_trial=5):\n",
    "    \"\"\"\n",
    "    Runs Hyperband and returns the best model + history.\n",
    "    \"\"\"\n",
    "    # Define the hypermodel function\n",
    "    def make_model(hp):\n",
    "        return build_seq2seq_model(\n",
    "            hp,\n",
    "            input_seq_length=input_seq_len,\n",
    "            input_features=input_feat,\n",
    "            output_seq_length=output_seq_len,\n",
    "            output_features=output_feat)\n",
    "    \n",
    "    # Check for distribution strategy\n",
    "    if 'strategy' in globals() and strategy is not None:\n",
    "        print(f\"Using distribution strategy: {strategy}\")\n",
    "        distribution = strategy\n",
    "    else:\n",
    "        print(\"No distribution strategy found, using default.\")\n",
    "        distribution = tf.distribute.get_strategy()\n",
    "        \n",
    "    # Initialize tuner with distribution strategy\n",
    "    tuner = kt.Hyperband(\n",
    "        hypermodel=make_model,\n",
    "        objective='val_loss',\n",
    "        max_epochs=epochs_per_trial,\n",
    "        factor=3,\n",
    "        directory='kt_tuner',\n",
    "        project_name='nfl_seq2seq',\n",
    "        overwrite=True,\n",
    "        distribution_strategy=distribution)\n",
    "\n",
    "    # Early-stopping inside each trial\n",
    "    stop_early = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=3,\n",
    "                                         restore_best_weights=True)\n",
    "\n",
    "    tuner.search(train_seq,\n",
    "                 validation_data=val_seq,\n",
    "                 callbacks=[stop_early],\n",
    "                 verbose=1)\n",
    "\n",
    "    # Retrieve the best hyper-parameters & model\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    # Train the best model a little longer (optional)\n",
    "    # Ensure training also uses the strategy if needed (model is already compiled with it)\n",
    "    final_history = best_model.fit(\n",
    "        train_seq,\n",
    "        validation_data=val_seq,\n",
    "        epochs=epochs_per_trial * 2,   # give it more epochs now that we know the arch.\n",
    "        callbacks=[callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                           patience=5,\n",
    "                                           restore_best_weights=True)],\n",
    "        verbose=1)\n",
    "\n",
    "    return best_model, final_history, best_hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Encoder Extraction Utility\n",
    "\n",
    "This cell defines a utility function to extract and save the encoder portion of a trained seq2seq model.\n",
    "\n",
    "### Purpose:\n",
    "\n",
    "The `save_encoder_from_model` function extracts the encoder layers from a complete autoencoder or seq2seq model, enabling:\n",
    "- **Transfer Learning**: Use pre-trained encoder in new models\n",
    "- **Feature Extraction**: Extract latent representations from sequences\n",
    "- **Model Deployment**: Deploy only the encoder for inference\n",
    "\n",
    "### Extraction Process:\n",
    "\n",
    "```\n",
    "Full Model                    Extracted Encoder\n",
    "â”â”â”â”â”â”â”â”â”â”                    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "Input Layer                   Input Layer\n",
    "    â†“                             â†“\n",
    "Encoder LSTMs    â”€â”€â”€â”€â”€â”€â”€â”€â†’    Encoder LSTMs\n",
    "    â†“                             â†“\n",
    "Latent Layer                  Latent Layer (output)\n",
    "    â†“                             \n",
    "Decoder LSTMs    (discarded)\n",
    "    â†“\n",
    "Output Layer     (discarded)\n",
    "```\n",
    "\n",
    "### Usage Example:\n",
    "```python\n",
    "# After training\n",
    "trained_model = keras.models.load_model('autoencoder.keras')\n",
    "\n",
    "# Extract encoder\n",
    "save_encoder_from_model(trained_model, 'encoder.keras')\n",
    "\n",
    "# Use in new model\n",
    "encoder = keras.models.load_model('encoder.keras')\n",
    "```\n",
    "\n",
    "**Error Handling:** Gracefully handles cases where the 'latent' layer is not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T16:03:09.621636Z",
     "iopub.status.busy": "2025-11-30T16:03:09.621353Z",
     "iopub.status.idle": "2025-11-30T18:39:07.193903Z",
     "shell.execute_reply": "2025-11-30T18:39:07.193284Z",
     "shell.execute_reply.started": "2025-11-30T16:03:09.621616Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_encoder_from_model(model, path):\n",
    "    \"\"\"\n",
    "    Extracts and saves the encoder part of the seq2seq model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The encoder input is the model input\n",
    "        encoder_inputs = model.input\n",
    "        \n",
    "        # The latent vector is the output of the layer named 'latent'\n",
    "        latent_layer = model.get_layer('latent')\n",
    "        latent_output = latent_layer.output\n",
    "        \n",
    "        # Create encoder model\n",
    "        encoder_model = models.Model(inputs=encoder_inputs, outputs=latent_output, name='encoder')\n",
    "        \n",
    "        # Save\n",
    "        encoder_model.save(path)\n",
    "        print(f\"Encoder model saved to {path}\")\n",
    "        return encoder_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving encoder: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1ï¸âƒ£  Load Unsupervised Data & Prepare Sequences\n",
    "    # ------------------------------------------------------------------\n",
    "    PREDICTION_TRAIN_DIR = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n",
    "    ANALYTICS_TRAIN_DIR = '/kaggle/input/nfl-big-data-bowl-2026-analytics/114239_nfl_competition_files_published_analytics_final/train'\n",
    "\n",
    "    print(\"Loading unsupervised data...\")\n",
    "    # Initialize loader\n",
    "    loader = UnsupervisedNFLDataLoader()\n",
    "    # Load from both directories\n",
    "    loader.load_files(\n",
    "        [PREDICTION_TRAIN_DIR, ANALYTICS_TRAIN_DIR],\n",
    "        include_labeled=True,\n",
    "        include_unlabeled=True,\n",
    "        normalize=True,\n",
    "    )\n",
    "    X_unsupervised = loader.get_sequences()\n",
    "\n",
    "    if len(X_unsupervised) == 0:\n",
    "        print(\"ERROR: No unsupervised data loaded!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Total unsupervised sequences: {len(X_unsupervised)}\")\n",
    "    \n",
    "    # Split into train/val\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val = train_test_split(X_unsupervised, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create sequences for Next-Step Prediction (Self-Supervised)\n",
    "    task = 'autoencoder'\n",
    "    prediction_steps = 10  # Predict next 5 steps\n",
    "    \n",
    "    train_seq = UnsupervisedNFLSequence(\n",
    "        X_train,\n",
    "        batch_size=64,\n",
    "        maxlen=10, \n",
    "        shuffle=False,\n",
    "        task=task,\n",
    "        prediction_steps=prediction_steps\n",
    "    )\n",
    "    \n",
    "    val_seq = UnsupervisedNFLSequence(\n",
    "        X_val,\n",
    "        batch_size=64,\n",
    "        maxlen=10,\n",
    "        shuffle=False,\n",
    "        task=task,\n",
    "        prediction_steps=prediction_steps\n",
    "    )\n",
    "\n",
    "    # Get shapes from a batch to configure the model\n",
    "    x_batch, y_batch = train_seq[0]\n",
    "    input_seq_len = x_batch.shape[1]\n",
    "    input_feat = x_batch.shape[2]\n",
    "    print(x_batch.shape)\n",
    "    output_seq_len = y_batch.shape[1]\n",
    "    output_feat = y_batch.shape[2]\n",
    "    print(y_batch.shape)\n",
    "    print(f\"Input shape: ({input_seq_len}, {input_feat})\")\n",
    "    print(f\"Output shape: ({output_seq_len}, {output_feat})\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2ï¸âƒ£  Launch the tuner\n",
    "    # ------------------------------------------------------------------\n",
    "    best_model, best_history, best_hp = tuner_search(\n",
    "            train_seq=train_seq,\n",
    "            val_seq=val_seq,\n",
    "            input_seq_len=input_seq_len,\n",
    "            input_feat=input_feat,\n",
    "            output_seq_len=output_seq_len,\n",
    "            output_feat=output_feat,\n",
    "            max_trials=30,          # increase if you have more time\n",
    "            epochs_per_trial=12)    # short trials for speed\n",
    "\n",
    "    print(\"\\n=== Best hyperâ€‘parameters ===\")\n",
    "    for name, value in best_hp.values.items():\n",
    "        print(f\"{name}: {value}\")\n",
    "        \n",
    "    # Save best model\n",
    "    best_model.save('best_hyperband_unsupervised_model_third_run.keras')\n",
    "    print(\"Best model saved to best_hyperband_unsupervised_model.keras\")\n",
    "\n",
    "    # Save encoder separately\n",
    "    save_encoder_from_model(best_model, 'best_hyperband_encoder_third_run.keras')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Training Results Visualization\n",
    "\n",
    "This cell creates a visualization of the training and validation loss curves for the best model found by Hyperband.\n",
    "\n",
    "### Visualization Purpose:\n",
    "\n",
    "**Loss Curves** help diagnose model training behavior:\n",
    "\n",
    "| Pattern | Interpretation | Action |\n",
    "|---------|---------------|--------|\n",
    "| Both decreasing | âœ… Good training | Continue |\n",
    "| Val loss plateaus | âš ï¸ Convergence reached | Can stop training |\n",
    "| Val loss increases | âŒ Overfitting | Reduce epochs or add regularization |\n",
    "| Gap between curves | âš ï¸ Possible overfitting | Increase dropout or data augmentation |\n",
    "\n",
    "### Plot Components:\n",
    "\n",
    "- **Training Loss** (blue): Performance on training data\n",
    "- **Validation Loss** (orange): Performance on held-out validation data\n",
    "- **Grid**: Easier to read exact values\n",
    "- **Legend**: Identifies each curve\n",
    "\n",
    "### Interpreting Results:\n",
    "\n",
    "**Ideal Pattern:**\n",
    "```\n",
    "Loss\n",
    " â”‚\n",
    " â”‚  â•²\n",
    " â”‚   â•²  â† Both curves decreasing\n",
    " â”‚    â•²â•²\n",
    " â”‚     â•²â•²\n",
    " â”‚      â•²â•²\n",
    " â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epochs\n",
    "```\n",
    "\n",
    "**Overfitting Pattern:**\n",
    "```\n",
    "Loss\n",
    " â”‚      â•± â† Val loss increasing\n",
    " â”‚  â•²  â•±\n",
    " â”‚   â•²â•±  â† Training loss still decreasing\n",
    " â”‚    â•²\n",
    " â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epochs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(best_history.history['loss'], label='Training loss')\n",
    "plt.plot(best_history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Best model â€“ Training & Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Supervised Training Pipeline (Main Execution)\n",
    "\n",
    "This cell implements the complete supervised training workflow, integrating all components for trajectory prediction.\n",
    "\n",
    "### Complete Training Pipeline:\n",
    "\n",
    "```\n",
    "1. Data Loading\n",
    "   â†“\n",
    "2. Data Preprocessing & Alignment\n",
    "   â†“\n",
    "3. Train/Val Split\n",
    "   â†“\n",
    "4. Model Building\n",
    "   â†“\n",
    "5. Transfer Learning (Load Pre-trained Encoder)\n",
    "   â†“\n",
    "6. Model Compilation\n",
    "   â†“\n",
    "7. Training with Callbacks\n",
    "   â†“\n",
    "8. Model Evaluation\n",
    "   â†“\n",
    "9. Save Final Model\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "#### Data Configuration:\n",
    "- **Input Directory**: `/kaggle/input/nfl-big-data-bowl-2026-prediction/train`\n",
    "- **Batch Size**: 64 (optimized for memory and convergence)\n",
    "- **Train/Val Split**: 80/20\n",
    "\n",
    "#### Model Configuration:\n",
    "- **Architecture**: Seq2Seq with pre-trained encoder\n",
    "- **LSTM Units**: 256 (from hyperparameter tuning)\n",
    "- **Optimizer**: AdamW with CosineDecay schedule\n",
    "- **Loss**: Mean Squared Error (MSE)\n",
    "- **Metrics**: MSE, MAE\n",
    "\n",
    "#### Training Configuration:\n",
    "- **Epochs**: 100 (with early stopping)\n",
    "- **Initial Epoch**: 24 (resuming from checkpoint)\n",
    "- **Callbacks**: \n",
    "  - EarlyStopping (patience=5)\n",
    "  - ModelCheckpoint (save best model)\n",
    "  - Learning rate schedule\n",
    "\n",
    "### Output Artifacts:\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `best_model.keras` | Best model based on val_loss |\n",
    "| `nfl_predictor_final.keras` | Final model after all epochs |\n",
    "\n",
    "### Transfer Learning Integration:\n",
    "\n",
    "The pipeline loads pre-trained encoder weights from:\n",
    "```python\n",
    "'/kaggle/working/best_hyperband_unsupervised_model_fifth_run.keras'\n",
    "```\n",
    "\n",
    "This enables the model to start with learned representations instead of random initialization, significantly improving convergence speed and final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, losses, metrics\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the manual_data_processing directory to the path\n",
    "# sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'manual_data_processing'))\n",
    "\n",
    "# from csv_to_numpy import NFLDataLoader, create_tf_datasets\n",
    "\n",
    "def build_seq2seq_model(input_seq_length, input_features, output_seq_length, output_features, lstm_units=256):\n",
    "    \"\"\"\n",
    "    Builds a sequence-to-sequence model with LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        input_seq_length (int): The length of input sequences (time steps).\n",
    "        input_features (int): The number of input features per timestep.\n",
    "        output_seq_length (int): The length of output sequences (time steps).\n",
    "        output_features (int): The number of output features per timestep.\n",
    "        lstm_units (int): The number of units in the LSTM layers.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = 42\n",
    "    # Encoder-decoder architecture for sequence-to-sequence prediction\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        keras.layers.Input(shape=(input_seq_length, input_features)),\n",
    "        \n",
    "        # Encoder LSTM layers\n",
    "        keras.layers.LSTM(\n",
    "            units=192,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.l2(6.7545425231694045e-06),\n",
    "            activation=\"relu\",\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=192,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.l2(6.7545425231694045e-06),\n",
    "            activation=\"relu\",\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=192,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.l2(6.7545425231694045e-06),\n",
    "            activation=\"relu\",\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=192,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.l2(6.7545425231694045e-06),\n",
    "            activation=\"relu\",\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=192,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.l2(6.7545425231694045e-06),\n",
    "            activation=\"relu\",\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        # Crop or slice to match output sequence length\n",
    "        # layers.Lambda(lambda x: x[:, :output_seq_length, :]),\n",
    "        # TimeDistributed dense layer for output features\n",
    "        layers.TimeDistributed(\n",
    "            keras.layers.Dense(units=32, activation=\"linear\")\n",
    "        ),\n",
    "        layers.TimeDistributed(\n",
    "            keras.layers.Dense(units=output_features, activation=\"linear\")\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    cosine_decay = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=415000,\n",
    "    alpha=1e-6,\n",
    "    )\n",
    "\n",
    "    starter_learning_rate = 0.0001\n",
    "    end_learning_rate = 0.000001\n",
    "    decay_steps = 10000\n",
    "    learning_rate_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "        starter_learning_rate,\n",
    "        decay_steps,\n",
    "        end_learning_rate,\n",
    "        power=0.5)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.AdamW(learning_rate=learning_rate_fn, global_clipnorm=1.0, weight_decay=3.6227649655164867e-09),\n",
    "                  loss=losses.MeanSquaredError(),\n",
    "                  metrics=[metrics.MeanSquaredError(), metrics.MeanAbsoluteError(), metrics.RootMeanSquaredError()]\n",
    "                  )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, train_sequence, val_sequence, epochs=100, callbacks=None):\n",
    "    \"\"\"\n",
    "    Trains the Keras model using Keras Sequence objects.\n",
    "    \n",
    "    Args:\n",
    "        model: The Keras model to train\n",
    "        train_sequence: Training data sequence (NFLDataSequence)\n",
    "        val_sequence: Validation data sequence (NFLDataSequence)\n",
    "        epochs (int): Number of training epochs\n",
    "        callbacks: List of Keras callbacks\n",
    "    \n",
    "    Returns:\n",
    "        history: Training history object\n",
    "    \"\"\"\n",
    "    if callbacks is None:\n",
    "        callbacks = []\n",
    "    \n",
    "    # Add early stopping and model checkpoint callbacks\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_root_mean_squared_error',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_root_mean_squared_error',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    callbacks.extend([early_stopping, model_checkpoint])\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    history = model.fit(\n",
    "        train_sequence,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_sequence,\n",
    "        callbacks=model_checkpoint,\n",
    "        verbose=1\n",
    "    )\n",
    "    # -------------------------------------------------\n",
    "    # Visualize training & validation loss\n",
    "    # -------------------------------------------------\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history.history['loss'],      label='Training loss')\n",
    "    plt.plot(history.history['val_loss'],  label='Validation loss')\n",
    "    plt.title('Training & Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"Model training finished.\")\n",
    "    return history\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data, build, and train the model.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    train_dir = '/home/samer/Desktop/competitions/NFL_Big_Data_Bowl_2026_dev/nfl-big-data-bowl-2026-prediction/train'\n",
    "    batch_size = 64\n",
    "    epochs = 2000\n",
    "    test_size = 0.2\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"NFL Big Data Bowl 2026 - Predictor Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n[1/4] Loading data from CSV files...\")\n",
    "    loader = NFLDataLoader(train_dir)\n",
    "    X, y = loader.get_aligned_data()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No data loaded. Please check the data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  Total sequences: {len(X)}\")\n",
    "    print(f\"  Sample input sequence length: {len(X[0])}\")\n",
    "    print(f\"  Sample output sequence length: {len(y[0])}\")\n",
    "    print(f\"  Input features per timestep: {len(X[0][0]) if len(X[0]) > 0 else 0}\")\n",
    "    print(f\"  Output features per timestep: {len(y[0][0]) if len(y[0]) > 0 else 0}\")\n",
    "    \n",
    "    # Create Keras Sequences with padding\n",
    "    print(f\"\\n[2/4] Creating training and validation sequences (test_size={test_size})...\")\n",
    "    train_seq, val_seq = create_tf_datasets(X, y, test_size=test_size, batch_size=batch_size)\n",
    "    \n",
    "    if train_seq is None:\n",
    "        print(\"Error: Failed to create training sequences.\")\n",
    "        return\n",
    "    \n",
    "    # Get one batch to determine shapes\n",
    "    x_sample, y_sample = train_seq[0]\n",
    "    input_seq_length = x_sample.shape[1]\n",
    "    input_features = x_sample.shape[2]\n",
    "    output_seq_length = y_sample.shape[1]\n",
    "    output_features = y_sample.shape[2]\n",
    "    \n",
    "    print(f\"\\nSequence Shapes:\")\n",
    "    print(f\"  Input: (batch_size, {input_seq_length}, {input_features})\")\n",
    "    print(f\"  Output: (batch_size, {output_seq_length}, {output_features})\")\n",
    "    \n",
    "    # Build model\n",
    "    print(f\"\\n[3/4] Building sequence-to-sequence model...\")\n",
    "    model = build_seq2seq_model(\n",
    "        input_seq_length=input_seq_length,\n",
    "        input_features=input_features,\n",
    "        output_seq_length=output_seq_length,\n",
    "        output_features=output_features,\n",
    "        lstm_units=256,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\n[4/4] Training model for {epochs} epochs...\")\n",
    "    history = train_model(model, train_seq, val_seq, epochs=epochs)\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = 'nfl_predictor_final.keras'\n",
    "    model.save(final_model_path)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Final model saved to: {final_model_path}\")\n",
    "    print(f\"Best model saved to: best_model.keras\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Print training summary\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"  Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    # print(f\"  Final training MAE: {history.history['mae'][-1]:.4f}\")\n",
    "    # print(f\"  Final validation MAE: {history.history['val_mae'][-1]:.4f}\")\n",
    "    print(f\"  Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14210809,
     "sourceId": 114239,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 13838823,
     "sourceId": 114250,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
