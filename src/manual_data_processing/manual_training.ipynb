{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:09:59.205232Z",
     "iopub.status.busy": "2025-11-24T18:09:59.204377Z",
     "iopub.status.idle": "2025-11-24T18:10:58.638918Z",
     "shell.execute_reply": "2025-11-24T18:10:58.638163Z",
     "shell.execute_reply.started": "2025-11-24T18:09:59.205196Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and filtering 18 Input files...\n",
      "Loading 18 Output files...\n",
      "Aligning Input and Output sequences...\n",
      "Processing complete.\n",
      "Total Unique Sequences (Matches): 46045\n",
      "Converting to NumPy arrays...\n",
      "Initial X shape: (46045,)\n",
      "Initial y shape: (46045,)\n",
      "\n",
      "--- Final Data Shapes ---\n",
      "X (Input) Shape: (46045,)\n",
      "y (Output) Shape: (46045,)\n",
      "Sample Input Sequence Length: 26\n",
      "Sample Output Sequence Length: 21\n",
      "\n",
      "--- Creating Keras Sequence Datasets with Padding ---\n",
      "Splitting data (test_size=0.2)...\n",
      "Train size: 36836\n",
      "Val size: 9209\n",
      "Creating Training Sequence...\n",
      "NFLDataSequence initialized: 36836 samples, batch_size=32\n",
      "Max sequence lengths - X: 10, y: 10\n",
      "Creating Validation Sequence...\n",
      "NFLDataSequence initialized: 9209 samples, batch_size=32\n",
      "Max sequence lengths - X: 10, y: 10\n",
      "Sequences created successfully.\n",
      "Training batches per epoch: 1152\n",
      "Validation batches per epoch: 288\n",
      "\n",
      "Verifying Sequence Element:\n",
      "Batch X shape: (32, 10, 18)\n",
      "Batch y shape: (32, 10, 2)\n",
      "Max sequence lengths - X: 10, y: 10\n",
      "\n",
      "Data loading, alignment, and sequence creation complete.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class NFLDataLoader:\n",
    "    \"\"\"Loads and processes NFL Big Data Bowl 2026 data from CSV files using Polars.\n",
    "\n",
    "    This class handles the loading of input and output CSV files, filtering for\n",
    "    specific players, and aligning input sequences with their corresponding\n",
    "    output sequences based on game, play, and NFL IDs.\n",
    "\n",
    "    Attributes:\n",
    "        train_dir (str): The directory containing the training CSV files.\n",
    "        input_sequences (pl.DataFrame): DataFrame containing input sequences.\n",
    "        output_sequences (pl.DataFrame): DataFrame containing output sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_dir):\n",
    "        self.train_dir = train_dir\n",
    "        self.input_sequences = None\n",
    "        self.output_sequences = None\n",
    "\n",
    "    def load_input_files(self):\n",
    "        \"\"\"Loads and filters input CSV files from the training directory using Polars.\n",
    "\n",
    "        Iterates through files starting with 'input' and ending with '.csv'.\n",
    "        Filters rows where 'player_to_predict' is True and groups them by\n",
    "        (game_id, play_id, nfl_id) to form sequences.\n",
    "        \"\"\"\n",
    "        input_files = sorted([f for f in os.listdir(self.train_dir) if f.startswith('input') and f.endswith('.csv')])\n",
    "        print(f\"Loading and filtering {len(input_files)} Input files...\")\n",
    "        \n",
    "        dataframes = []\n",
    "        for input_file in input_files:\n",
    "            input_path = os.path.join(self.train_dir, input_file)\n",
    "            try:\n",
    "                # Lazy load for efficiency, though read_csv is fine for smaller files\n",
    "                # Using read_csv to ensure we catch errors immediately\n",
    "                df = pl.read_csv(input_path, infer_schema_length=10000)\n",
    "                \n",
    "                # Filter for player_to_predict == True (case insensitive)\n",
    "                if \"player_to_predict\" in df.columns:\n",
    "                    df = df.filter(\n",
    "                        pl.col(\"player_to_predict\").cast(pl.Utf8).str.to_lowercase() == \"true\"\n",
    "                    )\n",
    "                \n",
    "                if df.height > 0:\n",
    "                    dataframes.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {input_file}: {e}\")\n",
    "\n",
    "        if not dataframes:\n",
    "            print(\"No valid input data found.\")\n",
    "            self.input_sequences = pl.DataFrame()\n",
    "            return\n",
    "\n",
    "        # Concatenate all input dataframes\n",
    "        full_df = pl.concat(dataframes, how=\"vertical_relaxed\")\n",
    "\n",
    "        # Process columns (Vectorized)\n",
    "        # Handle Booleans, Directions, Sides, etc.\n",
    "        \n",
    "        # Helper expression for boolean strings\n",
    "        def to_bool_float(col_name):\n",
    "            return (\n",
    "                pl.when(pl.col(col_name).cast(pl.Utf8).str.to_lowercase() == \"true\").then(1.0)\n",
    "                .when(pl.col(col_name).cast(pl.Utf8).str.to_lowercase() == \"false\").then(0.0)\n",
    "                .otherwise(0.0) # Default or handle errors\n",
    "            )\n",
    "\n",
    "        # Helper for direction\n",
    "        def to_dir_float(col_name):\n",
    "            return (\n",
    "                pl.when(pl.col(col_name).cast(pl.Utf8).str.to_lowercase() == \"left\").then(0.0)\n",
    "                .when(pl.col(col_name).cast(pl.Utf8).str.to_lowercase() == \"right\").then(1.0)\n",
    "                .otherwise(0.0)\n",
    "            )\n",
    "\n",
    "        # Helper for side\n",
    "        def to_side_float(col_name):\n",
    "            return (\n",
    "                pl.when(pl.col(col_name).cast(pl.Utf8).str.to_lowercase() == \"defense\").then(0.0)\n",
    "                .when(pl.col(col_name).cast(pl.Utf8).str.to_lowercase() == \"offense\").then(1.0)\n",
    "                .otherwise(0.0)\n",
    "            )\n",
    "            \n",
    "        # Apply transformations\n",
    "        # We need to identify columns to transform. Based on previous code:\n",
    "        # Booleans: player_to_predict (already filtered, but maybe others?)\n",
    "        # Direction: play_direction? (Not explicitly named in previous code but handled in generic process_value)\n",
    "        # Side: player_side?\n",
    "        \n",
    "        # For generic handling, we can inspect types, but for performance, explicit is better.\n",
    "        # Let's assume standard columns or iterate if needed.\n",
    "        # The previous code iterated every cell. Here we want vectorization.\n",
    "        # We will cast all remaining columns to float, hashing strings if needed.\n",
    "        \n",
    "        # Identify ID columns to exclude from feature processing\n",
    "        id_cols = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"player_to_predict\", \"time\"]\n",
    "        feature_cols = [c for c in full_df.columns if c not in id_cols]\n",
    "        \n",
    "        expressions = []\n",
    "        for col in feature_cols:\n",
    "            # Check if column is string type\n",
    "            if full_df[col].dtype == pl.Utf8:\n",
    "                # Try specific conversions first\n",
    "                # We can't easily check content of every row efficiently without scanning\n",
    "                # So we apply a complex expression:\n",
    "                # If 'true'/'false' -> 1/0\n",
    "                # If 'left'/'right' -> 0/1\n",
    "                # If 'defense'/'offense' -> 0/1\n",
    "                # Else try cast float\n",
    "                # Else hash\n",
    "                \n",
    "                expr = (\n",
    "                    pl.when(pl.col(col).str.to_lowercase() == \"true\").then(1.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"false\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"left\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"right\").then(1.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"defense\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"offense\").then(1.0)\n",
    "                    .otherwise(\n",
    "                        # Try cast to float, if null (failed), then hash\n",
    "                        pl.col(col).cast(pl.Float64, strict=False).fill_null(\n",
    "                            pl.col(col).hash() % 10000\n",
    "                        )\n",
    "                    ).cast(pl.Float64).alias(col)\n",
    "                )\n",
    "                expressions.append(expr)\n",
    "            else:\n",
    "                # Already numeric (int or float), cast to float\n",
    "                expressions.append(pl.col(col).cast(pl.Float64).alias(col))\n",
    "\n",
    "        # Select IDs and processed features\n",
    "        full_df = full_df.with_columns(expressions)\n",
    "        \n",
    "        # Group by keys and aggregate into lists\n",
    "        # We assume the order is defined by frame_id or file order. \n",
    "        # If frame_id exists, sort by it.\n",
    "        if \"frame_id\" in full_df.columns:\n",
    "            full_df = full_df.sort([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n",
    "        \n",
    "        # Group and aggregate features into lists\n",
    "        # We want a list of lists (sequence of steps, where each step is a list of features)\n",
    "        # Polars agg_list creates a list of values for a column.\n",
    "        # We need to combine these columns into a single \"features\" column which is a list of lists?\n",
    "        # Or just keep them as separate columns of lists.\n",
    "        # The previous code produced: [[f1, f2, ...], [f1, f2, ...], ...] for each sequence.\n",
    "        \n",
    "        # Let's aggregate each feature column into a list\n",
    "        agg_exprs = [pl.col(c) for c in feature_cols]\n",
    "        \n",
    "        grouped = full_df.group_by([\"game_id\", \"play_id\", \"nfl_id\"], maintain_order=True).agg(agg_exprs)\n",
    "        \n",
    "        # Now we have:\n",
    "        # game_id, play_id, nfl_id, col1_list, col2_list, ...\n",
    "        # We need to transpose this to:\n",
    "        # game_id, play_id, nfl_id, [[col1_t0, col2_t0, ...], [col1_t1, col2_t1, ...]]\n",
    "        # This is hard in Polars directly.\n",
    "        # Easier: Convert to numpy/pandas later or iterate.\n",
    "        \n",
    "        # Actually, for Keras, we usually want (samples, timesteps, features).\n",
    "        # If we have separate columns of lists:\n",
    "        # col1: [t0, t1, t2]\n",
    "        # col2: [t0, t1, t2]\n",
    "        # We can stack them.\n",
    "        \n",
    "        self.input_sequences = grouped\n",
    "\n",
    "    def load_output_files(self):\n",
    "        \"\"\"Loads output CSV files from the training directory using Polars.\n",
    "\n",
    "        Iterates through files starting with 'output' and ending with '.csv'.\n",
    "        Extracts 'x' and 'y' features, grouping them by (game_id, play_id, nfl_id)\n",
    "        to form sequences.\n",
    "        \"\"\"\n",
    "        output_files = sorted([f for f in os.listdir(self.train_dir) if f.startswith('output') and f.endswith('.csv')])\n",
    "        print(f\"Loading {len(output_files)} Output files...\")\n",
    "        \n",
    "        features_to_keep = ['x', 'y']\n",
    "        dataframes = []\n",
    "        \n",
    "        for output_file in output_files:\n",
    "            output_path = os.path.join(self.train_dir, output_file)\n",
    "            try:\n",
    "                df = pl.read_csv(output_path, columns=['game_id', 'play_id', 'nfl_id'] + features_to_keep, infer_schema_length=10000)\n",
    "                dataframes.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {output_file}: {e}\")\n",
    "\n",
    "        if not dataframes:\n",
    "            print(\"No valid output data found.\")\n",
    "            self.output_sequences = pl.DataFrame()\n",
    "            return\n",
    "\n",
    "        full_df = pl.concat(dataframes, how=\"vertical_relaxed\")\n",
    "        \n",
    "        # Ensure float type\n",
    "        full_df = full_df.with_columns([\n",
    "            pl.col(c).cast(pl.Float64) for c in features_to_keep\n",
    "        ])\n",
    "        \n",
    "        # Sort if frame info is implicit (usually matches input)\n",
    "        # We don't have frame_id in output usually? Assuming same order.\n",
    "        # Ideally we should sort by something, but without frame_id we rely on file order.\n",
    "        \n",
    "        grouped = full_df.group_by([\"game_id\", \"play_id\", \"nfl_id\"], maintain_order=True).agg([\n",
    "            pl.col('x'),\n",
    "            pl.col('y')\n",
    "        ])\n",
    "        \n",
    "        self.output_sequences = grouped\n",
    "\n",
    "    def get_aligned_data(self):\n",
    "        \"\"\"Aligns input and output sequences based on common keys.\n",
    "\n",
    "        Loads both input and output files, finds the intersection of keys,\n",
    "        and creates aligned lists of sequences.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - X (np.ndarray): Array of input sequences (object array).\n",
    "                - y (np.ndarray): Array of output sequences (object array).\n",
    "        \"\"\"\n",
    "        self.load_input_files()\n",
    "        self.load_output_files()\n",
    "\n",
    "        print(\"Aligning Input and Output sequences...\")\n",
    "        \n",
    "        if self.input_sequences is None or self.input_sequences.is_empty():\n",
    "            print(\"Input sequences empty.\")\n",
    "            return np.array([]), np.array([])\n",
    "            \n",
    "        if self.output_sequences is None or self.output_sequences.is_empty():\n",
    "            print(\"Output sequences empty.\")\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "        # Join on keys\n",
    "        # Inner join to keep only matching sequences\n",
    "        joined = self.input_sequences.join(\n",
    "            self.output_sequences, \n",
    "            on=[\"game_id\", \"play_id\", \"nfl_id\"], \n",
    "            how=\"inner\",\n",
    "            suffix=\"_out\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Processing complete.\")\n",
    "        print(f\"Total Unique Sequences (Matches): {len(joined)}\")\n",
    "\n",
    "        if len(joined) == 0:\n",
    "            print(\"No matching data found.\")\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "        # Convert to the format expected by NFLDataSequence\n",
    "        # X: list of [ [f1, f2, ...], [f1, f2, ...] ]\n",
    "        # y: list of [ [x, y], [x, y] ... ]\n",
    "        \n",
    "        # The joined dataframe has columns:\n",
    "        # game_id, play_id, nfl_id, feat1_list, feat2_list, ..., x_list, y_list\n",
    "        \n",
    "        # We need to identify feature columns vs output columns\n",
    "        # Output columns are 'x' and 'y' (from output_sequences, might be renamed if collision)\n",
    "        # Actually, input also has 'x' and 'y' usually.\n",
    "        # In load_output_files, we aggregated 'x' and 'y'.\n",
    "        # In load_input_files, we aggregated all features.\n",
    "        # If input has 'x', 'y', they will collide.\n",
    "        # The join suffix=\"_out\" handles this. Output cols will be 'x_out', 'y_out'.\n",
    "        \n",
    "        # Input feature columns: all columns from input_sequences except keys\n",
    "        input_cols = [c for c in self.input_sequences.columns if c not in [\"game_id\", \"play_id\", \"nfl_id\"]]\n",
    "        output_cols = [\"x_out\" if \"x\" in input_cols else \"x\", \"y_out\" if \"y\" in input_cols else \"y\"]\n",
    "        \n",
    "        # Check if output cols exist\n",
    "        if output_cols[0] not in joined.columns:\n",
    "            # Maybe input didn't have x/y, so no suffix\n",
    "            output_cols = [\"x\", \"y\"]\n",
    "            \n",
    "        # Convert to numpy\n",
    "        # This is the heavy part.\n",
    "        # We can iterate rows or use map_elements?\n",
    "        # Ideally we want to stack the feature lists.\n",
    "        \n",
    "        # Let's extract input features as a list of arrays\n",
    "        # Each row i has [feat1_seq, feat2_seq, ...]\n",
    "        # We want [[feat1_t0, feat2_t0], [feat1_t1, feat2_t1], ...]\n",
    "        \n",
    "        # Efficient way:\n",
    "        # 1. Convert relevant columns to a dict of lists or similar\n",
    "        # 2. Iterate and stack\n",
    "        \n",
    "        print(\"Converting to NumPy arrays...\")\n",
    "        \n",
    "        # Extract input data\n",
    "        # shape: (n_samples, n_features, n_timesteps) roughly, but variable timesteps\n",
    "        # We want (n_samples, n_timesteps, n_features)\n",
    "        \n",
    "        # Get all input feature lists as a list of lists of lists?\n",
    "        # joined.select(input_cols).to_dict(as_series=False) gives {col: [seq1, seq2...]}\n",
    "        \n",
    "        # This might be memory intensive.\n",
    "        # Let's try row iteration with a generator or list comp\n",
    "        \n",
    "        # Pre-fetch column indices for speed\n",
    "        input_col_indices = [joined.columns.index(c) for c in input_cols]\n",
    "        output_col_indices = [joined.columns.index(c) for c in output_cols]\n",
    "        \n",
    "        rows = joined.iter_rows()\n",
    "        \n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        \n",
    "        for row in rows:\n",
    "            # Input\n",
    "            # row[i] is a list of values for feature i for this sequence\n",
    "            # We want to stack them: [[val_0_0, val_1_0...], [val_0_1, val_1_1...]]\n",
    "            # Zip is useful here\n",
    "            \n",
    "            # Get all feature sequences for this row\n",
    "            feature_seqs = [row[i] for i in input_col_indices]\n",
    "            # feature_seqs is [ [t0, t1...], [t0, t1...] ... ] (n_features, n_timesteps)\n",
    "            # We want (n_timesteps, n_features)\n",
    "            # zip(*feature_seqs) does exactly this transpose\n",
    "            \n",
    "            # Note: Polars lists might be None if empty? Assuming data is clean.\n",
    "            # Also assuming all feature lists have same length (they should if from same rows)\n",
    "            \n",
    "            X_seq = list(zip(*feature_seqs))\n",
    "            X_list.append(X_seq)\n",
    "            \n",
    "            # Output\n",
    "            out_seqs = [row[i] for i in output_col_indices]\n",
    "            y_seq = list(zip(*out_seqs))\n",
    "            y_list.append(y_seq)\n",
    "            \n",
    "        X = np.array(X_list, dtype=object)\n",
    "        y = np.array(y_list, dtype=object)\n",
    "        \n",
    "        print(f\"Initial X shape: {X.shape}\")\n",
    "        print(f\"Initial y shape: {y.shape}\")\n",
    "            \n",
    "        return X, y\n",
    "\n",
    "\n",
    "class NFLDataSequence(Sequence):\n",
    "    \"\"\"Keras Sequence for NFL data with automatic padding of variable-length sequences.\n",
    "\n",
    "    Inherits from `tensorflow.keras.utils.Sequence` to provide a data generator\n",
    "    that can be used with Keras models. Handles batching, shuffling, and\n",
    "    padding of sequences to a uniform length.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size=32, maxlen_x=None, maxlen_y=None, shuffle=True):\n",
    "        \"\"\"Initializes the NFLDataSequence.\n",
    "\n",
    "        Args:\n",
    "            X (list or np.ndarray): List of input sequences, where each sequence\n",
    "                is a list of time steps.\n",
    "            y (list or np.ndarray): List of output sequences, where each sequence\n",
    "                is a list of time steps.\n",
    "            batch_size (int, optional): Number of samples per batch. Defaults to 32.\n",
    "            maxlen_x (int, optional): Maximum length for input sequences. If None,\n",
    "                it is calculated from the data. Defaults to None.\n",
    "            maxlen_y (int, optional): Maximum length for output sequences. If None,\n",
    "                it is calculated from the data. Defaults to None.\n",
    "            shuffle (bool, optional): Whether to shuffle the data at the end of\n",
    "                each epoch. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        \n",
    "        # Determine max lengths if not provided\n",
    "        if maxlen_x is None:\n",
    "            self.maxlen_x = max(len(seq) for seq in X)\n",
    "        else:\n",
    "            self.maxlen_x = maxlen_x\n",
    "            \n",
    "        if maxlen_y is None:\n",
    "            self.maxlen_y = max(len(seq) for seq in y)\n",
    "        else:\n",
    "            self.maxlen_y = maxlen_y\n",
    "        \n",
    "        print(f\"NFLDataSequence initialized: {len(self.X)} samples, batch_size={batch_size}\")\n",
    "        print(f\"Max sequence lengths - X: {self.maxlen_x}, y: {self.maxlen_y}\")\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Computes the number of batches per epoch.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of batches.\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generates one batch of data.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the batch.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (X_padded, y_padded) containing the padded input and\n",
    "                output sequences for the batch.\n",
    "        \"\"\"\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        # Get batch data\n",
    "        batch_X = [self.X[i] for i in batch_indices]\n",
    "        batch_y = [self.y[i] for i in batch_indices]\n",
    "        \n",
    "        # Process X sequences: handle mixed types\n",
    "        # With Polars preprocessing, data should already be numeric floats\n",
    "        # But let's ensure it's a list of lists of floats\n",
    "        \n",
    "        # batch_X is a list of sequences. Each sequence is a list of frames. Each frame is a list of features.\n",
    "        # We need to convert this to a 3D numpy array or list of 2D arrays for pad_sequences\n",
    "        \n",
    "        # Since we did the conversion in get_aligned_data, batch_X elements should be lists of tuples/lists of floats.\n",
    "        # We can directly pass this to pad_sequences if they are numeric.\n",
    "        \n",
    "        # Use pad_sequences for both X and y\n",
    "        # pad_sequences expects sequences of shape (n_samples, n_timesteps) for 2D\n",
    "        # For 3D (n_samples, n_timesteps, n_features), we need to pad manually or use padding='post'\n",
    "        \n",
    "        # Method: Pad each sequence to maxlen, filling with zeros\n",
    "        X_padded = pad_sequences(\n",
    "            batch_X, \n",
    "            maxlen=self.maxlen_x, \n",
    "            dtype='float32',\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0.0\n",
    "        )\n",
    "        \n",
    "        y_padded = pad_sequences(\n",
    "            batch_y,\n",
    "            maxlen=self.maxlen_y,\n",
    "            dtype='float32',\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0.0\n",
    "        )\n",
    "        \n",
    "        return X_padded, y_padded\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch.\n",
    "\n",
    "        If `self.shuffle` is True, the data indices are shuffled to ensure\n",
    "        random batch composition in the next epoch.\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "def create_tf_datasets(X, y, test_size=0.2, batch_size=32, maxlen_x=10, maxlen_y=10):\n",
    "    \"\"\"Splits data into training and validation sets and creates Keras Sequence datasets.\n",
    "\n",
    "    Uses `train_test_split` to divide the data and then wraps the resulting\n",
    "    sets in `NFLDataSequence` objects, which handle padding and batching.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input data (object array of variable-length sequences).\n",
    "        y (np.ndarray): Output data (object array of variable-length sequences).\n",
    "        test_size (float, optional): Proportion of the dataset to include in the\n",
    "            validation split. Defaults to 0.2.\n",
    "        batch_size (int, optional): Batch size for the datasets. Defaults to 32.\n",
    "        maxlen_x (int, optional): Maximum length for input sequences. If None,\n",
    "            auto-detects from the training set. Defaults to 10.\n",
    "        maxlen_y (int, optional): Maximum length for output sequences. If None,\n",
    "            auto-detects from the training set. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - train_sequence (NFLDataSequence): The training data sequence.\n",
    "            - val_sequence (NFLDataSequence): The validation data sequence.\n",
    "            Returns (None, None) if an error occurs.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Keras Sequence Datasets with Padding ---\")\n",
    "    \n",
    "    try:\n",
    "        # Convert object arrays to lists\n",
    "        X_list = X.tolist()\n",
    "        y_list = y.tolist()\n",
    "        \n",
    "        # Split into train and validation\n",
    "        print(f\"Splitting data (test_size={test_size})...\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_list, y_list, \n",
    "            test_size=test_size, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"Train size: {len(X_train)}\")\n",
    "        print(f\"Val size: {len(X_val)}\")\n",
    "        \n",
    "        # Create Sequence objects\n",
    "        print(\"Creating Training Sequence...\")\n",
    "        train_sequence = NFLDataSequence(\n",
    "            X_train, y_train, \n",
    "            batch_size=batch_size,\n",
    "            maxlen_x=maxlen_x,\n",
    "            maxlen_y=maxlen_y,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(\"Creating Validation Sequence...\")\n",
    "        val_sequence = NFLDataSequence(\n",
    "            X_val, y_val,\n",
    "            batch_size=batch_size,\n",
    "            maxlen_x=train_sequence.maxlen_x,  # Use same max lengths as training\n",
    "            maxlen_y=train_sequence.maxlen_y,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        print(\"Sequences created successfully.\")\n",
    "        print(f\"Training batches per epoch: {len(train_sequence)}\")\n",
    "        print(f\"Validation batches per epoch: {len(val_sequence)}\")\n",
    "        \n",
    "        return train_sequence, val_sequence\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Keras sequences: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TRAIN_DIR = '/home/samer/Desktop/competitions/NFL_Big_Data_Bowl_2026_dev/nfl-big-data-bowl-2026-prediction/train/'\n",
    "    \n",
    "    loader = NFLDataLoader(TRAIN_DIR)\n",
    "    X, y = loader.get_aligned_data()\n",
    "\n",
    "    print(\"\\n--- Final Data Shapes ---\")\n",
    "    print(f\"X (Input) Shape: {X.shape}\")\n",
    "    print(f\"y (Output) Shape: {y.shape}\")\n",
    "\n",
    "    if len(X) > 0:\n",
    "        print(f\"Sample Input Sequence Length: {len(X[0])}\")\n",
    "        print(f\"Sample Output Sequence Length: {len(y[0])}\")\n",
    "\n",
    "    # Create Keras Sequences with padding\n",
    "    train_seq, val_seq = create_tf_datasets(X, y, batch_size=32)\n",
    "    \n",
    "    if train_seq:\n",
    "        print(\"\\nVerifying Sequence Element:\")\n",
    "        # Get one batch to verify shapes\n",
    "        x_batch, y_batch = train_seq[0]\n",
    "        print(f\"Batch X shape: {x_batch.shape}\")\n",
    "        print(f\"Batch y shape: {y_batch.shape}\")\n",
    "        print(f\"Max sequence lengths - X: {train_seq.maxlen_x}, y: {train_seq.maxlen_y}\")\n",
    "\n",
    "    print(\"\\nData loading, alignment, and sequence creation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-11-24T03:23:40.846712Z",
     "iopub.status.busy": "2025-11-24T03:23:40.846171Z",
     "iopub.status.idle": "2025-11-24T04:45:21.292182Z",
     "shell.execute_reply": "2025-11-24T04:45:21.291053Z",
     "shell.execute_reply.started": "2025-11-24T03:23:40.846691Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NFL Big Data Bowl 2026 - Predictor Training\n",
      "============================================================\n",
      "\n",
      "[1/4] Loading data from CSV files...\n",
      "Loading and filtering 18 Input files...\n",
      "Loading 18 Output files...\n",
      "Aligning Input and Output sequences...\n",
      "Processing complete.\n",
      "Total Unique Sequences (Matches): 46045\n",
      "Converting to NumPy arrays...\n",
      "Initial X shape: (46045,)\n",
      "Initial y shape: (46045,)\n",
      "\n",
      "Data Summary:\n",
      "  Total sequences: 46045\n",
      "  Sample input sequence length: 26\n",
      "  Sample output sequence length: 21\n",
      "  Input features per timestep: 18\n",
      "  Output features per timestep: 2\n",
      "\n",
      "[2/4] Creating training and validation sequences (test_size=0.2)...\n",
      "\n",
      "--- Creating Keras Sequence Datasets with Padding ---\n",
      "Splitting data (test_size=0.2)...\n",
      "Train size: 36836\n",
      "Val size: 9209\n",
      "Creating Training Sequence...\n",
      "NFLDataSequence initialized: 36836 samples, batch_size=32\n",
      "Max sequence lengths - X: 10, y: 10\n",
      "Creating Validation Sequence...\n",
      "NFLDataSequence initialized: 9209 samples, batch_size=32\n",
      "Max sequence lengths - X: 10, y: 10\n",
      "Sequences created successfully.\n",
      "Training batches per epoch: 1152\n",
      "Validation batches per epoch: 288\n",
      "\n",
      "Detected shapes:\n",
      "  Input: (10, 18)\n",
      "  Output: (10, 2)\n",
      "Search space summary\n",
      "Default search space size: 5\n",
      "lr (Float)\n",
      "{'default': 1e-07, 'conditions': [], 'min_value': 1e-07, 'max_value': 0.001, 'step': None, 'sampling': 'log'}\n",
      "lu (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 160, 'max_value': 1024, 'step': 8, 'sampling': 'linear'}\n",
      "kr (Float)\n",
      "{'default': 1e-10, 'conditions': [], 'min_value': 1e-10, 'max_value': 1e-05, 'step': None, 'sampling': 'log'}\n",
      "af (Choice)\n",
      "{'default': 'sigmoid', 'conditions': [], 'values': ['sigmoid', 'hard_sigmoid', 'tanh', 'relu', 'softmax', 'linear'], 'ordered': False}\n",
      "wd (Float)\n",
      "{'default': 1e-10, 'conditions': [], 'min_value': 1e-10, 'max_value': 0.0009, 'step': None, 'sampling': 'log'}\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "3.6117e-05        |3.6117e-05        |lr\n",
      "192               |192               |lu\n",
      "1.1033e-08        |1.1033e-08        |kr\n",
      "hard_sigmoid      |hard_sigmoid      |af\n",
      "0.00015039        |0.00015039        |wd\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 06:32:19.154130: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1152/1152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 14ms/step - loss: 2090.9250 - mean_absolute_error: 37.6357 - val_loss: 1984.0410 - val_mean_absolute_error: 36.6212\n",
      "Epoch 2/5\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 1948.0458 - mean_absolute_error: 36.1911 - val_loss: 1899.2477 - val_mean_absolute_error: 35.6068\n",
      "Epoch 3/5\n",
      "\u001b[1m 279/1152\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 1893.4599 - mean_absolute_error: 35.6133"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 197\u001b[0m\n\u001b[1;32m    194\u001b[0m train_seq, val_seq \u001b[38;5;241m=\u001b[39m create_tf_datasets(X, y, test_size\u001b[38;5;241m=\u001b[39mtest_size, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Run the hyperparameter experimentation\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mexperimenting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_seq\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 158\u001b[0m, in \u001b[0;36mexperimenting\u001b[0;34m(training_dataset, validation_data)\u001b[0m\n\u001b[1;32m    154\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch_space_summary() \u001b[38;5;66;03m# Print a summary of the hyperparameter search space\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# NFLDataSequence is already batched, no need to call batch() again\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Run the hyperparameter search experiments\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m    162\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m tuner\u001b[38;5;241m.\u001b[39mresults_summary()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    242\u001b[0m     ):\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    255\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    398\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[0;32m--> 399\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    239\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    240\u001b[0m     ):\n\u001b[0;32m--> 241\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1703\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import sys\n",
    "import keras_tuner\n",
    "\n",
    "# Add the manual_data_processing directory to the path\n",
    "# sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'manual_data_processing'))\n",
    "\n",
    "# from csv_to_numpy import NFLDataLoader, create_tf_datasets\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Builds a compiled Keras LSTM model with hyperparameters to be experimented on.\n",
    "\n",
    "    This function defines the architecture of the LSTM model for sequence-to-sequence prediction.\n",
    "    It incorporates hyperparameter search spaces for key model parameters like learning rate,\n",
    "    number of LSTM units, kernel regularization, and activation functions.\n",
    "\n",
    "    Args:\n",
    "        hp (keras_tuner.HyperParameters): An instance of Keras Tuner's HyperParameters class,\n",
    "                                          used to define the search space for hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Keras LSTM model with hyperparameters set by Keras Tuner.\n",
    "    \"\"\"\n",
    "    \n",
    "    SEED = 42\n",
    "    # Define hyperparameter search spaces for tuning\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-7, max_value=1e-3, sampling=\"log\")\n",
    "    layer_u = hp.Int(\"lu\", min_value=160, max_value=1024, step=8)\n",
    "    kernel_r = hp.Float(\"kr\", min_value=1e-10, max_value=1e-5, sampling=\"log\")\n",
    "    acti_f = hp.Choice(\"af\", [\"sigmoid\", \"hard_sigmoid\", \"tanh\", \"relu\", \"softmax\", \"linear\"])\n",
    "    weight_d = hp.Float(\"wd\", min_value=1e-10, max_value=0.0009, sampling=\"log\")\n",
    "\n",
    "    # Define the model structure using Keras Sequential API\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        keras.layers.Input(shape=(input_seq_length, input_features)),\n",
    "        \n",
    "        # Encoder LSTM layers\n",
    "        keras.layers.LSTM(\n",
    "            units=layer_u,\n",
    "            activation=acti_f,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=layer_u // 2,\n",
    "            activation=acti_f,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=layer_u // 2,\n",
    "            activation=acti_f,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=layer_u // 2,\n",
    "            activation=acti_f,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=layer_u // 2,\n",
    "            activation=acti_f,\n",
    "            return_sequences=False,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        layers.RepeatVector(output_seq_length),\n",
    "        keras.layers.LSTM(\n",
    "            units=32,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            # kernel_regularizer=keras.regularizers.L2(l2=0.00000195),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        # Crop or slice to match output sequence length\n",
    "        # layers.Lambda(lambda x: x[:, :output_seq_length, :]),\n",
    "        # TimeDistributed dense layer for output features\n",
    "        layers.TimeDistributed(\n",
    "            keras.layers.Dense(units=output_features, activation=\"linear\")\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # Compile the model with a tunable optimizer and metrics\n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            global_clipnorm=1,\n",
    "            amsgrad=False,\n",
    "            # weight_decay=weight_d, # Tunable weight decay\n",
    "        ),\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError()],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def experimenting(training_dataset, validation_data):\n",
    "    \"\"\"\n",
    "    Runs Keras Tuner experiments for the LSTM model using the RandomSearch algorithm.\n",
    "\n",
    "    This function initializes a `RandomSearch` tuner with the `build_model` function,\n",
    "    configures the search objective (minimizing validation loss), and then executes\n",
    "    the hyperparameter search across the defined search spaces. It prints summaries\n",
    "    of the search space and the results.\n",
    "\n",
    "    Args:\n",
    "        training_dataset: NFLDataSequence object for training data\n",
    "        validation_data: NFLDataSequence object for validation data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    hp = keras_tuner.HyperParameters()\n",
    "    \n",
    "    # Get a batch from the sequence to determine shapes\n",
    "    x_batch, y_batch = training_dataset[0]\n",
    "    global input_features, input_seq_length, output_seq_length, output_features\n",
    "    input_seq_length = x_batch.shape[1]\n",
    "    input_features = x_batch.shape[2]\n",
    "    output_seq_length = y_batch.shape[1]\n",
    "    output_features = y_batch.shape[2]\n",
    "    \n",
    "    print(f\"\\nDetected shapes:\")\n",
    "    print(f\"  Input: ({input_seq_length}, {input_features})\")\n",
    "    print(f\"  Output: ({output_seq_length}, {output_features})\")\n",
    "    \n",
    "    build_model(hp) # Instantiate a dummy model to build the search space\n",
    "\n",
    "    # Initialize Keras Tuner's RandomSearch algorithm\n",
    "    tuner = keras_tuner.RandomSearch(\n",
    "        hypermodel=build_model,\n",
    "        max_trials=100, # Maximum number of hyperparameter combinations to try\n",
    "        objective=keras_tuner.Objective(\"val_loss\", \"min\"),   # Objective is to minimize validation loss\n",
    "        executions_per_trial=1, # Number of models to train for each trial (1 for efficiency)\n",
    "        overwrite=True, # Overwrite previous results in the directory\n",
    "        directory=os.getenv(\"KERAS_TUNER_EXPERIMENTS_DIR\", \"./tuner_results\"), # Directory to save experiment logs and checkpoints\n",
    "        project_name=\"nfl_prediction\", # Name of the Keras Tuner project\n",
    "        seed = 42,\n",
    "        max_consecutive_failed_trials=5,\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary() # Print a summary of the hyperparameter search space\n",
    "\n",
    "    # NFLDataSequence is already batched, no need to call batch() again\n",
    "    # Run the hyperparameter search experiments\n",
    "    tuner.search(\n",
    "        training_dataset, \n",
    "        validation_data=validation_data, \n",
    "        epochs=5\n",
    "    )\n",
    "\n",
    "    tuner.results_summary() # Print a summary of the best performing trials\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dir = '/home/samer/Desktop/competitions/NFL_Big_Data_Bowl_2026_dev/nfl-big-data-bowl-2026-analytics/114239_nfl_competition_files_published_analytics_final/train'\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    test_size = 0.2\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"NFL Big Data Bowl 2026 - Predictor Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n[1/4] Loading data from CSV files...\")\n",
    "    loader = NFLDataLoader(train_dir)\n",
    "    X, y = loader.get_aligned_data()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No data loaded. Please check the data directory.\")\n",
    "    \n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  Total sequences: {len(X)}\")\n",
    "    print(f\"  Sample input sequence length: {len(X[0])}\")\n",
    "    print(f\"  Sample output sequence length: {len(y[0])}\")\n",
    "    print(f\"  Input features per timestep: {len(X[0][0]) if len(X[0]) > 0 else 0}\")\n",
    "    print(f\"  Output features per timestep: {len(y[0][0]) if len(y[0]) > 0 else 0}\")\n",
    "    \n",
    "    # Create Keras Sequences with padding\n",
    "    print(f\"\\n[2/4] Creating training and validation sequences (test_size={test_size})...\")\n",
    "    train_seq, val_seq = create_tf_datasets(X, y, test_size=test_size, batch_size=batch_size)\n",
    "    \n",
    "    # Run the hyperparameter experimentation\n",
    "    experimenting(train_seq, val_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:11:04.664851Z",
     "iopub.status.busy": "2025-11-24T18:11:04.664315Z",
     "iopub.status.idle": "2025-11-24T18:24:24.368835Z",
     "shell.execute_reply": "2025-11-24T18:24:24.367748Z",
     "shell.execute_reply.started": "2025-11-24T18:11:04.664825Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NFL Big Data Bowl 2026 - Predictor Training\n",
      "============================================================\n",
      "\n",
      "[1/4] Loading data from CSV files...\n",
      "Loading and filtering 18 Input files...\n",
      "Loading 18 Output files...\n",
      "Aligning Input and Output sequences...\n",
      "Processing complete.\n",
      "Total Unique Sequences (Matches): 46045\n",
      "Initial X shape: (46045,)\n",
      "Initial y shape: (46045,)\n",
      "\n",
      "Data Summary:\n",
      "  Total sequences: 46045\n",
      "  Sample input sequence length: 38\n",
      "  Sample output sequence length: 12\n",
      "  Input features per timestep: 23\n",
      "  Output features per timestep: 2\n",
      "\n",
      "[2/4] Creating training and validation sequences (test_size=0.2)...\n",
      "\n",
      "--- Creating Keras Sequence Datasets with Padding ---\n",
      "Splitting data (test_size=0.2)...\n",
      "Train size: 36836\n",
      "Val size: 9209\n",
      "Creating Training Sequence...\n",
      "NFLDataSequence initialized: 36836 samples, batch_size=32\n",
      "Max sequence lengths - X: 123, y: 94\n",
      "Creating Validation Sequence...\n",
      "NFLDataSequence initialized: 9209 samples, batch_size=32\n",
      "Max sequence lengths - X: 123, y: 94\n",
      "Sequences created successfully.\n",
      "Training batches per epoch: 1152\n",
      "Validation batches per epoch: 288\n",
      "\n",
      "Sequence Shapes:\n",
      "  Input: (batch_size, 123, 23)\n",
      "  Output: (batch_size, 94, 2)\n",
      "\n",
      "[3/4] Building sequence-to-sequence model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764060650.569659   18530 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2143 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">123</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">696</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,004,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">123</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">348</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,454,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">123</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">348</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">970,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">123</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">348</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">970,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">348</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">970,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">348</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,768</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)          │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m123\u001b[0m, \u001b[38;5;34m696\u001b[0m)       │     \u001b[38;5;34m2,004,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m123\u001b[0m, \u001b[38;5;34m348\u001b[0m)       │     \u001b[38;5;34m1,454,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m123\u001b[0m, \u001b[38;5;34m348\u001b[0m)       │       \u001b[38;5;34m970,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m123\u001b[0m, \u001b[38;5;34m348\u001b[0m)       │       \u001b[38;5;34m970,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m348\u001b[0m)            │       \u001b[38;5;34m970,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ repeat_vector (\u001b[38;5;33mRepeatVector\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m348\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │        \u001b[38;5;34m48,768\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m2\u001b[0m)          │            \u001b[38;5;34m66\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,418,626</span> (24.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,418,626\u001b[0m (24.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,418,626</span> (24.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,418,626\u001b[0m (24.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/4] Training model for 200 epochs...\n",
      "Starting model training...\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "2025-11-25 11:50:56.985906: I external/local_xla/xla/service/service.cc:163] XLA service 0x37ac3550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-25 11:50:56.985923: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "2025-11-25 11:50:57.196835: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-25 11:50:58.404346: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91400\n",
      "2025-11-25 11:50:58.790698: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-25 11:50:58.790880: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-25 11:50:58.791059: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-25 11:50:58.791223: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-25 11:50:59.338873: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_384', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-25 11:50:59.642726: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_371', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-25 11:50:59.806514: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_393', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-25 11:50:59.919729: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_384', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:00.065428: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_371', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:00.096935: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_393', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:00.304932: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_370', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:00.415641: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_377', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:00.442586: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_391', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:00.945563: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_377', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:01.207033: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_377', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:01.542592: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_372', 368 bytes spill stores, 368 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:01.557166: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_378', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:01.832996: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_391', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:01.905404: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_392', 352 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:01.988390: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_372', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:02.076600: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_370', 240 bytes spill stores, 240 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:02.352133: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_372', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:02.859677: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_392', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:03.113410: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_391', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:03.172249: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_377', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:03.188032: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_378', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:03.198610: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_392', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:03.334961: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_378', 368 bytes spill stores, 368 bytes spill loads\n",
      "\n",
      "2025-11-25 11:51:04.875620: W external/local_xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3023] Can't reduce memory use below 2.73GiB (2931238666 bytes) by rematerialization; only reduced to 3.25GiB (3495177184 bytes), down from 3.25GiB (3495177184 bytes) originally\n",
      "2025-11-25 11:51:10.033511: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'loop_add_fusion_23', 688 bytes spill stores, 616 bytes spill loads\n",
      "\n",
      "I0000 00:00:1764060670.114478  121790 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-11-25 11:51:20.138344: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.18GiB (rounded to 3418306816)requested by op \n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-11-25 11:51:20.138363: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1049] BFCAllocator dump for GPU_0_bfc\n",
      "2025-11-25 11:51:20.138368: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (256): \tTotal Chunks: 32, Chunks in use: 32. 8.0KiB allocated for chunks. 8.0KiB in use in bin. 724B client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138369: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (512): \tTotal Chunks: 3, Chunks in use: 3. 1.8KiB allocated for chunks. 1.8KiB in use in bin. 1.5KiB client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138371: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138372: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138375: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (4096): \tTotal Chunks: 5, Chunks in use: 5. 28.5KiB allocated for chunks. 28.5KiB in use in bin. 27.2KiB client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138376: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (8192): \tTotal Chunks: 2, Chunks in use: 2. 22.0KiB allocated for chunks. 22.0KiB in use in bin. 21.8KiB client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138378: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (16384): \tTotal Chunks: 3, Chunks in use: 3. 55.5KiB allocated for chunks. 55.5KiB in use in bin. 55.5KiB client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138379: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138380: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138382: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (131072): \tTotal Chunks: 3, Chunks in use: 3. 598.2KiB allocated for chunks. 598.2KiB in use in bin. 598.1KiB client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138383: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (262144): \tTotal Chunks: 2, Chunks in use: 2. 778.2KiB allocated for chunks. 778.2KiB in use in bin. 603.8KiB client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138384: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138386: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (1048576): \tTotal Chunks: 10, Chunks in use: 8. 17.76MiB allocated for chunks. 14.78MiB in use in bin. 14.78MiB client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138388: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (2097152): \tTotal Chunks: 2, Chunks in use: 2. 7.39MiB allocated for chunks. 7.39MiB in use in bin. 7.39MiB client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138389: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (4194304): \tTotal Chunks: 2, Chunks in use: 2. 14.78MiB allocated for chunks. 14.78MiB in use in bin. 14.78MiB client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138390: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138394: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138396: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138397: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138398: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138399: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1056] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 0. 2.05GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-11-25 11:51:20.138401: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1072] Bin for 3.18GiB was 256.00MiB, Chunk State: \n",
      "2025-11-25 11:51:20.138406: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1078]   Size: 2.05GiB | Requested Size: 16.02MiB | in_use: 0 | bin_num: 20, prev:   Size: 3.70MiB | Requested Size: 3.70MiB | in_use: 1 | bin_num: -1\n",
      "2025-11-25 11:51:20.138407: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1085] Next region of size 2247360512\n",
      "2025-11-25 11:51:20.138409: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee000000 of size 1280 next 1\n",
      "2025-11-25 11:51:20.138410: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee000500 of size 256 next 2\n",
      "2025-11-25 11:51:20.138412: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee000600 of size 256 next 3\n",
      "2025-11-25 11:51:20.138413: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee000700 of size 256 next 4\n",
      "2025-11-25 11:51:20.138414: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee000800 of size 256 next 5\n",
      "2025-11-25 11:51:20.138415: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee000900 of size 256 next 6\n",
      "2025-11-25 11:51:20.138416: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee000a00 of size 256 next 7\n",
      "2025-11-25 11:51:20.138417: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee000b00 of size 256 next 8\n",
      "2025-11-25 11:51:20.138418: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee000c00 of size 256 next 9\n",
      "2025-11-25 11:51:20.138419: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee000d00 of size 256 next 10\n",
      "2025-11-25 11:51:20.138420: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee000e00 of size 256 next 11\n",
      "2025-11-25 11:51:20.138421: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee000f00 of size 256 next 13\n",
      "2025-11-25 11:51:20.138422: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001000 of size 256 next 12\n",
      "2025-11-25 11:51:20.138422: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001100 of size 256 next 18\n",
      "2025-11-25 11:51:20.138423: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001200 of size 256 next 23\n",
      "2025-11-25 11:51:20.138424: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001300 of size 256 next 30\n",
      "2025-11-25 11:51:20.138425: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001400 of size 256 next 31\n",
      "2025-11-25 11:51:20.138426: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001500 of size 256 next 40\n",
      "2025-11-25 11:51:20.138428: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001600 of size 256 next 36\n",
      "2025-11-25 11:51:20.138429: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001700 of size 256 next 44\n",
      "2025-11-25 11:51:20.138429: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001800 of size 256 next 43\n",
      "2025-11-25 11:51:20.138430: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001900 of size 256 next 48\n",
      "2025-11-25 11:51:20.138431: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001a00 of size 512 next 15\n",
      "2025-11-25 11:51:20.138432: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001c00 of size 256 next 14\n",
      "2025-11-25 11:51:20.138433: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001d00 of size 256 next 49\n",
      "2025-11-25 11:51:20.138434: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001e00 of size 256 next 42\n",
      "2025-11-25 11:51:20.138435: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee001f00 of size 256 next 50\n",
      "2025-11-25 11:51:20.138436: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee002000 of size 256 next 51\n",
      "2025-11-25 11:51:20.138437: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee002100 of size 512 next 53\n",
      "2025-11-25 11:51:20.138438: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee002300 of size 256 next 54\n",
      "2025-11-25 11:51:20.138439: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee002400 of size 6656 next 19\n",
      "2025-11-25 11:51:20.138440: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee003e00 of size 11264 next 21\n",
      "2025-11-25 11:51:20.138441: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee006a00 of size 5632 next 27\n",
      "2025-11-25 11:51:20.138442: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee008000 of size 5632 next 32\n",
      "2025-11-25 11:51:20.138443: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee009600 of size 5632 next 39\n",
      "2025-11-25 11:51:20.138444: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee00ac00 of size 5632 next 33\n",
      "2025-11-25 11:51:20.138445: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee00c200 of size 16384 next 45\n",
      "2025-11-25 11:51:20.138446: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee010200 of size 16384 next 47\n",
      "2025-11-25 11:51:20.138447: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee014200 of size 434688 next 16\n",
      "2025-11-25 11:51:20.138448: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee07e400 of size 256256 next 17\n",
      "2025-11-25 11:51:20.138449: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee0bcd00 of size 1937664 next 25\n",
      "2025-11-25 11:51:20.138451: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee295e00 of size 178176 next 46\n",
      "2025-11-25 11:51:20.138452: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee2c1600 of size 178176 next 55\n",
      "2025-11-25 11:51:20.138453: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee2ece00 of size 256 next 56\n",
      "2025-11-25 11:51:20.138454: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee2ecf00 of size 256 next 57\n",
      "2025-11-25 11:51:20.138455: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee2ed000 of size 256 next 58\n",
      "2025-11-25 11:51:20.138456: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee2ed100 of size 11264 next 59\n",
      "2025-11-25 11:51:20.138457: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee2efd00 of size 24064 next 61\n",
      "2025-11-25 11:51:20.138459: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee2f5b00 of size 362240 next 62\n",
      "2025-11-25 11:51:20.138460: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee34e200 of size 768 next 63\n",
      "2025-11-25 11:51:20.138461: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee34e500 of size 256 next 64\n",
      "2025-11-25 11:51:20.138462: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee34e600 of size 256 next 65\n",
      "2025-11-25 11:51:20.138463: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 7f71ee34e700 of size 1181696 next 38\n",
      "2025-11-25 11:51:20.138464: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee46ef00 of size 1937664 next 35\n",
      "2025-11-25 11:51:20.138465: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee648000 of size 1937664 next 29\n",
      "2025-11-25 11:51:20.138466: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ee821100 of size 3875328 next 28\n",
      "2025-11-25 11:51:20.138467: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71eebd3300 of size 1937664 next 34\n",
      "2025-11-25 11:51:20.138468: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71eedac400 of size 1937664 next 26\n",
      "2025-11-25 11:51:20.138469: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71eef85500 of size 1937664 next 37\n",
      "2025-11-25 11:51:20.138470: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ef15e600 of size 1937664 next 24\n",
      "2025-11-25 11:51:20.138471: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 7f71ef337700 of size 1937664 next 41\n",
      "2025-11-25 11:51:20.138472: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ef510800 of size 1937664 next 20\n",
      "2025-11-25 11:51:20.138473: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71ef6e9900 of size 7750656 next 22\n",
      "2025-11-25 11:51:20.138474: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71efe4dd00 of size 7750656 next 52\n",
      "2025-11-25 11:51:20.138475: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] InUse at 7f71f05b2100 of size 3875328 next 60\n",
      "2025-11-25 11:51:20.138476: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1105] Free  at 7f71f0964300 of size 2203958528 next 18446744073709551615\n",
      "2025-11-25 11:51:20.138477: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1110]      Summary of in-use Chunks by size: \n",
      "2025-11-25 11:51:20.138480: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 32 Chunks of size 256 totalling 8.0KiB\n",
      "2025-11-25 11:51:20.138481: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 512 totalling 1.0KiB\n",
      "2025-11-25 11:51:20.138482: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 768 totalling 768B\n",
      "2025-11-25 11:51:20.138483: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2025-11-25 11:51:20.138485: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 4 Chunks of size 5632 totalling 22.0KiB\n",
      "2025-11-25 11:51:20.138486: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 6656 totalling 6.5KiB\n",
      "2025-11-25 11:51:20.138487: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 11264 totalling 22.0KiB\n",
      "2025-11-25 11:51:20.138488: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 16384 totalling 32.0KiB\n",
      "2025-11-25 11:51:20.138489: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 24064 totalling 23.5KiB\n",
      "2025-11-25 11:51:20.138491: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 178176 totalling 348.0KiB\n",
      "2025-11-25 11:51:20.138492: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 256256 totalling 250.2KiB\n",
      "2025-11-25 11:51:20.138494: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 362240 totalling 353.8KiB\n",
      "2025-11-25 11:51:20.138495: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 1 Chunks of size 434688 totalling 424.5KiB\n",
      "2025-11-25 11:51:20.138496: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 8 Chunks of size 1937664 totalling 14.78MiB\n",
      "2025-11-25 11:51:20.138497: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 3875328 totalling 7.39MiB\n",
      "2025-11-25 11:51:20.138499: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1113] 2 Chunks of size 7750656 totalling 14.78MiB\n",
      "2025-11-25 11:51:20.138500: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] Sum Total of in-use chunks: 38.42MiB\n",
      "2025-11-25 11:51:20.138502: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1119] Total bytes in pool: 2247360512 memory_limit_: 2247360512 available bytes: 0 curr_region_allocation_bytes_: 4494721024\n",
      "2025-11-25 11:51:20.138505: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1124] Stats: \n",
      "Limit:                      2247360512\n",
      "InUse:                        40282624\n",
      "MaxInUse:                     90298112\n",
      "NumAllocs:                         890\n",
      "MaxAllocSize:                 24527872\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-11-25 11:51:20.138508: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:512] **__________________________________________________________________________________________________\n",
      "2025-11-25 11:51:20.138606: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 3418306800 bytes.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      " [tf-allocator-allocation-error='']\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/asyncio/base_events.py\", line 618, in run_forever\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/asyncio/base_events.py\", line 1951, in _run_once\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/asyncio/events.py\", line 84, in _run\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_18530/1390323504.py\", line 228, in <module>\n\n  File \"/tmp/ipykernel_18530/1390323504.py\", line 208, in main\n\n  File \"/tmp/ipykernel_18530/1390323504.py\", line 134, in train_model\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 399, in fit\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 241, in function\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 154, in multi_step_on_iterator\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 125, in wrapper\n\nOut of memory while trying to allocate 3418306800 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_13325]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 228\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Best validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 228\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 208\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[4/4] Training model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 208\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[1;32m    211\u001b[0m final_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfl_predictor_final.keras\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 134\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_sequence, val_sequence, epochs, callbacks)\u001b[0m\n\u001b[1;32m    131\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mextend([early_stopping, model_checkpoint])\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 134\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/asyncio/base_events.py\", line 618, in run_forever\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/asyncio/base_events.py\", line 1951, in _run_once\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/asyncio/events.py\", line 84, in _run\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_18530/1390323504.py\", line 228, in <module>\n\n  File \"/tmp/ipykernel_18530/1390323504.py\", line 208, in main\n\n  File \"/tmp/ipykernel_18530/1390323504.py\", line 134, in train_model\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 399, in fit\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 241, in function\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 154, in multi_step_on_iterator\n\n  File \"/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 125, in wrapper\n\nOut of memory while trying to allocate 3418306800 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_13325]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the manual_data_processing directory to the path\n",
    "# sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'manual_data_processing'))\n",
    "\n",
    "# from csv_to_numpy import NFLDataLoader, create_tf_datasets\n",
    "\n",
    "def build_seq2seq_model(input_seq_length, input_features, output_seq_length, output_features, lstm_units=128):\n",
    "    \"\"\"\n",
    "    Builds a sequence-to-sequence model with LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        input_seq_length (int): The length of input sequences (time steps).\n",
    "        input_features (int): The number of input features per timestep.\n",
    "        output_seq_length (int): The length of output sequences (time steps).\n",
    "        output_features (int): The number of output features per timestep.\n",
    "        lstm_units (int): The number of units in the LSTM layers.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = 42\n",
    "    # Encoder-decoder architecture for sequence-to-sequence prediction\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        keras.layers.Input(shape=(input_seq_length, input_features)),\n",
    "        \n",
    "        # Encoder LSTM layers\n",
    "        keras.layers.LSTM(\n",
    "            units=696,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=3.7001e-06),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=696 // 2,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=3.7001e-06),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=696 // 2,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=3.7001e-06),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=696 // 2,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=3.7001e-06),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=696 // 2,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=False,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=3.7001e-06),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        layers.RepeatVector(output_seq_length),\n",
    "        keras.layers.LSTM(\n",
    "            units=32,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            # kernel_regularizer=keras.regularizers.L2(l2=0.00000195),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        # Crop or slice to match output sequence length\n",
    "        # layers.Lambda(lambda x: x[:, :output_seq_length, :]),\n",
    "        # TimeDistributed dense layer for output features\n",
    "        layers.TimeDistributed(\n",
    "            keras.layers.Dense(units=output_features, activation=\"linear\")\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    cosine_decay = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=415000,\n",
    "    alpha=1e-5,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.00081926),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, train_sequence, val_sequence, epochs=10, callbacks=None):\n",
    "    \"\"\"\n",
    "    Trains the Keras model using Keras Sequence objects.\n",
    "    \n",
    "    Args:\n",
    "        model: The Keras model to train\n",
    "        train_sequence: Training data sequence (NFLDataSequence)\n",
    "        val_sequence: Validation data sequence (NFLDataSequence)\n",
    "        epochs (int): Number of training epochs\n",
    "        callbacks: List of Keras callbacks\n",
    "    \n",
    "    Returns:\n",
    "        history: Training history object\n",
    "    \"\"\"\n",
    "    if callbacks is None:\n",
    "        callbacks = []\n",
    "    \n",
    "    # Add early stopping and model checkpoint callbacks\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    callbacks.extend([early_stopping, model_checkpoint])\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    history = model.fit(\n",
    "        train_sequence,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_sequence,\n",
    "        callbacks=model_checkpoint,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Model training finished.\")\n",
    "    return history\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data, build, and train the model.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    train_dir = '/home/samer/Desktop/competitions/NFL_Big_Data_Bowl_2026_dev/nfl-big-data-bowl-2026-prediction/train'\n",
    "    batch_size = 32\n",
    "    epochs = 200\n",
    "    test_size = 0.2\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"NFL Big Data Bowl 2026 - Predictor Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n[1/4] Loading data from CSV files...\")\n",
    "    loader = NFLDataLoader(train_dir)\n",
    "    X, y = loader.get_aligned_data()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No data loaded. Please check the data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  Total sequences: {len(X)}\")\n",
    "    print(f\"  Sample input sequence length: {len(X[0])}\")\n",
    "    print(f\"  Sample output sequence length: {len(y[0])}\")\n",
    "    print(f\"  Input features per timestep: {len(X[0][0]) if len(X[0]) > 0 else 0}\")\n",
    "    print(f\"  Output features per timestep: {len(y[0][0]) if len(y[0]) > 0 else 0}\")\n",
    "    \n",
    "    # Create Keras Sequences with padding\n",
    "    print(f\"\\n[2/4] Creating training and validation sequences (test_size={test_size})...\")\n",
    "    train_seq, val_seq = create_tf_datasets(X, y, test_size=test_size, batch_size=batch_size)\n",
    "    \n",
    "    if train_seq is None:\n",
    "        print(\"Error: Failed to create training sequences.\")\n",
    "        return\n",
    "    \n",
    "    # Get one batch to determine shapes\n",
    "    x_sample, y_sample = train_seq[0]\n",
    "    input_seq_length = x_sample.shape[1]\n",
    "    input_features = x_sample.shape[2]\n",
    "    output_seq_length = y_sample.shape[1]\n",
    "    output_features = y_sample.shape[2]\n",
    "    \n",
    "    print(f\"\\nSequence Shapes:\")\n",
    "    print(f\"  Input: (batch_size, {input_seq_length}, {input_features})\")\n",
    "    print(f\"  Output: (batch_size, {output_seq_length}, {output_features})\")\n",
    "    \n",
    "    # Build model\n",
    "    print(f\"\\n[3/4] Building sequence-to-sequence model...\")\n",
    "    model = build_seq2seq_model(\n",
    "        input_seq_length=input_seq_length,\n",
    "        input_features=input_features,\n",
    "        output_seq_length=output_seq_length,\n",
    "        output_features=output_features,\n",
    "        lstm_units=128\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\n[4/4] Training model for {epochs} epochs...\")\n",
    "    history = train_model(model, train_seq, val_seq, epochs=epochs)\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = 'nfl_predictor_final.keras'\n",
    "    model.save(final_model_path)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Final model saved to: {final_model_path}\")\n",
    "    print(f\"Best model saved to: best_model.keras\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Print training summary\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"  Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"  Final training MAE: {history.history['mae'][-1]:.4f}\")\n",
    "    print(f\"  Final validation MAE: {history.history['val_mae'][-1]:.4f}\")\n",
    "    print(f\"  Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14210809,
     "sourceId": 114239,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
