{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T08:19:09.030425Z",
     "iopub.status.busy": "2025-11-26T08:19:09.030147Z",
     "iopub.status.idle": "2025-11-26T08:20:34.116314Z",
     "shell.execute_reply": "2025-11-26T08:20:34.115244Z",
     "shell.execute_reply.started": "2025-11-26T08:19:09.030398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install polars\n",
    "!pip install keras-tuner\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "class UnsupervisedNFLDataLoader:\n",
    "    \"\"\"Loads NFL data for unsupervised learning (no trajectory labels needed).\n",
    "    \n",
    "    This loader processes ALL player sequences (player_to_predict=True and False)\n",
    "    to maximize the amount of training data for representation learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_sequences = None\n",
    "        \n",
    "    def load_files(self, directories, include_labeled=True, include_unlabeled=True):\n",
    "        \"\"\"Load input files from specified directories.\n",
    "        \n",
    "        Args:\n",
    "            directories (list): List of directory paths to load from\n",
    "            include_labeled (bool): Include player_to_predict=True sequences\n",
    "            include_unlabeled (bool): Include player_to_predict=False sequences\n",
    "        \"\"\"\n",
    "        input_dfs = []\n",
    "        \n",
    "        print(f\"Loading unsupervised data from {len(directories)} directories...\")\n",
    "        print(f\"Include labeled: {include_labeled}, Include unlabeled: {include_unlabeled}\")\n",
    "        \n",
    "        for d in directories:\n",
    "            if not os.path.exists(d):\n",
    "                print(f\"Warning: Directory not found: {d}\")\n",
    "                continue\n",
    "                \n",
    "            input_files = sorted([f for f in os.listdir(d) if f.startswith('input') and f.endswith('.csv')])\n",
    "            print(f\"  Found {len(input_files)} input files in {d}\")\n",
    "            \n",
    "            for f in input_files:\n",
    "                try:\n",
    "                    df = pl.read_csv(os.path.join(d, f), infer_schema_length=10000)\n",
    "                    \n",
    "                    initial_rows = len(df)\n",
    "                    \n",
    "                    # Filter based on player_to_predict flag\n",
    "                    if \"player_to_predict\" in df.columns:\n",
    "                        if include_labeled and not include_unlabeled:\n",
    "                            # Only labeled\n",
    "                            if df[\"player_to_predict\"].dtype == pl.Boolean:\n",
    "                                df = df.filter(pl.col(\"player_to_predict\") == True)\n",
    "                            else:\n",
    "                                df = df.filter(pl.col(\"player_to_predict\").cast(pl.Utf8).str.to_lowercase() == \"true\")\n",
    "                        elif include_unlabeled and not include_labeled:\n",
    "                            # Only unlabeled\n",
    "                            if df[\"player_to_predict\"].dtype == pl.Boolean:\n",
    "                                df = df.filter(pl.col(\"player_to_predict\") == False)\n",
    "                            else:\n",
    "                                df = df.filter(pl.col(\"player_to_predict\").cast(pl.Utf8).str.to_lowercase() == \"false\")\n",
    "                        # If both True, include all (no filtering)\n",
    "                    \n",
    "                    if len(df) > 0:\n",
    "                        input_dfs.append(df)\n",
    "                        print(f\"    {f}: {initial_rows} -> {len(df)} rows\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {f}: {e}\")\n",
    "        \n",
    "        if not input_dfs:\n",
    "            print(\"No data found.\")\n",
    "            self.input_sequences = pl.DataFrame()\n",
    "            return\n",
    "        \n",
    "        # Concatenate all dataframes\n",
    "        print(\"Concatenating dataframes...\")\n",
    "        full_input = pl.concat(input_dfs, how=\"vertical_relaxed\")\n",
    "        \n",
    "        # Deduplicate\n",
    "        full_input = full_input.unique(subset=[\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n",
    "        \n",
    "        # Process features\n",
    "        print(\"Processing features...\")\n",
    "        id_cols = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"player_to_predict\", \"time\"]\n",
    "        feature_cols = [c for c in full_input.columns if c not in id_cols]\n",
    "        \n",
    "        expressions = []\n",
    "        for col in feature_cols:\n",
    "            if full_input[col].dtype == pl.Utf8:\n",
    "                expr = (\n",
    "                    pl.when(pl.col(col).str.to_lowercase() == \"true\").then(1.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"false\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"left\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"right\").then(1.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"defense\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"offense\").then(1.0)\n",
    "                    .otherwise(\n",
    "                        pl.col(col).cast(pl.Float64, strict=False).fill_null(\n",
    "                            pl.col(col).hash() % 10000\n",
    "                        )\n",
    "                    ).cast(pl.Float64).alias(col)\n",
    "                )\n",
    "                expressions.append(expr)\n",
    "            else:\n",
    "                expressions.append(pl.col(col).cast(pl.Float64).alias(col))\n",
    "        \n",
    "        full_input = full_input.with_columns(expressions)\n",
    "        \n",
    "        # Sort by frame_id\n",
    "        if \"frame_id\" in full_input.columns:\n",
    "            full_input = full_input.sort([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n",
    "        \n",
    "        # Group into sequences\n",
    "        agg_exprs = [pl.col(c) for c in feature_cols]\n",
    "        self.input_sequences = full_input.group_by(\n",
    "            [\"game_id\", \"play_id\", \"nfl_id\"], \n",
    "            maintain_order=True\n",
    "        ).agg(agg_exprs)\n",
    "        \n",
    "        print(f\"Total sequences: {len(self.input_sequences)}\")\n",
    "        \n",
    "    def get_sequences(self):\n",
    "        \"\"\"Convert sequences to numpy arrays.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Array of input sequences (object array)\n",
    "        \"\"\"\n",
    "        if self.input_sequences is None or self.input_sequences.is_empty():\n",
    "            return np.array([])\n",
    "        \n",
    "        print(\"Converting to NumPy arrays...\")\n",
    "        \n",
    "        # Get feature columns (exclude keys)\n",
    "        input_cols = [c for c in self.input_sequences.columns \n",
    "                     if c not in [\"game_id\", \"play_id\", \"nfl_id\"]]\n",
    "        \n",
    "        # Convert to sequences\n",
    "        input_col_indices = [self.input_sequences.columns.index(c) for c in input_cols]\n",
    "        rows = self.input_sequences.iter_rows()\n",
    "        \n",
    "        X_list = []\n",
    "        for row in rows:\n",
    "            feature_seqs = [row[i] for i in input_col_indices]\n",
    "            X_seq = list(zip(*feature_seqs))\n",
    "            X_list.append(X_seq)\n",
    "        \n",
    "        X = np.array(X_list, dtype=object)\n",
    "        print(f\"Loaded {len(X)} sequences\")\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "class UnsupervisedNFLSequence(Sequence):\n",
    "    \"\"\"Keras Sequence for unsupervised learning on NFL data.\n",
    "    \n",
    "    For autoencoder: input and output are the same (reconstruction)\n",
    "    For next-step prediction: input is sequence[:-n], output is sequence[n:]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, batch_size=32, maxlen=10, shuffle=True, \n",
    "                 task='autoencoder', prediction_steps=1):\n",
    "        \"\"\"Initialize the sequence.\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequences\n",
    "            batch_size: Batch size\n",
    "            maxlen: Maximum sequence length (fixed to 10 by default)\n",
    "            shuffle: Whether to shuffle\n",
    "            task: 'autoencoder' or 'next_step'\n",
    "            prediction_steps: For next_step, how many steps ahead to predict\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.task = task\n",
    "        self.prediction_steps = prediction_steps\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        \n",
    "        # Fixed sequence length to 10\n",
    "        self.maxlen = 10\n",
    "        \n",
    "        print(f\"UnsupervisedNFLSequence initialized:\")\n",
    "        print(f\"  Samples: {len(self.X)}\")\n",
    "        print(f\"  Batch size: {batch_size}\")\n",
    "        print(f\"  Max length: {self.maxlen} (FIXED)\")\n",
    "        print(f\"  Task: {task}\")\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_X = [self.X[i] for i in batch_indices]\n",
    "        \n",
    "        if self.task == 'autoencoder':\n",
    "            # Input and output are the same (reconstruction task)\n",
    "            X_padded = pad_sequences(\n",
    "                batch_X,\n",
    "                maxlen=self.maxlen,\n",
    "                dtype='float32',\n",
    "                padding='post',\n",
    "                truncating='post',\n",
    "                value=0.0\n",
    "            )\n",
    "            return X_padded, X_padded\n",
    "            \n",
    "        elif self.task == 'next_step':\n",
    "            # Input: sequence up to -prediction_steps\n",
    "            # Output: last prediction_steps frames\n",
    "            batch_X_input = []\n",
    "            batch_y_output = []\n",
    "            \n",
    "            for seq in batch_X:\n",
    "                if len(seq) > self.prediction_steps:\n",
    "                    batch_X_input.append(seq[:-self.prediction_steps])\n",
    "                    batch_y_output.append(seq[-self.prediction_steps:])\n",
    "                else:\n",
    "                    # If sequence too short, use full sequence for both\n",
    "                    batch_X_input.append(seq)\n",
    "                    batch_y_output.append(seq)\n",
    "            \n",
    "            X_padded = pad_sequences(\n",
    "                batch_X_input,\n",
    "                maxlen=10,\n",
    "                dtype='float32',\n",
    "                padding='post',\n",
    "                truncating='post',\n",
    "                value=0.0\n",
    "            )\n",
    "            \n",
    "            y_padded = pad_sequences(\n",
    "                batch_y_output,\n",
    "                maxlen=10,\n",
    "                dtype='float32',\n",
    "                padding='post',\n",
    "                truncating='post',\n",
    "                value=0.0\n",
    "            )\n",
    "            \n",
    "            return X_padded, y_padded\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the loader\n",
    "    PREDICTION_TRAIN_DIR = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n",
    "    \n",
    "    print(\"=== Testing Unsupervised Data Loader ===\\n\")\n",
    "    \n",
    "    # Test 1: Load only unlabeled data\n",
    "    print(\"Test 1: Loading UNLABELED data only\")\n",
    "    loader = UnsupervisedNFLDataLoader()\n",
    "    loader.load_files([PREDICTION_TRAIN_DIR], include_labeled=False, include_unlabeled=True)\n",
    "    X_unlabeled = loader.get_sequences()\n",
    "    print(f\"Unlabeled sequences: {len(X_unlabeled)}\\n\")\n",
    "    \n",
    "    # Test 2: Load ALL data\n",
    "    print(\"Test 2: Loading ALL data (labeled + unlabeled)\")\n",
    "    loader_all = UnsupervisedNFLDataLoader()\n",
    "    loader_all.load_files([PREDICTION_TRAIN_DIR], include_labeled=True, include_unlabeled=True)\n",
    "    X_all = loader_all.get_sequences()\n",
    "    print(f\"Total sequences: {len(X_all)}\\n\")\n",
    "    \n",
    "    if len(X_all) > 0:\n",
    "        print(f\"Sample sequence length: {len(X_all[0])}\")\n",
    "        print(f\"Sample features per timestep: {len(X_all[0][0])}\")\n",
    "        \n",
    "        # Test sequence generators\n",
    "        print(\"\\n=== Testing Sequence Generators ===\")\n",
    "        \n",
    "        print(\"\\nAutoencoder sequence:\")\n",
    "        ae_seq = UnsupervisedNFLSequence(X_all[:1000], batch_size=32, task='autoencoder')\n",
    "        x_batch, y_batch = ae_seq[0]\n",
    "        print(f\"Input shape: {x_batch.shape}\")\n",
    "        print(f\"Output shape: {y_batch.shape}\")\n",
    "        print(f\"Are input and output same? {np.array_equal(x_batch, y_batch)}\")\n",
    "        \n",
    "        print(\"\\nNext-step prediction sequence:\")\n",
    "        ns_seq = UnsupervisedNFLSequence(X_all[:1000], batch_size=32, task='next_step', prediction_steps=5)\n",
    "        x_batch, y_batch = ns_seq[0]\n",
    "        print(f\"Input shape: {x_batch.shape}\")\n",
    "        print(f\"Output shape: {y_batch.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised models architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T08:20:34.117462Z",
     "iopub.status.busy": "2025-11-26T08:20:34.117068Z",
     "iopub.status.idle": "2025-11-26T08:20:34.885884Z",
     "shell.execute_reply": "2025-11-26T08:20:34.884948Z",
     "shell.execute_reply.started": "2025-11-26T08:20:34.117443Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class LSTMAutoencoder:\n",
    "    \"\"\"LSTM Autoencoder for unsupervised representation learning on NFL sequences.\n",
    "    \n",
    "    The encoder learns to compress player movement sequences into a latent representation,\n",
    "    and the decoder reconstructs the original sequence. The encoder can then be used\n",
    "    to initialize supervised models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, latent_dim=128, lstm_units=[512, 256, 128, 64, 32]):\n",
    "        \"\"\"Initialize the LSTM Autoencoder.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of input (timesteps, features)\n",
    "            latent_dim: Dimension of latent representation\n",
    "            lstm_units: List of LSTM units for encoder layers\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.autoencoder = None\n",
    "        \n",
    "    def build_encoder(self):\n",
    "        \"\"\"Build the encoder network.\"\"\"\n",
    "        inputs = layers.Input(shape=self.input_shape, name='encoder_input')\n",
    "        \n",
    "        x = inputs\n",
    "        # Stack LSTM layers\n",
    "        for i, units in enumerate(self.lstm_units[:-1]):\n",
    "            x = layers.LSTM(\n",
    "                units, \n",
    "                return_sequences=True,\n",
    "                name=f'encoder_lstm_{i+1}'\n",
    "            )(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Last LSTM layer doesn't return sequences\n",
    "        x = layers.LSTM(\n",
    "            self.lstm_units[-1],\n",
    "            return_sequences=False,\n",
    "            name=f'encoder_lstm_{len(self.lstm_units)}'\n",
    "        )(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Latent representation\n",
    "        latent = layers.Dense(self.latent_dim, activation='relu', name='latent')(x)\n",
    "        \n",
    "        self.encoder = Model(inputs, latent, name='encoder')\n",
    "        return self.encoder\n",
    "    \n",
    "    def build_decoder(self):\n",
    "        \"\"\"Build the decoder network.\"\"\"\n",
    "        # Decoder input is the latent vector\n",
    "        latent_inputs = layers.Input(shape=(self.latent_dim,), name='decoder_input')\n",
    "        \n",
    "        # Repeat the latent vector for each timestep\n",
    "        x = layers.RepeatVector(self.input_shape[0])(latent_inputs)\n",
    "        \n",
    "        # Stack LSTM layers in reverse\n",
    "        for i, units in enumerate(reversed(self.lstm_units)):\n",
    "            x = layers.LSTM(\n",
    "                units,\n",
    "                return_sequences=True,\n",
    "                name=f'decoder_lstm_{i+1}'\n",
    "            )(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Output layer to reconstruct features\n",
    "        outputs = layers.TimeDistributed(\n",
    "            layers.Dense(self.input_shape[1], activation='linear'),\n",
    "            name='reconstruction'\n",
    "        )(x)\n",
    "        \n",
    "        self.decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "        return self.decoder\n",
    "    \n",
    "    def build_autoencoder(self):\n",
    "        \"\"\"Build the complete autoencoder.\"\"\"\n",
    "        if self.encoder is None:\n",
    "            self.build_encoder()\n",
    "        if self.decoder is None:\n",
    "            self.build_decoder()\n",
    "        \n",
    "        # Connect encoder and decoder\n",
    "        inputs = layers.Input(shape=self.input_shape, name='autoencoder_input')\n",
    "        latent = self.encoder(inputs)\n",
    "        outputs = self.decoder(latent)\n",
    "        \n",
    "        self.autoencoder = Model(inputs, outputs, name='autoencoder')\n",
    "        return self.autoencoder\n",
    "    \n",
    "    def compile(self, learning_rate=0.001):\n",
    "        \"\"\"Compile the autoencoder.\"\"\"\n",
    "        if self.autoencoder is None:\n",
    "            self.build_autoencoder()\n",
    "        \n",
    "        self.autoencoder.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "    def get_summary(self):\n",
    "        \"\"\"Print model summaries.\"\"\"\n",
    "        if self.autoencoder:\n",
    "            print(\"\\n=== Autoencoder Summary ===\")\n",
    "            self.autoencoder.summary()\n",
    "        if self.encoder:\n",
    "            print(\"\\n=== Encoder Summary ===\")\n",
    "            self.encoder.summary()\n",
    "        if self.decoder:\n",
    "            print(\"\\n=== Decoder Summary ===\")\n",
    "            self.decoder.summary()\n",
    "\n",
    "\n",
    "class NextStepPredictor:\n",
    "    \"\"\"LSTM model for self-supervised next-step prediction.\n",
    "    \n",
    "    Predicts future timesteps given past timesteps, which can be used\n",
    "    as a pre-training task for the supervised trajectory prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, output_steps=5, lstm_units=[256, 128], output_features=None):\n",
    "        \"\"\"Initialize the next-step predictor.\n",
    "        \n",
    "        Args:\n",
    "            input_shape: Shape of input (timesteps, features)\n",
    "            output_steps: Number of future steps to predict\n",
    "            lstm_units: List of LSTM units\n",
    "            output_features: Number of output features (if None, same as input features)\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_steps = output_steps\n",
    "        self.lstm_units = lstm_units\n",
    "        self.output_features = output_features or input_shape[1]\n",
    "        self.model = None\n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\"Build the next-step prediction model.\"\"\"\n",
    "        inputs = layers.Input(shape=self.input_shape, name='input')\n",
    "        \n",
    "        x = inputs\n",
    "        # Stack LSTM layers\n",
    "        for i, units in enumerate(self.lstm_units):\n",
    "            return_seq = (i < len(self.lstm_units) - 1)\n",
    "            x = layers.LSTM(\n",
    "                units,\n",
    "                return_sequences=return_seq,\n",
    "                name=f'lstm_{i+1}'\n",
    "            )(x)\n",
    "            x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Prediction head\n",
    "        # Expand to output_steps timesteps\n",
    "        x = layers.RepeatVector(self.output_steps)(x)\n",
    "        x = layers.LSTM(128, return_sequences=True, name='prediction_lstm')(x)\n",
    "        \n",
    "        # Output for each timestep\n",
    "        outputs = layers.TimeDistributed(\n",
    "            layers.Dense(self.output_features, activation='linear'),\n",
    "            name='predictions'\n",
    "        )(x)\n",
    "        \n",
    "        self.model = Model(inputs, outputs, name='next_step_predictor')\n",
    "        return self.model\n",
    "    \n",
    "    def compile(self, learning_rate=0.001):\n",
    "        \"\"\"Compile the model.\"\"\"\n",
    "        if self.model is None:\n",
    "            self.build()\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Print model summary.\"\"\"\n",
    "        if self.model:\n",
    "            self.model.summary()\n",
    "\n",
    "\n",
    "def create_training_callbacks(model_path, patience=10):\n",
    "    \"\"\"Create standard callbacks for training.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to save best model\n",
    "        patience: Patience for early stopping\n",
    "        \n",
    "    Returns:\n",
    "        List of callbacks\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            model_path,\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "def transfer_encoder_weights(pretrained_encoder, supervised_model, freeze_encoder=False):\n",
    "    \"\"\"Transfer weights from pretrained encoder to supervised model.\n",
    "    \n",
    "    Args:\n",
    "        pretrained_encoder: The pretrained encoder model\n",
    "        supervised_model: The supervised model to transfer weights to\n",
    "        freeze_encoder: Whether to freeze the transferred weights\n",
    "        \n",
    "    Returns:\n",
    "        The supervised model with transferred weights\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Transferring Encoder Weights ===\")\n",
    "    \n",
    "    # Get encoder layers from pretrained model\n",
    "    encoder_layer_names = [layer.name for layer in pretrained_encoder.layers]\n",
    "    \n",
    "    # Transfer weights to matching layers in supervised model\n",
    "    transferred_count = 0\n",
    "    for layer in supervised_model.layers:\n",
    "        if layer.name in encoder_layer_names:\n",
    "            try:\n",
    "                pretrained_layer = pretrained_encoder.get_layer(layer.name)\n",
    "                layer.set_weights(pretrained_layer.get_weights())\n",
    "                \n",
    "                if freeze_encoder:\n",
    "                    layer.trainable = False\n",
    "                \n",
    "                transferred_count += 1\n",
    "                print(f\"Transferred weights for layer: {layer.name} (frozen={freeze_encoder})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not transfer weights for {layer.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nTransferred weights for {transferred_count} layers\")\n",
    "    return supervised_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Testing Unsupervised Models ===\\n\")\n",
    "    \n",
    "    # Test parameters\n",
    "    timesteps = 28\n",
    "    features = 18\n",
    "    latent_dim = 64\n",
    "    \n",
    "    print(\"1. Testing LSTM Autoencoder\")\n",
    "    print(\"-\" * 50)\n",
    "    ae = LSTMAutoencoder(\n",
    "        input_shape=(timesteps, features),\n",
    "        latent_dim=latent_dim,\n",
    "        lstm_units=[128, 64]\n",
    "    )\n",
    "    ae.build_autoencoder()\n",
    "    ae.compile()\n",
    "    ae.get_summary()\n",
    "    \n",
    "    print(\"\\n2. Testing Next-Step Predictor\")\n",
    "    print(\"-\" * 50)\n",
    "    predictor = NextStepPredictor(\n",
    "        input_shape=(timesteps, features),\n",
    "        output_steps=5,\n",
    "        lstm_units=[128, 64],\n",
    "        output_features=features\n",
    "    )\n",
    "    predictor.build()\n",
    "    predictor.compile()\n",
    "    predictor.get_summary()\n",
    "    \n",
    "    # Test with dummy data\n",
    "    print(\"\\n3. Testing with dummy data\")\n",
    "    print(\"-\" * 50)\n",
    "    dummy_input = tf.random.normal((32, timesteps, features))\n",
    "    \n",
    "    print(\"Autoencoder forward pass:\")\n",
    "    ae_output = ae.autoencoder(dummy_input)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {ae_output.shape}\")\n",
    "    \n",
    "    print(\"\\nNext-step predictor forward pass:\")\n",
    "    ns_output = predictor.model(dummy_input)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    print(f\"Output shape: {ns_output.shape}\")\n",
    "    \n",
    "    print(\"\\nEncoder output (latent representation):\")\n",
    "    latent = ae.encoder(dummy_input)\n",
    "    print(f\"Latent shape: {latent.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unsupervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T08:20:34.886707Z",
     "iopub.status.busy": "2025-11-26T08:20:34.886541Z",
     "iopub.status.idle": "2025-11-26T09:36:36.133955Z",
     "shell.execute_reply": "2025-11-26T09:36:36.132993Z",
     "shell.execute_reply.started": "2025-11-26T08:20:34.886692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unsupervised Pre-training Script for NFL Player Trajectory Prediction\n",
    "\n",
    "This script performs unsupervised pre-training using LSTM autoencoders on all available\n",
    "NFL player sequences (both labeled and unlabeled). The pretrained encoder can then be\n",
    "used to initialize supervised models for better performance.\n",
    "\n",
    "Usage:\n",
    "    python unsupervised_pretraining.py --task autoencoder --epochs 50\n",
    "    python unsupervised_pretraining.py --task next_step --epochs 50\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directory to path\n",
    "# sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "# from unsupervised_data_loader import UnsupervisedNFLDataLoader, UnsupervisedNFLSequence\n",
    "# from unsupervised_models import (\n",
    "#     LSTMAutoencoder, \n",
    "#     NextStepPredictor, \n",
    "#     create_training_callbacks\n",
    "# )\n",
    "\n",
    "\n",
    "def train_autoencoder(train_seq, val_seq, epochs=50, latent_dim=128, model_save_path='autoencoder.keras'):\n",
    "    \"\"\"Train LSTM autoencoder for representation learning.\n",
    "    \n",
    "    Args:\n",
    "        train_seq: Training data sequence\n",
    "        val_seq: Validation data sequence\n",
    "        epochs: Number of training epochs\n",
    "        latent_dim: Dimension of latent space\n",
    "        model_save_path: Path to save the trained model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING LSTM AUTOENCODER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get input shape from first batch\n",
    "    x_sample, _ = train_seq[0]\n",
    "    input_shape = (x_sample.shape[1], x_sample.shape[2])\n",
    "    \n",
    "    print(f\"\\nInput shape: {input_shape}\")\n",
    "    print(f\"Latent dimension: {latent_dim}\")\n",
    "    \n",
    "    # Build autoencoder\n",
    "    ae = LSTMAutoencoder(\n",
    "        input_shape=input_shape,\n",
    "        latent_dim=latent_dim,\n",
    "        lstm_units=[512, 256, 128, 64, 32]\n",
    "    )\n",
    "    ae.build_autoencoder()\n",
    "    ae.compile(learning_rate=0.0001)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    ae.get_summary()\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks = create_training_callbacks(model_save_path, patience=10)\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    history = ae.autoencoder.fit(\n",
    "        train_seq,\n",
    "        validation_data=val_seq,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "    print(f\"Model saved to: {model_save_path}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Save encoder separately\n",
    "    encoder_path = model_save_path.replace('.keras', '_encoder.keras')\n",
    "    ae.encoder.save(encoder_path)\n",
    "    print(f\"Encoder saved to: {encoder_path}\")\n",
    "    \n",
    "    return ae, history\n",
    "\n",
    "\n",
    "def train_next_step_predictor(train_seq, val_seq, epochs=50, prediction_steps=5, \n",
    "                               model_save_path='next_step_predictor.keras'):\n",
    "    \"\"\"Train next-step predictor for self-supervised learning.\n",
    "    \n",
    "    Args:\n",
    "        train_seq: Training data sequence\n",
    "        val_seq: Validation data sequence\n",
    "        epochs: Number of training epochs\n",
    "        prediction_steps: Number of steps to predict ahead\n",
    "        model_save_path: Path to save the trained model\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING NEXT-STEP PREDICTOR\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get input shape from first batch\n",
    "    x_sample, y_sample = train_seq[0]\n",
    "    input_shape = (x_sample.shape[1], x_sample.shape[2])\n",
    "    output_features = y_sample.shape[2]\n",
    "    \n",
    "    print(f\"\\nInput shape: {input_shape}\")\n",
    "    print(f\"Output steps: {prediction_steps}\")\n",
    "    print(f\"Output features: {output_features}\")\n",
    "    \n",
    "    # Build model\n",
    "    predictor = NextStepPredictor(\n",
    "        input_shape=input_shape,\n",
    "        output_steps=prediction_steps,\n",
    "        lstm_units=[256, 128],\n",
    "        output_features=output_features\n",
    "    )\n",
    "    predictor.build()\n",
    "    predictor.compile(learning_rate=0.001)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    predictor.get_summary()\n",
    "    \n",
    "    # Create callbacks\n",
    "    callbacks = create_training_callbacks(model_save_path, patience=10)\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"Starting training...\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    history = predictor.model.fit(\n",
    "        train_seq,\n",
    "        validation_data=val_seq,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "    print(f\"Model saved to: {model_save_path}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return predictor, history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    PREDICTION_TRAIN_DIR = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n",
    "    ANALYTICS_TRAIN_DIR = '/kaggle/input/nfl-big-data-bowl-2026-analytics/114239_nfl_competition_files_published_analytics_final/train'\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"UNSUPERVISED PRE-TRAINING FOR NFL PLAYER TRAJECTORY PREDICTION\")\n",
    "    print(\"=\"*70)\n",
    "    # print(f\"\\nTask: {args.task}\")\n",
    "    # print(f\"Epochs: {args.epochs}\")\n",
    "    # print(f\"Batch size: {args.batch_size}\")\n",
    "    # print(f\"Include labeled: {args.include_labeled}\")\n",
    "    # print(f\"Include unlabeled: {args.include_unlabeled}\")\n",
    "    # print(f\"Validation split: {args.val_split}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    loader = UnsupervisedNFLDataLoader()\n",
    "    loader.load_files(\n",
    "        [PREDICTION_TRAIN_DIR, ANALYTICS_TRAIN_DIR],\n",
    "        include_labeled=True,\n",
    "        include_unlabeled=True\n",
    "    )\n",
    "    X = loader.get_sequences()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"ERROR: No data loaded!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nTotal sequences loaded: {len(X)}\")\n",
    "    print(f\"Sample sequence length: {len(X[0])}\")\n",
    "    print(f\"Sample features: {len(X[0][0])}\")\n",
    "    \n",
    "    # Split into train/val\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_train, X_val = train_test_split(\n",
    "        X, \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining sequences: {len(X_train)}\")\n",
    "    print(f\"Validation sequences: {len(X_val)}\")\n",
    "    \n",
    "    # Create data sequences based on task\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CREATING DATA GENERATORS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    train_seq = UnsupervisedNFLSequence(\n",
    "        X_train,\n",
    "        batch_size=32,\n",
    "        maxlen=10,\n",
    "        shuffle=True,\n",
    "        task=\"autoencoder\",\n",
    "        prediction_steps=10\n",
    "    )\n",
    "    \n",
    "    val_seq = UnsupervisedNFLSequence(\n",
    "        X_val,\n",
    "        batch_size=32,\n",
    "        maxlen=10,\n",
    "        shuffle=False,\n",
    "        task=\"autoencoder\",\n",
    "        prediction_steps=10\n",
    "    )\n",
    "    \n",
    "    # Generate timestamp for model name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Train based on task\n",
    "    model_path = os.path.join(\"/kaggle/working/\", f'autoencoder_{timestamp}.keras')\n",
    "    model, history = train_autoencoder(\n",
    "        train_seq, \n",
    "        val_seq, \n",
    "        epochs=100,\n",
    "        latent_dim=256,\n",
    "        model_save_path=model_path\n",
    "    )\n",
    "    \n",
    "    # model_path = os.path.join(args.output_dir, f'next_step_{timestamp}.keras')\n",
    "    # model, history = train_next_step_predictor(\n",
    "    #     train_seq,\n",
    "    #     val_seq,\n",
    "    #     epochs=args.epochs,\n",
    "    #     prediction_steps=args.prediction_steps,\n",
    "    #     model_save_path=model_path\n",
    "    # )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "    print(f\"\\nModel saved to: {model_path}\")\n",
    "    \n",
    "    encoder_path = model_path.replace('.keras', '_encoder.keras')\n",
    "    print(f\"Encoder saved to: {encoder_path}\")\n",
    "    print(\"\\nTo use the pretrained encoder in your supervised model:\")\n",
    "    print(f\"  from tensorflow import keras\")\n",
    "    print(f\"  from unsupervised_models import transfer_encoder_weights\")\n",
    "    print(f\"  pretrained_encoder = keras.models.load_model('{encoder_path}')\")\n",
    "    print(f\"  supervised_model = transfer_encoder_weights(pretrained_encoder, supervised_model)\")\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"DONE!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T10:19:11.563717Z",
     "iopub.status.busy": "2025-11-26T10:19:11.563471Z",
     "iopub.status.idle": "2025-11-26T10:19:21.740531Z",
     "shell.execute_reply": "2025-11-26T10:19:21.739362Z",
     "shell.execute_reply.started": "2025-11-26T10:19:11.563700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the manual_data_processing directory to the path\n",
    "# sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'manual_data_processing'))\n",
    "\n",
    "# from csv_to_numpy import NFLDataLoader, create_tf_datasets\n",
    "\n",
    "def build_seq2seq_model(input_seq_length, input_features, output_seq_length, output_features, lstm_units=128):\n",
    "    \"\"\"\n",
    "    Builds a sequence-to-sequence model with LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        input_seq_length (int): The length of input sequences (time steps).\n",
    "        input_features (int): The number of input features per timestep.\n",
    "        output_seq_length (int): The length of output sequences (time steps).\n",
    "        output_features (int): The number of output features per timestep.\n",
    "        lstm_units (int): The number of units in the LSTM layers.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = 42\n",
    "    # Encoder-decoder architecture for sequence-to-sequence prediction\n",
    "    # model = keras.Sequential([\n",
    "    #     # Input layer\n",
    "    #     keras.layers.Input(shape=(input_seq_length, input_features)),\n",
    "        \n",
    "    #     # Encoder LSTM layers\n",
    "    #     keras.layers.LSTM(\n",
    "    #         units=696,\n",
    "    #         activation=\"sigmoid\",\n",
    "    #         return_sequences=True,\n",
    "    #         kernel_regularizer=keras.regularizers.L2(l2=3.7001e-06),\n",
    "    #         seed=SEED,\n",
    "    #     ),\n",
    "    inputs = layers.Input(shape=(input_seq_length, input_features), name='encoder_input')\n",
    "    \n",
    "    x = inputs\n",
    "    # Stack LSTM layers\n",
    "\n",
    "    x = layers.LSTM(\n",
    "        512, \n",
    "        return_sequences=True,\n",
    "        name=\"encoder_lstm_1\"\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.2, name=\"dropout_6\")(x)\n",
    "\n",
    "    x = layers.LSTM(\n",
    "        256, \n",
    "        return_sequences=True,\n",
    "        name=\"encoder_lstm_2\"\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.2, name=\"dropout_7\")(x)\n",
    "\n",
    "    x = layers.LSTM(\n",
    "        128, \n",
    "        return_sequences=True,\n",
    "        name=\"encoder_lstm_3\"\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.2, name=\"dropout_8\")(x)\n",
    "\n",
    "    x = layers.LSTM(\n",
    "        64, \n",
    "        return_sequences=True,\n",
    "        name=\"encoder_lstm_4\"\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.2, name=\"dropout_9\")(x)\n",
    "\n",
    "    # Last LSTM layer doesn't return sequences\n",
    "    x = layers.LSTM(\n",
    "        32,\n",
    "        return_sequences=False,\n",
    "        name=\"encoder_lstm_5\"\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.2, name=\"dropout_10\")(x)\n",
    "    \n",
    "    # Latent representation\n",
    "    latent = layers.Dense(256, activation='relu', name='latent')(x)\n",
    "    \n",
    "    model = Model(inputs, latent, name='encoder')\n",
    "\n",
    "    cosine_decay = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=415000,\n",
    "    alpha=1e-5,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.00081926),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, train_sequence, val_sequence, epochs=10, callbacks=None):\n",
    "    \"\"\"\n",
    "    Trains the Keras model using Keras Sequence objects.\n",
    "    \n",
    "    Args:\n",
    "        model: The Keras model to train\n",
    "        train_sequence: Training data sequence (NFLDataSequence)\n",
    "        val_sequence: Validation data sequence (NFLDataSequence)\n",
    "        epochs (int): Number of training epochs\n",
    "        callbacks: List of Keras callbacks\n",
    "    \n",
    "    Returns:\n",
    "        history: Training history object\n",
    "    \"\"\"\n",
    "    pretrained_encoder = keras.models.load_model('/kaggle/working/autoencoder_20251126_082108_encoder.keras')\n",
    "    supervised_model = transfer_encoder_weights(pretrained_encoder, model)\n",
    "    print(\"pretrained encoder\")\n",
    "    pretrained_encoder.summary()\n",
    "    print(\"supervised model\")\n",
    "    supervised_model.summary()\n",
    "    if callbacks is None:\n",
    "        callbacks = []\n",
    "    \n",
    "    # Add early stopping and model checkpoint callbacks\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    callbacks.extend([early_stopping, model_checkpoint])\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    history = supervised_model.fit(\n",
    "        train_sequence,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_sequence,\n",
    "        callbacks=model_checkpoint,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Model training finished.\")\n",
    "    return history\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data, build, and train the model.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    train_dir = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n",
    "    batch_size = 32\n",
    "    epochs = 20\n",
    "    test_size = 0.2\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"NFL Big Data Bowl 2026 - Predictor Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n[1/4] Loading data from CSV files...\")\n",
    "    loader = NFLDataLoader(train_dir)\n",
    "    X, y = loader.get_aligned_data()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No data loaded. Please check the data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  Total sequences: {len(X)}\")\n",
    "    print(f\"  Sample input sequence length: {len(X[0])}\")\n",
    "    print(f\"  Sample output sequence length: {len(y[0])}\")\n",
    "    print(f\"  Input features per timestep: {len(X[0][0]) if len(X[0]) > 0 else 0}\")\n",
    "    print(f\"  Output features per timestep: {len(y[0][0]) if len(y[0]) > 0 else 0}\")\n",
    "    \n",
    "    # Create Keras Sequences with padding\n",
    "    print(f\"\\n[2/4] Creating training and validation sequences (test_size={test_size})...\")\n",
    "    train_seq, val_seq = create_tf_datasets(X, y, test_size=test_size, batch_size=batch_size)\n",
    "    \n",
    "    if train_seq is None:\n",
    "        print(\"Error: Failed to create training sequences.\")\n",
    "        return\n",
    "    \n",
    "    # Get one batch to determine shapes\n",
    "    x_sample, y_sample = train_seq[0]\n",
    "    input_seq_length = x_sample.shape[1]\n",
    "    input_features = x_sample.shape[2]\n",
    "    output_seq_length = y_sample.shape[1]\n",
    "    output_features = y_sample.shape[2]\n",
    "    \n",
    "    print(f\"\\nSequence Shapes:\")\n",
    "    print(f\"  Input: (batch_size, {input_seq_length}, {input_features})\")\n",
    "    print(f\"  Output: (batch_size, {output_seq_length}, {output_features})\")\n",
    "    \n",
    "    # Build model\n",
    "    print(f\"\\n[3/4] Building sequence-to-sequence model...\")\n",
    "    model = build_seq2seq_model(\n",
    "        input_seq_length=input_seq_length,\n",
    "        input_features=input_features,\n",
    "        output_seq_length=output_seq_length,\n",
    "        output_features=output_features,\n",
    "        lstm_units=128\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\n[4/4] Training model for {epochs} epochs...\")\n",
    "    history = train_model(model, train_seq, val_seq, epochs=epochs)\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = 'nfl_predictor_final.keras'\n",
    "    model.save(final_model_path)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Final model saved to: {final_model_path}\")\n",
    "    print(f\"Best model saved to: best_model.keras\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Print training summary\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"  Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"  Final training MAE: {history.history['mae'][-1]:.4f}\")\n",
    "    print(f\"  Final validation MAE: {history.history['val_mae'][-1]:.4f}\")\n",
    "    print(f\"  Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T09:37:50.141579Z",
     "iopub.status.busy": "2025-11-26T09:37:50.141291Z",
     "iopub.status.idle": "2025-11-26T09:38:00.022608Z",
     "shell.execute_reply": "2025-11-26T09:38:00.021392Z",
     "shell.execute_reply.started": "2025-11-26T09:37:50.141559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and filtering 18 Input files...\n",
      "Loading 18 Output files...\n",
      "Aligning Input and Output sequences...\n",
      "Processing complete.\n",
      "Total Unique Sequences (Matches): 46045\n",
      "Converting to NumPy arrays...\n",
      "Initial X shape: (46045,)\n",
      "Initial y shape: (46045,)\n",
      "\n",
      "--- Final Data Shapes ---\n",
      "X (Input) Shape: (46045,)\n",
      "y (Output) Shape: (46045,)\n",
      "Sample Input Sequence Length: 26\n",
      "Sample Output Sequence Length: 21\n",
      "\n",
      "--- Creating Keras Sequence Datasets with Padding ---\n",
      "Splitting data (test_size=0.2)...\n",
      "Train size: 36836\n",
      "Val size: 9209\n",
      "Creating Training Sequence...\n",
      "NFLDataSequence initialized: 36836 samples, batch_size=32\n",
      "Max sequence lengths - X: 54, y: 54\n",
      "Creating Validation Sequence...\n",
      "NFLDataSequence initialized: 9209 samples, batch_size=32\n",
      "Max sequence lengths - X: 54, y: 54\n",
      "Sequences created successfully.\n",
      "Training batches per epoch: 1152\n",
      "Validation batches per epoch: 288\n",
      "\n",
      "Verifying Sequence Element:\n",
      "Batch X shape: (32, 54, 14)\n",
      "Batch y shape: (32, 54, 2)\n",
      "Max sequence lengths - X: 54, y: 54\n",
      "\n",
      "Data loading, alignment, and sequence creation complete.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class NFLDataLoader:\n",
    "    \"\"\"Loads and processes NFL Big Data Bowl 2026 data from CSV files using Polars.\n",
    "\n",
    "    This class handles the loading of input and output CSV files, filtering for\n",
    "    specific players, and aligning input sequences with their corresponding\n",
    "    output sequences based on game, play, and NFL IDs.\n",
    "\n",
    "    Attributes:\n",
    "        train_dir (str): The directory containing the training CSV files.\n",
    "        input_sequences (pl.DataFrame): DataFrame containing input sequences.\n",
    "        output_sequences (pl.DataFrame): DataFrame containing output sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_dir):\n",
    "        self.train_dir = train_dir\n",
    "        self.input_sequences = None\n",
    "        self.output_sequences = None\n",
    "\n",
    "    def load_input_files(self):\n",
    "        \"\"\"Loads and filters input CSV files from the training directory using Polars.\n",
    "\n",
    "        Iterates through files starting with 'input' and ending with '.csv'.\n",
    "        Filters rows where 'player_to_predict' is True and groups them by\n",
    "        (game_id, play_id, nfl_id) to form sequences.\n",
    "        \"\"\"\n",
    "        input_files = sorted([f for f in os.listdir(self.train_dir) if f.startswith('input') and f.endswith('.csv')])\n",
    "        print(f\"Loading and filtering {len(input_files)} Input files...\")\n",
    "        \n",
    "        dataframes = []\n",
    "        for input_file in input_files:\n",
    "            input_path = os.path.join(self.train_dir, input_file)\n",
    "            try:\n",
    "                # Lazy load for efficiency, though read_csv is fine for smaller files\n",
    "                # Using read_csv to ensure we catch errors immediately\n",
    "                df = pl.read_csv(input_path, infer_schema_length=10000)\n",
    "                \n",
    "                # Filter for player_to_predict == True (case insensitive)\n",
    "                if \"player_to_predict\" in df.columns:\n",
    "                    df = df.filter(\n",
    "                        pl.col(\"player_to_predict\").cast(pl.Utf8).str.to_lowercase() == \"true\"\n",
    "                    )\n",
    "                \n",
    "                if df.height > 0:\n",
    "                    dataframes.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {input_file}: {e}\")\n",
    "\n",
    "        if not dataframes:\n",
    "            print(\"No valid input data found.\")\n",
    "            self.input_sequences = pl.DataFrame()\n",
    "            return\n",
    "\n",
    "        # Concatenate all input dataframes\n",
    "        full_df = pl.concat(dataframes, how=\"vertical_relaxed\")\n",
    "\n",
    "        # Process columns (Vectorized)\n",
    "        # Handle Booleans, Directions, Sides, etc.\n",
    "        \n",
    "        # Helper expression for boolean strings\n",
    "        def to_bool_float(col_name):\n",
    "            return (\n",
    "                pl.when(pl.col(col_name).cast(pl.Utf8).str.to_lowercase() == \"true\").then(1.0)\n",
    "                .when(pl.col(col_name).cast(pl.Utf8).str.to_lowercase() == \"false\").then(0.0)\n",
    "                .otherwise(0.0) # Default or handle errors\n",
    "            )\n",
    "\n",
    "        # Helper for direction\n",
    "        def to_dir_float(col_name):\n",
    "            return (\n",
    "                pl.when(pl.col(col_name).cast(pl.Utf8).str.to_lowercase() == \"left\").then(0.0)\n",
    "                .when(pl.col(col_name).cast(pl.Utf8).str.to_lowercase() == \"right\").then(1.0)\n",
    "                .otherwise(0.0)\n",
    "            )\n",
    "\n",
    "        # Helper for side\n",
    "        def to_side_float(col_name):\n",
    "            return (\n",
    "                pl.when(pl.col(col_name).cast(pl.Utf8).str.to_lowercase() == \"defense\").then(0.0)\n",
    "                .when(pl.col(col_name).cast(pl.Utf8).str.to_lowercase() == \"offense\").then(1.0)\n",
    "                .otherwise(0.0)\n",
    "            )\n",
    "            \n",
    "        # Apply transformations\n",
    "        # We need to identify columns to transform. Based on previous code:\n",
    "        # Booleans: player_to_predict (already filtered, but maybe others?)\n",
    "        # Direction: play_direction? (Not explicitly named in previous code but handled in generic process_value)\n",
    "        # Side: player_side?\n",
    "        \n",
    "        # For generic handling, we can inspect types, but for performance, explicit is better.\n",
    "        # Let's assume standard columns or iterate if needed.\n",
    "        # The previous code iterated every cell. Here we want vectorization.\n",
    "        # We will cast all remaining columns to float, hashing strings if needed.\n",
    "        \n",
    "        # Identify ID columns to exclude from feature processing\n",
    "        id_cols = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"player_to_predict\", \"time\", \"player_name\", \"num_frames_output\", \"ball_land_x\", \"ball_land_y\"]\n",
    "        feature_cols = [c for c in full_df.columns if c not in id_cols]\n",
    "        \n",
    "        expressions = []\n",
    "        for col in feature_cols:\n",
    "            # Handle specific columns based on name\n",
    "            if col == \"player_birth_date\":\n",
    "                expr = (pl.lit(\"2023-09-01\").str.to_date() - pl.col(col).str.to_date(format=\"%Y-%m-%d\", strict=False)).dt.total_days() / 365.25\n",
    "                expressions.append(expr.alias(col))\n",
    "                continue\n",
    "\n",
    "            # Check if column is string type\n",
    "            if full_df[col].dtype == pl.Utf8:\n",
    "                # Try specific conversions first\n",
    "                # We can't easily check content of every row efficiently without scanning\n",
    "                # So we apply a complex expression:\n",
    "                # If 'true'/'false' -> 1/0\n",
    "                # If 'left'/'right' -> 0/1\n",
    "                # If 'defense'/'offense' -> 0/1\n",
    "                # Else try cast float\n",
    "                # Else hash\n",
    "                \n",
    "                expr = (\n",
    "                    pl.when(pl.col(col).str.contains(r\"^\\d+-\\d+$\")).then(\n",
    "                        pl.col(col).str.extract(r\"(\\d+)-(\\d+)\", 1).cast(pl.Int32) * 12 +\n",
    "                        pl.col(col).str.extract(r\"(\\d+)-(\\d+)\", 2).cast(pl.Int32)\n",
    "                    )\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"true\").then(1.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"false\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"left\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"right\").then(1.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"defense\").then(0.0)\n",
    "                    .when(pl.col(col).str.to_lowercase() == \"offense\").then(1.0)\n",
    "                    .otherwise(\n",
    "                        # Try cast to float, if null (failed), then hash\n",
    "                        pl.col(col).cast(pl.Float64, strict=False).fill_null(\n",
    "                            pl.col(col).hash() % 10000\n",
    "                        )\n",
    "                    ).cast(pl.Float64).alias(col)\n",
    "                )\n",
    "                expressions.append(expr)\n",
    "            else:\n",
    "                # Already numeric (int or float), cast to float\n",
    "                expressions.append(pl.col(col).cast(pl.Float64).alias(col))\n",
    "\n",
    "        # Select IDs and processed features\n",
    "        full_df = full_df.with_columns(expressions)\n",
    "        \n",
    "        # Group by keys and aggregate into lists\n",
    "        # We assume the order is defined by frame_id or file order. \n",
    "        # If frame_id exists, sort by it.\n",
    "        if \"frame_id\" in full_df.columns:\n",
    "            full_df = full_df.sort([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n",
    "        \n",
    "        # Group and aggregate features into lists\n",
    "        # We want a list of lists (sequence of steps, where each step is a list of features)\n",
    "        # Polars agg_list creates a list of values for a column.\n",
    "        # We need to combine these columns into a single \"features\" column which is a list of lists?\n",
    "        # Or just keep them as separate columns of lists.\n",
    "        # The previous code produced: [[f1, f2, ...], [f1, f2, ...], ...] for each sequence.\n",
    "        \n",
    "        # Let's aggregate each feature column into a list\n",
    "        agg_exprs = [pl.col(c) for c in feature_cols]\n",
    "        \n",
    "        grouped = full_df.group_by([\"game_id\", \"play_id\", \"nfl_id\"], maintain_order=True).agg(agg_exprs)\n",
    "        \n",
    "        # Now we have:\n",
    "        # game_id, play_id, nfl_id, col1_list, col2_list, ...\n",
    "        # We need to transpose this to:\n",
    "        # game_id, play_id, nfl_id, [[col1_t0, col2_t0, ...], [col1_t1, col2_t1, ...]]\n",
    "        # This is hard in Polars directly.\n",
    "        # Easier: Convert to numpy/pandas later or iterate.\n",
    "        \n",
    "        # Actually, for Keras, we usually want (samples, timesteps, features).\n",
    "        # If we have separate columns of lists:\n",
    "        # col1: [t0, t1, t2]\n",
    "        # col2: [t0, t1, t2]\n",
    "        # We can stack them.\n",
    "        \n",
    "        self.input_sequences = grouped\n",
    "\n",
    "    def load_output_files(self):\n",
    "        \"\"\"Loads output CSV files from the training directory using Polars.\n",
    "\n",
    "        Iterates through files starting with 'output' and ending with '.csv'.\n",
    "        Extracts 'x' and 'y' features, grouping them by (game_id, play_id, nfl_id)\n",
    "        to form sequences.\n",
    "        \"\"\"\n",
    "        output_files = sorted([f for f in os.listdir(self.train_dir) if f.startswith('output') and f.endswith('.csv')])\n",
    "        print(f\"Loading {len(output_files)} Output files...\")\n",
    "        \n",
    "        features_to_keep = ['x', 'y']\n",
    "        dataframes = []\n",
    "        \n",
    "        for output_file in output_files:\n",
    "            output_path = os.path.join(self.train_dir, output_file)\n",
    "            try:\n",
    "                df = pl.read_csv(output_path, columns=['game_id', 'play_id', 'nfl_id'] + features_to_keep, infer_schema_length=10000)\n",
    "                dataframes.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {output_file}: {e}\")\n",
    "\n",
    "        if not dataframes:\n",
    "            print(\"No valid output data found.\")\n",
    "            self.output_sequences = pl.DataFrame()\n",
    "            return\n",
    "\n",
    "        full_df = pl.concat(dataframes, how=\"vertical_relaxed\")\n",
    "        \n",
    "        # Ensure float type\n",
    "        full_df = full_df.with_columns([\n",
    "            pl.col(c).cast(pl.Float64) for c in features_to_keep\n",
    "        ])\n",
    "        \n",
    "        # Sort if frame info is implicit (usually matches input)\n",
    "        # We don't have frame_id in output usually? Assuming same order.\n",
    "        # Ideally we should sort by something, but without frame_id we rely on file order.\n",
    "        \n",
    "        grouped = full_df.group_by([\"game_id\", \"play_id\", \"nfl_id\"], maintain_order=True).agg([\n",
    "            pl.col('x'),\n",
    "            pl.col('y')\n",
    "        ])\n",
    "        \n",
    "        self.output_sequences = grouped\n",
    "\n",
    "    def get_aligned_data(self):\n",
    "        \"\"\"Aligns input and output sequences based on common keys.\n",
    "\n",
    "        Loads both input and output files, finds the intersection of keys,\n",
    "        and creates aligned lists of sequences.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - X (np.ndarray): Array of input sequences (object array).\n",
    "                - y (np.ndarray): Array of output sequences (object array).\n",
    "        \"\"\"\n",
    "        self.load_input_files()\n",
    "        self.load_output_files()\n",
    "\n",
    "        print(\"Aligning Input and Output sequences...\")\n",
    "        \n",
    "        if self.input_sequences is None or self.input_sequences.is_empty():\n",
    "            print(\"Input sequences empty.\")\n",
    "            return np.array([]), np.array([])\n",
    "            \n",
    "        if self.output_sequences is None or self.output_sequences.is_empty():\n",
    "            print(\"Output sequences empty.\")\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "        # Join on keys\n",
    "        # Inner join to keep only matching sequences\n",
    "        joined = self.input_sequences.join(\n",
    "            self.output_sequences, \n",
    "            on=[\"game_id\", \"play_id\", \"nfl_id\"], \n",
    "            how=\"inner\",\n",
    "            suffix=\"_out\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Processing complete.\")\n",
    "        print(f\"Total Unique Sequences (Matches): {len(joined)}\")\n",
    "\n",
    "        if len(joined) == 0:\n",
    "            print(\"No matching data found.\")\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "        # Convert to the format expected by NFLDataSequence\n",
    "        # X: list of [ [f1, f2, ...], [f1, f2, ...] ]\n",
    "        # y: list of [ [x, y], [x, y] ... ]\n",
    "        \n",
    "        # The joined dataframe has columns:\n",
    "        # game_id, play_id, nfl_id, feat1_list, feat2_list, ..., x_list, y_list\n",
    "        \n",
    "        # We need to identify feature columns vs output columns\n",
    "        # Output columns are 'x' and 'y' (from output_sequences, might be renamed if collision)\n",
    "        # Actually, input also has 'x' and 'y' usually.\n",
    "        # In load_output_files, we aggregated 'x' and 'y'.\n",
    "        # In load_input_files, we aggregated all features.\n",
    "        # If input has 'x', 'y', they will collide.\n",
    "        # The join suffix=\"_out\" handles this. Output cols will be 'x_out', 'y_out'.\n",
    "        \n",
    "        # Input feature columns: all columns from input_sequences except keys\n",
    "        input_cols = [c for c in self.input_sequences.columns if c not in [\"game_id\", \"play_id\", \"nfl_id\"]]\n",
    "        output_cols = [\"x_out\" if \"x\" in input_cols else \"x\", \"y_out\" if \"y\" in input_cols else \"y\"]\n",
    "        \n",
    "        # Check if output cols exist\n",
    "        if output_cols[0] not in joined.columns:\n",
    "            # Maybe input didn't have x/y, so no suffix\n",
    "            output_cols = [\"x\", \"y\"]\n",
    "            \n",
    "        # Convert to numpy\n",
    "        # This is the heavy part.\n",
    "        # We can iterate rows or use map_elements?\n",
    "        # Ideally we want to stack the feature lists.\n",
    "        \n",
    "        # Let's extract input features as a list of arrays\n",
    "        # Each row i has [feat1_seq, feat2_seq, ...]\n",
    "        # We want [[feat1_t0, feat2_t0], [feat1_t1, feat2_t1], ...]\n",
    "        \n",
    "        # Efficient way:\n",
    "        # 1. Convert relevant columns to a dict of lists or similar\n",
    "        # 2. Iterate and stack\n",
    "        \n",
    "        print(\"Converting to NumPy arrays...\")\n",
    "        \n",
    "        # Extract input data\n",
    "        # shape: (n_samples, n_features, n_timesteps) roughly, but variable timesteps\n",
    "        # We want (n_samples, n_timesteps, n_features)\n",
    "        \n",
    "        # Get all input feature lists as a list of lists of lists?\n",
    "        # joined.select(input_cols).to_dict(as_series=False) gives {col: [seq1, seq2...]}\n",
    "        \n",
    "        # This might be memory intensive.\n",
    "        # Let's try row iteration with a generator or list comp\n",
    "        \n",
    "        # Pre-fetch column indices for speed\n",
    "        input_col_indices = [joined.columns.index(c) for c in input_cols]\n",
    "        output_col_indices = [joined.columns.index(c) for c in output_cols]\n",
    "        \n",
    "        rows = joined.iter_rows()\n",
    "        \n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        \n",
    "        for row in rows:\n",
    "            # Input\n",
    "            # row[i] is a list of values for feature i for this sequence\n",
    "            # We want to stack them: [[val_0_0, val_1_0...], [val_0_1, val_1_1...]]\n",
    "            # Zip is useful here\n",
    "            \n",
    "            # Get all feature sequences for this row\n",
    "            feature_seqs = [row[i] for i in input_col_indices]\n",
    "            # feature_seqs is [ [t0, t1...], [t0, t1...] ... ] (n_features, n_timesteps)\n",
    "            # We want (n_timesteps, n_features)\n",
    "            # zip(*feature_seqs) does exactly this transpose\n",
    "            \n",
    "            # Note: Polars lists might be None if empty? Assuming data is clean.\n",
    "            # Also assuming all feature lists have same length (they should if from same rows)\n",
    "            \n",
    "            X_seq = list(zip(*feature_seqs))\n",
    "            X_list.append(X_seq)\n",
    "            \n",
    "            # Output\n",
    "            out_seqs = [row[i] for i in output_col_indices]\n",
    "            y_seq = list(zip(*out_seqs))\n",
    "            y_list.append(y_seq)\n",
    "            \n",
    "        X = np.array(X_list, dtype=object)\n",
    "        y = np.array(y_list, dtype=object)\n",
    "        \n",
    "        print(f\"Initial X shape: {X.shape}\")\n",
    "        print(f\"Initial y shape: {y.shape}\")\n",
    "            \n",
    "        return X, y\n",
    "\n",
    "\n",
    "class NFLDataSequence(Sequence):\n",
    "    \"\"\"Keras Sequence for NFL data with automatic padding of variable-length sequences.\n",
    "\n",
    "    Inherits from `tensorflow.keras.utils.Sequence` to provide a data generator\n",
    "    that can be used with Keras models. Handles batching, shuffling, and\n",
    "    padding of sequences to a uniform length.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size=32, maxlen_x=None, maxlen_y=None, shuffle=True):\n",
    "        \"\"\"Initializes the NFLDataSequence.\n",
    "\n",
    "        Args:\n",
    "            X (list or np.ndarray): List of input sequences, where each sequence\n",
    "                is a list of time steps.\n",
    "            y (list or np.ndarray): List of output sequences, where each sequence\n",
    "                is a list of time steps.\n",
    "            batch_size (int, optional): Number of samples per batch. Defaults to 32.\n",
    "            maxlen_x (int, optional): Maximum length for input sequences. If None,\n",
    "                it is calculated from the data. Defaults to None.\n",
    "            maxlen_y (int, optional): Maximum length for output sequences. If None,\n",
    "                it is calculated from the data. Defaults to None.\n",
    "            shuffle (bool, optional): Whether to shuffle the data at the end of\n",
    "                each epoch. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        \n",
    "        # Determine max lengths if not provided\n",
    "        if maxlen_x is None:\n",
    "            self.maxlen_x = max(len(seq) for seq in X)\n",
    "        else:\n",
    "            self.maxlen_x = maxlen_x\n",
    "            \n",
    "        if maxlen_y is None:\n",
    "            self.maxlen_y = max(len(seq) for seq in y)\n",
    "        else:\n",
    "            self.maxlen_y = maxlen_y\n",
    "        \n",
    "        print(f\"NFLDataSequence initialized: {len(self.X)} samples, batch_size={batch_size}\")\n",
    "        print(f\"Max sequence lengths - X: {self.maxlen_x}, y: {self.maxlen_y}\")\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Computes the number of batches per epoch.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of batches.\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generates one batch of data.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the batch.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple (X_padded, y_padded) containing the padded input and\n",
    "                output sequences for the batch.\n",
    "        \"\"\"\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        # Get batch data\n",
    "        batch_X = [self.X[i] for i in batch_indices]\n",
    "        batch_y = [self.y[i] for i in batch_indices]\n",
    "        \n",
    "        # Process X sequences: handle mixed types\n",
    "        # With Polars preprocessing, data should already be numeric floats\n",
    "        # But let's ensure it's a list of lists of floats\n",
    "        \n",
    "        # batch_X is a list of sequences. Each sequence is a list of frames. Each frame is a list of features.\n",
    "        # We need to convert this to a 3D numpy array or list of 2D arrays for pad_sequences\n",
    "        \n",
    "        # Since we did the conversion in get_aligned_data, batch_X elements should be lists of tuples/lists of floats.\n",
    "        # We can directly pass this to pad_sequences if they are numeric.\n",
    "        \n",
    "        # Use pad_sequences for both X and y\n",
    "        # pad_sequences expects sequences of shape (n_samples, n_timesteps) for 2D\n",
    "        # For 3D (n_samples, n_timesteps, n_features), we need to pad manually or use padding='post'\n",
    "        \n",
    "        # Method: Pad each sequence to maxlen, filling with zeros\n",
    "        X_padded = pad_sequences(\n",
    "            batch_X, \n",
    "            maxlen=self.maxlen_x, \n",
    "            dtype='float32',\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0.0\n",
    "        )\n",
    "        \n",
    "        y_padded = pad_sequences(\n",
    "            batch_y,\n",
    "            maxlen=self.maxlen_y,\n",
    "            dtype='float32',\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0.0\n",
    "        )\n",
    "        \n",
    "        return X_padded, y_padded\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch.\n",
    "\n",
    "        If `self.shuffle` is True, the data indices are shuffled to ensure\n",
    "        random batch composition in the next epoch.\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "def create_tf_datasets(X, y, test_size=0.2, batch_size=32, maxlen_x=54, maxlen_y=54):\n",
    "    \"\"\"Splits data into training and validation sets and creates Keras Sequence datasets.\n",
    "\n",
    "    Uses `train_test_split` to divide the data and then wraps the resulting\n",
    "    sets in `NFLDataSequence` objects, which handle padding and batching.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input data (object array of variable-length sequences).\n",
    "        y (np.ndarray): Output data (object array of variable-length sequences).\n",
    "        test_size (float, optional): Proportion of the dataset to include in the\n",
    "            validation split. Defaults to 0.2.\n",
    "        batch_size (int, optional): Batch size for the datasets. Defaults to 32.\n",
    "        maxlen_x (int, optional): Maximum length for input sequences. If None,\n",
    "            auto-detects from the training set. Defaults to 10.\n",
    "        maxlen_y (int, optional): Maximum length for output sequences. If None,\n",
    "            auto-detects from the training set. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - train_sequence (NFLDataSequence): The training data sequence.\n",
    "            - val_sequence (NFLDataSequence): The validation data sequence.\n",
    "            Returns (None, None) if an error occurs.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Keras Sequence Datasets with Padding ---\")\n",
    "    \n",
    "    try:\n",
    "        # Convert object arrays to lists\n",
    "        X_list = X.tolist()\n",
    "        y_list = y.tolist()\n",
    "        \n",
    "        # Split into train and validation\n",
    "        print(f\"Splitting data (test_size={test_size})...\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_list, y_list, \n",
    "            test_size=test_size, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"Train size: {len(X_train)}\")\n",
    "        print(f\"Val size: {len(X_val)}\")\n",
    "        \n",
    "        # Create Sequence objects\n",
    "        print(\"Creating Training Sequence...\")\n",
    "        train_sequence = NFLDataSequence(\n",
    "            X_train, y_train, \n",
    "            batch_size=batch_size,\n",
    "            maxlen_x=maxlen_x,\n",
    "            maxlen_y=maxlen_y,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(\"Creating Validation Sequence...\")\n",
    "        val_sequence = NFLDataSequence(\n",
    "            X_val, y_val,\n",
    "            batch_size=batch_size,\n",
    "            maxlen_x=train_sequence.maxlen_x,  # Use same max lengths as training\n",
    "            maxlen_y=train_sequence.maxlen_y,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        print(\"Sequences created successfully.\")\n",
    "        print(f\"Training batches per epoch: {len(train_sequence)}\")\n",
    "        print(f\"Validation batches per epoch: {len(val_sequence)}\")\n",
    "        \n",
    "        return train_sequence, val_sequence\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Keras sequences: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TRAIN_DIR = '/home/samer/Desktop/competitions/NFL_Big_Data_Bowl_2026_dev/nfl-big-data-bowl-2026-prediction/train/'\n",
    "    \n",
    "    loader = NFLDataLoader(TRAIN_DIR)\n",
    "    X, y = loader.get_aligned_data()\n",
    "\n",
    "    print(\"\\n--- Final Data Shapes ---\")\n",
    "    print(f\"X (Input) Shape: {X.shape}\")\n",
    "    print(f\"y (Output) Shape: {y.shape}\")\n",
    "\n",
    "    if len(X) > 0:\n",
    "        print(f\"Sample Input Sequence Length: {len(X[0])}\")\n",
    "        print(f\"Sample Output Sequence Length: {len(y[0])}\")\n",
    "\n",
    "    # Create Keras Sequences with padding\n",
    "    train_seq, val_seq = create_tf_datasets(X, y, batch_size=32)\n",
    "    \n",
    "    if train_seq:\n",
    "        print(\"\\nVerifying Sequence Element:\")\n",
    "        # Get one batch to verify shapes\n",
    "        x_batch, y_batch = train_seq[0]\n",
    "        print(f\"Batch X shape: {x_batch.shape}\")\n",
    "        print(f\"Batch y shape: {y_batch.shape}\")\n",
    "        print(f\"Max sequence lengths - X: {train_seq.maxlen_x}, y: {train_seq.maxlen_y}\")\n",
    "\n",
    "    print(\"\\nData loading, alignment, and sequence creation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-26T04:52:12.825Z",
     "iopub.execute_input": "2025-11-26T04:32:10.631764Z",
     "iopub.status.busy": "2025-11-26T04:32:10.631345Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 26m 48s]\n",
      "val_loss: 566.2125854492188\n",
      "\n",
      "Best val_loss So Far: 480.52069091796875\n",
      "Total elapsed time: 00h 33m 22s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.00081926        |3.6117e-05        |lr\n",
      "696               |192               |lu\n",
      "3.7001e-06        |1.1033e-08        |kr\n",
      "sigmoid           |hard_sigmoid      |af\n",
      "0.00052148        |0.00015039        |wd\n",
      "\n",
      "Epoch 1/5\n",
      "\u001b[1m 756/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2:20\u001b[0m 356ms/step - loss: 530.3284 - mean_absolute_error: 12.0249"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 205\u001b[0m\n\u001b[1;32m    202\u001b[0m train_seq, val_seq \u001b[38;5;241m=\u001b[39m create_tf_datasets(X, y, test_size\u001b[38;5;241m=\u001b[39mtest_size, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Run the hyperparameter experimentation\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m \u001b[43mexperimenting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_seq\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 166\u001b[0m, in \u001b[0;36mexperimenting\u001b[0;34m(training_dataset, validation_data)\u001b[0m\n\u001b[1;32m    162\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch_space_summary() \u001b[38;5;66;03m# Print a summary of the hyperparameter search space\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# NFLDataSequence is already batched, no need to call batch() again\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Run the hyperparameter search experiments\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m tuner\u001b[38;5;241m.\u001b[39mresults_summary()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    242\u001b[0m     ):\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    255\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    398\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[0;32m--> 399\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    239\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    240\u001b[0m     ):\n\u001b[0;32m--> 241\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1703\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import sys\n",
    "import keras_tuner\n",
    "\n",
    "# Add the manual_data_processing directory to the path\n",
    "# sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'manual_data_processing'))\n",
    "\n",
    "# from csv_to_numpy import NFLDataLoader, create_tf_datasets\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Builds a compiled Keras LSTM model with hyperparameters to be experimented on.\n",
    "\n",
    "    This function defines the architecture of the LSTM model for sequence-to-sequence prediction.\n",
    "    It incorporates hyperparameter search spaces for key model parameters like learning rate,\n",
    "    number of LSTM units, kernel regularization, and activation functions.\n",
    "\n",
    "    Args:\n",
    "        hp (keras_tuner.HyperParameters): An instance of Keras Tuner's HyperParameters class,\n",
    "                                          used to define the search space for hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Keras LSTM model with hyperparameters set by Keras Tuner.\n",
    "    \"\"\"\n",
    "    k_init = keras.initializers.RandomNormal(mean=1503.17, \n",
    "    stddev=2755.38, \n",
    "    seed=42)\n",
    "    SEED = 42\n",
    "    # Define hyperparameter search spaces for tuning\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-7, max_value=1e-3, sampling=\"log\")\n",
    "    layer_u = hp.Int(\"lu\", min_value=160, max_value=1024, step=8)\n",
    "    kernel_r = hp.Float(\"kr\", min_value=1e-10, max_value=1e-5, sampling=\"log\")\n",
    "    acti_f = hp.Choice(\"af\", [\"sigmoid\", \"hard_sigmoid\", \"tanh\", \"relu\", \"softmax\", \"linear\"])\n",
    "    weight_d = hp.Float(\"wd\", min_value=1e-10, max_value=0.0009, sampling=\"log\")\n",
    "\n",
    "    # Define the model structure using Keras Sequential API\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        keras.layers.Input(shape=(input_seq_length, input_features)),\n",
    "        \n",
    "        # Encoder LSTM layers\n",
    "        keras.layers.LSTM(\n",
    "            units=layer_u,\n",
    "            activation=acti_f,\n",
    "            kernel_initializer=k_init,\n",
    "            return_sequences=True,\n",
    "            # kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=layer_u // 2,\n",
    "            activation=acti_f,\n",
    "            kernel_initializer=k_init,\n",
    "            return_sequences=True,\n",
    "            # kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        # keras.layers.LSTM(\n",
    "        #     units=layer_u // 2,\n",
    "        #     activation=acti_f,\n",
    "        #     kernel_initializer=k_init,\n",
    "        #     return_sequences=True,\n",
    "        #     # kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "        #     seed=SEED,\n",
    "        # ),\n",
    "        # keras.layers.LSTM(\n",
    "        #     units=layer_u // 2,\n",
    "        #     activation=acti_f,\n",
    "        #     kernel_initializer=k_init,\n",
    "        #     return_sequences=True,\n",
    "        #     # kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "        #     seed=SEED,\n",
    "        # ),\n",
    "        # keras.layers.LSTM(\n",
    "        #     units=layer_u // 2,\n",
    "        #     activation=acti_f,\n",
    "        #     kernel_initializer=k_init,\n",
    "        #     return_sequences=True,\n",
    "        #     # kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "        #     seed=SEED,\n",
    "        # ),\n",
    "        # layers.RepeatVector(output_seq_length),\n",
    "        keras.layers.LSTM(\n",
    "            units=32,\n",
    "            activation=\"sigmoid\",\n",
    "            kernel_initializer=k_init,\n",
    "            return_sequences=True,\n",
    "            # kernel_regularizer=keras.regularizers.L2(l2=0.00000195),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        # Crop or slice to match output sequence length\n",
    "        # layers.Lambda(lambda x: x[:, :output_seq_length, :]),\n",
    "        # TimeDistributed dense layer for output features\n",
    "        layers.TimeDistributed(\n",
    "        keras.layers.Dense(units=output_features, activation=\"linear\")\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # Compile the model with a tunable optimizer and metrics\n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            global_clipnorm=1,\n",
    "            amsgrad=False,\n",
    "            weight_decay=weight_d, # Tunable weight decay\n",
    "        ),\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError()],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def experimenting(training_dataset, validation_data):\n",
    "    \"\"\"\n",
    "    Runs Keras Tuner experiments for the LSTM model using the RandomSearch algorithm.\n",
    "\n",
    "    This function initializes a `RandomSearch` tuner with the `build_model` function,\n",
    "    configures the search objective (minimizing validation loss), and then executes\n",
    "    the hyperparameter search across the defined search spaces. It prints summaries\n",
    "    of the search space and the results.\n",
    "\n",
    "    Args:\n",
    "        training_dataset: NFLDataSequence object for training data\n",
    "        validation_data: NFLDataSequence object for validation data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    hp = keras_tuner.HyperParameters()\n",
    "    \n",
    "    # Get a batch from the sequence to determine shapes\n",
    "    x_batch, y_batch = training_dataset[0]\n",
    "    global input_features, input_seq_length, output_seq_length, output_features\n",
    "    input_seq_length = x_batch.shape[1]\n",
    "    input_features = x_batch.shape[2]\n",
    "    output_seq_length = y_batch.shape[1]\n",
    "    output_features = y_batch.shape[2]\n",
    "    \n",
    "    print(f\"\\nDetected shapes:\")\n",
    "    print(f\"  Input: ({input_seq_length}, {input_features})\")\n",
    "    print(f\"  Output: ({output_seq_length}, {output_features})\")\n",
    "    \n",
    "    build_model(hp) # Instantiate a dummy model to build the search space\n",
    "\n",
    "    # Initialize Keras Tuner's RandomSearch algorithm\n",
    "    tuner = keras_tuner.RandomSearch(\n",
    "        hypermodel=build_model,\n",
    "        max_trials=10, # Maximum number of hyperparameter combinations to try\n",
    "        objective=keras_tuner.Objective(\"val_loss\", \"min\"),   # Objective is to minimize validation loss\n",
    "        executions_per_trial=1, # Number of models to train for each trial (1 for efficiency)\n",
    "        overwrite=True, # Overwrite previous results in the directory\n",
    "        directory=os.getenv(\"KERAS_TUNER_EXPERIMENTS_DIR\", \"./tuner_results\"), # Directory to save experiment logs and checkpoints\n",
    "        project_name=\"nfl_prediction\", # Name of the Keras Tuner project\n",
    "        seed = 42,\n",
    "        max_consecutive_failed_trials=5,\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary() # Print a summary of the hyperparameter search space\n",
    "\n",
    "    # NFLDataSequence is already batched, no need to call batch() again\n",
    "    # Run the hyperparameter search experiments\n",
    "    tuner.search(\n",
    "        training_dataset, \n",
    "        validation_data=validation_data, \n",
    "        epochs=5\n",
    "    )\n",
    "\n",
    "    tuner.results_summary() # Print a summary of the best performing trials\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dir = '/home/samer/Desktop/competitions/NFL_Big_Data_Bowl_2026_dev/nfl-big-data-bowl-2026-prediction/train'\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    test_size = 0.2\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"NFL Big Data Bowl 2026 - Predictor Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n[1/4] Loading data from CSV files...\")\n",
    "    loader = NFLDataLoader(train_dir)\n",
    "    X, y = loader.get_aligned_data()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No data loaded. Please check the data directory.\")\n",
    "    \n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  Total sequences: {len(X)}\")\n",
    "    print(f\"  Sample input sequence length: {len(X[0])}\")\n",
    "    print(f\"  Sample output sequence length: {len(y[0])}\")\n",
    "    print(f\"  Input features per timestep: {len(X[0][0]) if len(X[0]) > 0 else 0}\")\n",
    "    print(f\"  Output features per timestep: {len(y[0][0]) if len(y[0]) > 0 else 0}\")\n",
    "    \n",
    "    # Create Keras Sequences with padding\n",
    "    print(f\"\\n[2/4] Creating training and validation sequences (test_size={test_size})...\")\n",
    "    train_seq, val_seq = create_tf_datasets(X, y, test_size=test_size, batch_size=batch_size)\n",
    "    \n",
    "    # Run the hyperparameter experimentation\n",
    "    experimenting(train_seq, val_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_seq2seq_model(hp, \n",
    "                        input_seq_length, input_features,\n",
    "                        output_seq_length, output_features):\n",
    "    \"\"\"\n",
    "    Returns a compiled Keras model.\n",
    "    hp  HyperParameters object supplied by KerasTuner.\n",
    "    \"\"\"\n",
    "    # ---------- Hyperparameters ----------\n",
    "    # Number of LSTM layers (encoder + decoder)\n",
    "    n_encoder_layers = hp.Int('enc_layers', 2, 4, step=1)\n",
    "    n_decoder_layers = hp.Int('dec_layers', 2, 4, step=1)\n",
    "\n",
    "    # LSTM units per layer (same for all layers for simplicity)\n",
    "    lstm_units = hp.Choice('lstm_units', [64, 128, 256, 384])\n",
    "\n",
    "    # Dropout rate\n",
    "    dropout_rate = hp.Float('dropout', 0.0, 0.3, step=0.05)\n",
    "\n",
    "    # Learningrate schedule\n",
    "    init_lr = hp.Float('init_lr', 1e-4, 5e-3, sampling='log')\n",
    "    \n",
    "    # ---------- Model ----------\n",
    "    # Encoder\n",
    "    encoder_inputs = layers.Input(shape=(input_seq_length, input_features),\n",
    "                                  name='encoder_inputs')\n",
    "    x = encoder_inputs\n",
    "    for i in range(n_encoder_layers):\n",
    "        # Residual LSTM block\n",
    "        lstm_out = layers.LSTM(lstm_units,\n",
    "                               return_sequences=True,\n",
    "                               name=f'enc_lstm_{i+1}')(x)\n",
    "        lstm_out = layers.Dropout(dropout_rate,\n",
    "                                  name=f'enc_dropout_{i+1}')(lstm_out)\n",
    "        # Add residual connection (if dimensions match)\n",
    "        if lstm_out.shape[-1] == x.shape[-1]:\n",
    "            lstm_out = layers.Add(name=f'enc_res_{i+1}')([x, lstm_out])\n",
    "        # Normalise\n",
    "        lstm_out = layers.LayerNormalization(name=f'enc_norm_{i+1}')(lstm_out)\n",
    "        x = lstm_out\n",
    "\n",
    "    # Grab the final hidden state as the latent vector\n",
    "    latent = layers.LSTM(lstm_units,\n",
    "                         return_sequences=False,\n",
    "                         name='latent')(x)\n",
    "\n",
    "    # Decoder  repeat latent vector for each output timestep\n",
    "    decoder_inputs = layers.RepeatVector(output_seq_length,\n",
    "                                         name='repeat_latent')(latent)\n",
    "    y = decoder_inputs\n",
    "    for i in range(n_decoder_layers):\n",
    "        lstm_out = layers.LSTM(lstm_units,\n",
    "                               return_sequences=True,\n",
    "                               name=f'dec_lstm_{i+1}')(y)\n",
    "        lstm_out = layers.Dropout(dropout_rate,\n",
    "                                  name=f'dec_dropout_{i+1}')(lstm_out)\n",
    "        # Residual connection (again only when shapes match)\n",
    "        if lstm_out.shape[-1] == y.shape[-1]:\n",
    "            lstm_out = layers.Add(name=f'dec_res_{i+1}')([y, lstm_out])\n",
    "        lstm_out = layers.LayerNormalization(name=f'dec_norm_{i+1}')(lstm_out)\n",
    "        y = lstm_out\n",
    "\n",
    "    # Final TimeDistributed dense layer\n",
    "    decoder_outputs = layers.TimeDistributed(\n",
    "        layers.Dense(output_features, activation='linear'),\n",
    "        name='decoder_output')(y)\n",
    "\n",
    "    model = models.Model(inputs=encoder_inputs, outputs=decoder_outputs,\n",
    "                         name='tunable_seq2seq')\n",
    "\n",
    "    # ---------- Learningrate schedule ----------\n",
    "    # Simplified to just CosineDecay to avoid TypeError\n",
    "    total_steps = hp.Int('total_steps', 1000, 5000, step=500)\n",
    "    learning_rate = optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=init_lr,\n",
    "        decay_steps=total_steps,\n",
    "        alpha=1e-5)\n",
    "\n",
    "    optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuner_search(train_seq, val_seq,\n",
    "                 input_seq_len, input_feat,\n",
    "                 output_seq_len, output_feat,\n",
    "                 max_trials=30, epochs_per_trial=15):\n",
    "    \"\"\"\n",
    "    Runs Hyperband and returns the best model + history.\n",
    "    \"\"\"\n",
    "    tuner = kt.Hyperband(\n",
    "        hypermodel=lambda hp: build_seq2seq_model(\n",
    "            hp,\n",
    "            input_seq_length=input_seq_len,\n",
    "            input_features=input_feat,\n",
    "            output_seq_length=output_seq_len,\n",
    "            output_features=output_feat),\n",
    "        objective='val_loss',\n",
    "        max_epochs=epochs_per_trial,\n",
    "        factor=3,\n",
    "        directory='kt_tuner',\n",
    "        project_name='nfl_seq2seq',\n",
    "        overwrite=True)\n",
    "\n",
    "    # Earlystopping inside each trial\n",
    "    stop_early = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=3,\n",
    "                                         restore_best_weights=True)\n",
    "\n",
    "    tuner.search(train_seq,\n",
    "                 validation_data=val_seq,\n",
    "                 callbacks=[stop_early],\n",
    "                 verbose=1)\n",
    "\n",
    "    # Retrieve the best hyperparameters & model\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    # Train the best model a little longer (optional)\n",
    "    final_history = best_model.fit(\n",
    "        train_seq,\n",
    "        validation_data=val_seq,\n",
    "        epochs=epochs_per_trial * 2,   # give it more epochs now that we know the arch.\n",
    "        callbacks=[callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                           patience=5,\n",
    "                                           restore_best_weights=True)],\n",
    "        verbose=1)\n",
    "\n",
    "    return best_model, final_history, best_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 02m 14s]\n",
      "val_loss: 373.9458312988281\n",
      "\n",
      "Best val_loss So Far: 371.5519104003906\n",
      "Total elapsed time: 00h 22m 53s\n",
      "\n",
      "Search: Running Trial #16\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "3                 |4                 |enc_layers\n",
      "2                 |4                 |dec_layers\n",
      "256               |128               |lstm_units\n",
      "0.2               |0.15              |dropout\n",
      "0.00055933        |0.00024339        |init_lr\n",
      "3000              |4000              |total_steps\n",
      "4                 |2                 |tuner/epochs\n",
      "2                 |0                 |tuner/initial_epoch\n",
      "2                 |2                 |tuner/bracket\n",
      "1                 |0                 |tuner/round\n",
      "0002              |None              |tuner/trial_id\n",
      "\n",
      "Epoch 3/4\n",
      "\u001b[1m 605/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 23ms/step - loss: 376.9789 - mae: 12.5361"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m output_feat     \u001b[38;5;241m=\u001b[39m output_features    \u001b[38;5;66;03m# same as input_features for autoreconstruction\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 2  Launch the tuner\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# ------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m best_model, best_history, best_hp \u001b[38;5;241m=\u001b[39m \u001b[43mtuner_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_feat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_feat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# increase if you have more time\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# short trials for speed\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Best hyperparameters ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m best_hp\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m, in \u001b[0;36mtuner_search\u001b[0;34m(train_seq, val_seq, input_seq_len, input_feat, output_seq_len, output_feat, max_trials, epochs_per_trial)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Earlystopping inside each trial\u001b[39;00m\n\u001b[1;32m     23\u001b[0m stop_early \u001b[38;5;241m=\u001b[39m callbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m                                      patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     25\u001b[0m                                      restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m             \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m             \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstop_early\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m             \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Retrieve the best hyperparameters & model\u001b[39;00m\n\u001b[1;32m     33\u001b[0m best_hp \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    242\u001b[0m     ):\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    255\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/tuners/hyperband.py:427\u001b[0m, in \u001b[0;36mHyperband.run_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m     fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuner/epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    426\u001b[0m     fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuner/initial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras_tuner/src/engine/hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    398\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[0;32m--> 399\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    239\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    240\u001b[0m     ):\n\u001b[0;32m--> 241\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1703\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1  Gather shapes (already computed earlier in the notebook)\n",
    "# ------------------------------------------------------------------\n",
    "input_seq_len   = input_seq_length   # e.g. 28\n",
    "input_feat      = input_features     # e.g. 23\n",
    "output_seq_len  = output_seq_length  # e.g. 5\n",
    "output_feat     = output_features    # same as input_features for autoreconstruction\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2  Launch the tuner\n",
    "# ------------------------------------------------------------------\n",
    "best_model, best_history, best_hp = tuner_search(\n",
    "        train_seq=train_seq,\n",
    "        val_seq=val_seq,\n",
    "        input_seq_len=input_seq_len,\n",
    "        input_feat=input_feat,\n",
    "        output_seq_len=output_seq_len,\n",
    "        output_feat=output_feat,\n",
    "        max_trials=30,          # increase if you have more time\n",
    "        epochs_per_trial=12)    # short trials for speed\n",
    "\n",
    "print(\"\\n=== Best hyperparameters ===\")\n",
    "for name, value in best_hp.values.items():\n",
    "    print(f\"{name}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(best_history.history['loss'], label='Training loss')\n",
    "plt.plot(best_history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Best model  Training & Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NFL Big Data Bowl 2026 - Predictor Training\n",
      "============================================================\n",
      "\n",
      "[1/4] Loading data from CSV files...\n",
      "Loading and filtering 18 Input files...\n",
      "Loading 18 Output files...\n",
      "Aligning Input and Output sequences...\n",
      "Processing complete.\n",
      "Total Unique Sequences (Matches): 46045\n",
      "Converting to NumPy arrays...\n",
      "Initial X shape: (46045,)\n",
      "Initial y shape: (46045,)\n",
      "\n",
      "Data Summary:\n",
      "  Total sequences: 46045\n",
      "  Sample input sequence length: 26\n",
      "  Sample output sequence length: 21\n",
      "  Input features per timestep: 18\n",
      "  Output features per timestep: 2\n",
      "\n",
      "[2/4] Creating training and validation sequences (test_size=0.2)...\n",
      "\n",
      "--- Creating Keras Sequence Datasets with Padding ---\n",
      "Splitting data (test_size=0.2)...\n",
      "Train size: 36836\n",
      "Val size: 9209\n",
      "Creating Training Sequence...\n",
      "NFLDataSequence initialized: 36836 samples, batch_size=32\n",
      "Max sequence lengths - X: 1, y: 1\n",
      "Creating Validation Sequence...\n",
      "NFLDataSequence initialized: 9209 samples, batch_size=32\n",
      "Max sequence lengths - X: 1, y: 1\n",
      "Sequences created successfully.\n",
      "Training batches per epoch: 1152\n",
      "Validation batches per epoch: 288\n",
      "\n",
      "Sequence Shapes:\n",
      "  Input: (batch_size, 1, 18)\n",
      "  Output: (batch_size, 1, 2)\n",
      "\n",
      "[3/4] Building sequence-to-sequence model...\n",
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">696</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,990,560</span> \n",
       "\n",
       " lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">348</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">1,454,640</span> \n",
       "\n",
       " lstm_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">348</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">970,224</span> \n",
       "\n",
       " lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">348</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">970,224</span> \n",
       "\n",
       " lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">348</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">970,224</span> \n",
       "\n",
       " lstm_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">48,768</span> \n",
       "\n",
       " time_distributed_1               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                                                      \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m696\u001b[0m)              \u001b[38;5;34m1,990,560\u001b[0m \n",
       "\n",
       " lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m348\u001b[0m)              \u001b[38;5;34m1,454,640\u001b[0m \n",
       "\n",
       " lstm_8 (\u001b[38;5;33mLSTM\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m348\u001b[0m)                \u001b[38;5;34m970,224\u001b[0m \n",
       "\n",
       " lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m348\u001b[0m)                \u001b[38;5;34m970,224\u001b[0m \n",
       "\n",
       " lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m348\u001b[0m)                \u001b[38;5;34m970,224\u001b[0m \n",
       "\n",
       " lstm_11 (\u001b[38;5;33mLSTM\u001b[0m)                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  \u001b[38;5;34m48,768\u001b[0m \n",
       "\n",
       " time_distributed_1               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                       \u001b[38;5;34m66\u001b[0m \n",
       " (\u001b[38;5;33mTimeDistributed\u001b[0m)                                                      \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,404,706</span> (24.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,404,706\u001b[0m (24.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,404,706</span> (24.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,404,706\u001b[0m (24.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/4] Training model for 20 epochs...\n",
      "Starting model training...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samer/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 730/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2267.3192 - mae: 40.4973"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 17:21:21.286350: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_163', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-26 17:21:22.077779: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_163', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2187.1285 - mae: 39.4973"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 17:21:28.981750: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_73', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from None to 1597.72180, saving model to best_model.keras\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 11ms/step - loss: 1975.0459 - mae: 36.8037 - val_loss: 1597.7218 - val_mae: 31.8607\n",
      "Epoch 2/20\n",
      "\u001b[1m1146/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1462.1113 - mae: 30.0195\n",
      "Epoch 2: val_loss improved from 1597.72180 to 1067.50989, saving model to best_model.keras\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 1327.8428 - mae: 28.1716 - val_loss: 1067.5099 - val_mae: 24.7425\n",
      "Epoch 3/20\n",
      "\u001b[1m1140/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 970.0174 - mae: 23.4465\n",
      "Epoch 3: val_loss improved from 1067.50989 to 722.47778, saving model to best_model.keras\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 883.0762 - mae: 22.2754 - val_loss: 722.4778 - val_mae: 20.0996\n",
      "Epoch 4/20\n",
      "\u001b[1m1148/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 676.5377 - mae: 19.4557\n",
      "Epoch 4: val_loss improved from 722.47778 to 524.41815, saving model to best_model.keras\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 618.2572 - mae: 18.6354 - val_loss: 524.4182 - val_mae: 17.2776\n",
      "Epoch 5/20\n",
      "\u001b[1m1148/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 495.3516 - mae: 16.8796\n",
      "Epoch 5: val_loss improved from 524.41815 to 408.59558, saving model to best_model.keras\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 460.1112 - mae: 16.3368 - val_loss: 408.5956 - val_mae: 15.5951\n",
      "Epoch 6/20\n",
      "\u001b[1m1149/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 395.6571 - mae: 15.4418\n",
      "Epoch 6: val_loss improved from 408.59558 to 365.05060, saving model to best_model.keras\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 380.8065 - mae: 15.2027 - val_loss: 365.0506 - val_mae: 14.9750\n",
      "Epoch 7/20\n",
      "\u001b[1m1145/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 361.8782 - mae: 14.9279\n",
      "Epoch 7: val_loss improved from 365.05060 to 361.00616, saving model to best_model.keras\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 362.6019 - mae: 14.9500 - val_loss: 361.0062 - val_mae: 14.9203\n",
      "Epoch 8/20\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 361.3613 - mae: 14.9319\n",
      "Epoch 8: val_loss improved from 361.00616 to 360.87573, saving model to best_model.keras\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 361.1461 - mae: 14.9353 - val_loss: 360.8757 - val_mae: 14.9210\n",
      "Epoch 9/20\n",
      "\u001b[1m1150/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 363.8518 - mae: 14.9823\n",
      "Epoch 9: val_loss did not improve from 360.87573\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 361.0975 - mae: 14.9344 - val_loss: 360.9245 - val_mae: 14.9220\n",
      "Epoch 10/20\n",
      "\u001b[1m1149/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 358.5309 - mae: 14.8929\n",
      "Epoch 10: val_loss did not improve from 360.87573\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 361.1055 - mae: 14.9357 - val_loss: 360.8939 - val_mae: 14.9199\n",
      "Epoch 11/20\n",
      "\u001b[1m1139/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 365.9842 - mae: 15.0182\n",
      "Epoch 11: val_loss did not improve from 360.87573\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 361.1100 - mae: 14.9351 - val_loss: 360.8952 - val_mae: 14.9212\n",
      "Epoch 12/20\n",
      "\u001b[1m1142/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 363.7170 - mae: 15.0018\n",
      "Epoch 12: val_loss did not improve from 360.87573\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 361.1081 - mae: 14.9353 - val_loss: 360.8990 - val_mae: 14.9208\n",
      "Epoch 13/20\n",
      "\u001b[1m1150/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 363.2665 - mae: 14.9762\n",
      "Epoch 13: val_loss did not improve from 360.87573\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 361.1060 - mae: 14.9349 - val_loss: 360.8821 - val_mae: 14.9214\n",
      "Epoch 14/20\n",
      "\u001b[1m1142/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 364.6276 - mae: 14.9817\n",
      "Epoch 14: val_loss did not improve from 360.87573\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 361.1019 - mae: 14.9351 - val_loss: 360.8993 - val_mae: 14.9202\n",
      "Epoch 15/20\n",
      "\u001b[1m1149/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 362.4890 - mae: 14.9522\n",
      "Epoch 15: val_loss did not improve from 360.87573\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 361.1021 - mae: 14.9350 - val_loss: 360.8890 - val_mae: 14.9205\n",
      "Epoch 16/20\n",
      "\u001b[1m1141/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 359.9898 - mae: 14.9490\n",
      "Epoch 16: val_loss did not improve from 360.87573\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 361.1117 - mae: 14.9351 - val_loss: 360.8842 - val_mae: 14.9201\n",
      "Epoch 17/20\n",
      "\u001b[1m1149/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 361.9279 - mae: 14.9393\n",
      "Epoch 17: val_loss did not improve from 360.87573\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 361.1089 - mae: 14.9348 - val_loss: 360.8918 - val_mae: 14.9201\n",
      "Epoch 18/20\n",
      "\u001b[1m1150/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 358.5100 - mae: 14.9038\n",
      "Epoch 18: val_loss did not improve from 360.87573\n",
      "\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 361.1001 - mae: 14.9351 - val_loss: 360.9150 - val_mae: 14.9213\n",
      "Epoch 19/20\n",
      "\u001b[1m 406/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 366.6267 - mae: 15.0407"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 228\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Best validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 228\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 208\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[4/4] Training model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 208\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[1;32m    211\u001b[0m final_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfl_predictor_final.keras\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 134\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_sequence, val_sequence, epochs, callbacks)\u001b[0m\n\u001b[1;32m    131\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mextend([early_stopping, model_checkpoint])\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting model training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 134\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    398\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[0;32m--> 399\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:242\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    239\u001b[0m     iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    240\u001b[0m ):\n\u001b[1;32m    241\u001b[0m     opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m--> 242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mget_value()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/data/ops/optional_ops.py:176\u001b[0m, in \u001b[0;36m_OptionalImpl.has_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.12/site-packages/tensorflow/python/ops/gen_optional_ops.py:172\u001b[0m, in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m    171\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptionalHasValue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the manual_data_processing directory to the path\n",
    "# sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'manual_data_processing'))\n",
    "\n",
    "# from csv_to_numpy import NFLDataLoader, create_tf_datasets\n",
    "\n",
    "def build_seq2seq_model(input_seq_length, input_features, output_seq_length, output_features, lstm_units=128):\n",
    "    \"\"\"\n",
    "    Builds a sequence-to-sequence model with LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        input_seq_length (int): The length of input sequences (time steps).\n",
    "        input_features (int): The number of input features per timestep.\n",
    "        output_seq_length (int): The length of output sequences (time steps).\n",
    "        output_features (int): The number of output features per timestep.\n",
    "        lstm_units (int): The number of units in the LSTM layers.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = 42\n",
    "    # Encoder-decoder architecture for sequence-to-sequence prediction\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        keras.layers.Input(shape=(input_seq_length, input_features)),\n",
    "        \n",
    "        # Encoder LSTM layers\n",
    "        keras.layers.LSTM(\n",
    "            units=696,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=3.7001e-06),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=696 // 2,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=3.7001e-06),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=696 // 2,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=3.7001e-06),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=696 // 2,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=3.7001e-06),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=696 // 2,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=3.7001e-06),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        # layers.RepeatVector(output_seq_length),\n",
    "        keras.layers.LSTM(\n",
    "            units=32,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            # kernel_regularizer=keras.regularizers.L2(l2=0.00000195),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        # Crop or slice to match output sequence length\n",
    "        # layers.Lambda(lambda x: x[:, :output_seq_length, :]),\n",
    "        # TimeDistributed dense layer for output features\n",
    "        layers.TimeDistributed(\n",
    "            keras.layers.Dense(units=output_features, activation=\"linear\")\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    cosine_decay = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=415000,\n",
    "    alpha=1e-5,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.00081926),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, train_sequence, val_sequence, epochs=10, callbacks=None):\n",
    "    \"\"\"\n",
    "    Trains the Keras model using Keras Sequence objects.\n",
    "    \n",
    "    Args:\n",
    "        model: The Keras model to train\n",
    "        train_sequence: Training data sequence (NFLDataSequence)\n",
    "        val_sequence: Validation data sequence (NFLDataSequence)\n",
    "        epochs (int): Number of training epochs\n",
    "        callbacks: List of Keras callbacks\n",
    "    \n",
    "    Returns:\n",
    "        history: Training history object\n",
    "    \"\"\"\n",
    "    if callbacks is None:\n",
    "        callbacks = []\n",
    "    \n",
    "    # Add early stopping and model checkpoint callbacks\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    callbacks.extend([early_stopping, model_checkpoint])\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    history = model.fit(\n",
    "        train_sequence,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_sequence,\n",
    "        callbacks=model_checkpoint,\n",
    "        verbose=1\n",
    "    )\n",
    "    # -------------------------------------------------\n",
    "    # Visualize training & validation loss\n",
    "    # -------------------------------------------------\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history.history['loss'],      label='Training loss')\n",
    "    plt.plot(history.history['val_loss'],  label='Validation loss')\n",
    "    plt.title('Training & Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"Model training finished.\")\n",
    "    return history\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data, build, and train the model.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    train_dir = '/home/samer/Desktop/competitions/NFL_Big_Data_Bowl_2026_dev/nfl-big-data-bowl-2026-prediction/train'\n",
    "    batch_size = 32\n",
    "    epochs = 20\n",
    "    test_size = 0.2\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"NFL Big Data Bowl 2026 - Predictor Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n[1/4] Loading data from CSV files...\")\n",
    "    loader = NFLDataLoader(train_dir)\n",
    "    X, y = loader.get_aligned_data()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No data loaded. Please check the data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  Total sequences: {len(X)}\")\n",
    "    print(f\"  Sample input sequence length: {len(X[0])}\")\n",
    "    print(f\"  Sample output sequence length: {len(y[0])}\")\n",
    "    print(f\"  Input features per timestep: {len(X[0][0]) if len(X[0]) > 0 else 0}\")\n",
    "    print(f\"  Output features per timestep: {len(y[0][0]) if len(y[0]) > 0 else 0}\")\n",
    "    \n",
    "    # Create Keras Sequences with padding\n",
    "    print(f\"\\n[2/4] Creating training and validation sequences (test_size={test_size})...\")\n",
    "    train_seq, val_seq = create_tf_datasets(X, y, test_size=test_size, batch_size=batch_size)\n",
    "    \n",
    "    if train_seq is None:\n",
    "        print(\"Error: Failed to create training sequences.\")\n",
    "        return\n",
    "    \n",
    "    # Get one batch to determine shapes\n",
    "    x_sample, y_sample = train_seq[0]\n",
    "    input_seq_length = x_sample.shape[1]\n",
    "    input_features = x_sample.shape[2]\n",
    "    output_seq_length = y_sample.shape[1]\n",
    "    output_features = y_sample.shape[2]\n",
    "    \n",
    "    print(f\"\\nSequence Shapes:\")\n",
    "    print(f\"  Input: (batch_size, {input_seq_length}, {input_features})\")\n",
    "    print(f\"  Output: (batch_size, {output_seq_length}, {output_features})\")\n",
    "    \n",
    "    # Build model\n",
    "    print(f\"\\n[3/4] Building sequence-to-sequence model...\")\n",
    "    model = build_seq2seq_model(\n",
    "        input_seq_length=input_seq_length,\n",
    "        input_features=input_features,\n",
    "        output_seq_length=output_seq_length,\n",
    "        output_features=output_features,\n",
    "        lstm_units=128\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\n[4/4] Training model for {epochs} epochs...\")\n",
    "    history = train_model(model, train_seq, val_seq, epochs=epochs)\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = 'nfl_predictor_final.keras'\n",
    "    model.save(final_model_path)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Final model saved to: {final_model_path}\")\n",
    "    print(f\"Best model saved to: best_model.keras\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Print training summary\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"  Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"  Final training MAE: {history.history['mae'][-1]:.4f}\")\n",
    "    print(f\"  Final validation MAE: {history.history['val_mae'][-1]:.4f}\")\n",
    "    print(f\"  Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpuV5e8",
   "dataSources": [
    {
     "databundleVersionId": 14210809,
     "sourceId": 114239,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
