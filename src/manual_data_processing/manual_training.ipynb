{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":14210809,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import csv\nimport numpy as np\nimport os\n\nclass NFLDataLoader:\n    \"\"\"\n    Loads and processes NFL Big Data Bowl 2026 data from CSV files.\n    Filters input data for 'player_to_predict' == True and aligns with output data.\n    \"\"\"\n    def __init__(self, train_dir):\n        self.train_dir = train_dir\n        self.input_sequences = {}\n        self.output_sequences = {}\n        self.input_header = []\n        self.output_header = []\n\n    def process_value(self, val):\n        \"\"\"\n        Converts a CSV string value into the appropriate type.\n        \"\"\"\n        val_lower = val.lower()\n\n        # Handle Booleans\n        if val_lower == 'true':\n            return 1.0\n        if val_lower == 'false':\n            return 0.0\n        \n        # Handle Direction (left/right)\n        if val_lower == 'left':\n            return 0.0\n        if val_lower == 'right':\n            return 1.0\n\n        # Handle Player Side (defense/offense)\n        if val_lower == 'defense':\n            return 0.0\n        if val_lower == 'offense':\n            return 1.0\n        \n        # Handle Numbers (Integers and Floats)\n        try:\n            return float(val)\n        except ValueError:\n            pass\n            \n        # Handle Strings (Object type)\n        return str(val)\n\n    def load_input_files(self):\n        \"\"\"\n        Loads and filters input CSV files.\n        \"\"\"\n        input_files = sorted([f for f in os.listdir(self.train_dir) if f.startswith('input') and f.endswith('.csv')])\n        print(f\"Loading and filtering {len(input_files)} Input files...\")\n        \n        for input_file in input_files:\n            input_path = os.path.join(self.train_dir, input_file)\n            with open(input_path, 'r') as f:\n                reader = csv.reader(f)\n                first_row = True\n                \n                # Indices for ID columns\n                player_to_predict_idx = -1\n                game_id_idx = -1\n                play_id_idx = -1\n                nfl_id_idx = -1\n\n                for row in reader:\n                    if first_row:\n                        if not self.input_header:\n                            self.input_header = row\n                        \n                        try:\n                            player_to_predict_idx = row.index('player_to_predict')\n                            game_id_idx = row.index('game_id')\n                            play_id_idx = row.index('play_id')\n                            nfl_id_idx = row.index('nfl_id')\n                        except ValueError as e:\n                            print(f\"Error finding columns in {input_file}: {e}\")\n                            break\n                        \n                        first_row = False\n                        continue\n                    \n                    # Filter: Only keep rows where player_to_predict is True\n                    if player_to_predict_idx != -1:\n                        val = row[player_to_predict_idx].lower()\n                        if val != 'true':\n                            continue \n\n                    # Extract Key\n                    key = (row[game_id_idx], row[play_id_idx], row[nfl_id_idx])\n                    \n                    if key not in self.input_sequences:\n                        self.input_sequences[key] = []\n                    self.input_sequences[key].append([self.process_value(item) for item in row])\n\n    def load_output_files(self):\n        \"\"\"\n        Loads output CSV files.\n        \"\"\"\n        output_files = sorted([f for f in os.listdir(self.train_dir) if f.startswith('output') and f.endswith('.csv')])\n        print(f\"Loading {len(output_files)} Output files...\")\n        \n        features_to_keep = ['x', 'y']\n\n        for output_file in output_files:\n            output_path = os.path.join(self.train_dir, output_file)\n            with open(output_path, 'r') as f:\n                reader = csv.reader(f)\n                first_row = True\n                \n                # Indices for ID columns\n                game_id_idx = -1\n                play_id_idx = -1\n                nfl_id_idx = -1\n                \n                # Indices for feature columns\n                feature_indices = []\n\n                for row in reader:\n                    if first_row:\n                        if not self.output_header:\n                            self.output_header = row\n                        \n                        try:\n                            game_id_idx = row.index('game_id')\n                            play_id_idx = row.index('play_id')\n                            nfl_id_idx = row.index('nfl_id')\n                            \n                            # Find indices for the features we want to keep\n                            feature_indices = [row.index(feat) for feat in features_to_keep]\n                            \n                        except ValueError as e:\n                            print(f\"Error finding columns in {output_file}: {e}\")\n                            break\n\n                        first_row = False\n                        continue\n                    \n                    # Extract Key\n                    key = (row[game_id_idx], row[play_id_idx], row[nfl_id_idx])\n                    \n                    if key not in self.output_sequences:\n                        self.output_sequences[key] = []\n                    \n                    # Append only the selected features (x, y)\n                    self.output_sequences[key].append([float(row[i]) for i in feature_indices])\n\n    def get_aligned_data(self):\n        \"\"\"\n        Aligns input and output sequences and returns NumPy arrays.\n        Returns:\n            X (np.ndarray): Input sequences\n            y (np.ndarray): Output sequences\n        \"\"\"\n        self.load_input_files()\n        self.load_output_files()\n\n        print(\"Aligning Input and Output sequences...\")\n        common_keys = sorted(list(set(self.input_sequences.keys()).intersection(set(self.output_sequences.keys()))))\n\n        aligned_X = []\n        aligned_y = []\n\n        for key in common_keys:\n            aligned_X.append(self.input_sequences[key])\n            aligned_y.append(self.output_sequences[key])\n\n        print(f\"Processing complete.\")\n        print(f\"Total Unique Sequences (Matches): {len(common_keys)}\")\n\n        if not aligned_X:\n            print(\"No matching data found.\")\n            return np.array([]), np.array([])\n\n        # Convert to NumPy arrays\n        # Using dtype=object to handle potential variable lengths or mixed types safely\n        try:\n            X = np.array(aligned_X, dtype=object)\n            print(f\"Initial X shape: {X.shape}\")\n        except Exception as e:\n            print(f\"Error creating X array: {e}\")\n            X = np.array([])\n\n        try:\n            y = np.array(aligned_y, dtype=object)\n            print(f\"Initial y shape: {y.shape}\")\n        except Exception as e:\n            print(f\"Error creating y array: {e}\")\n            y = np.array([])\n            \n        return X, y\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import Sequence\n\nclass NFLDataSequence(Sequence):\n    \"\"\"\n    Keras Sequence for NFL data with automatic padding of variable-length sequences.\n    \"\"\"\n    def __init__(self, X, y, batch_size=32, maxlen_x=None, maxlen_y=None, shuffle=True):\n        \"\"\"\n        Args:\n            X (list): List of input sequences (each sequence is a list of time steps)\n            y (list): List of output sequences (each sequence is a list of time steps)\n            batch_size (int): Batch size\n            maxlen_x (int, optional): Maximum length for input sequences. If None, uses max length in data.\n            maxlen_y (int, optional): Maximum length for output sequences. If None, uses max length in data.\n            shuffle (bool): Whether to shuffle data at the end of each epoch\n        \"\"\"\n        self.X = X\n        self.y = y\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.indices = np.arange(len(self.X))\n        \n        # Determine max lengths if not provided\n        if maxlen_x is None:\n            self.maxlen_x = max(len(seq) for seq in X)\n        else:\n            self.maxlen_x = maxlen_x\n            \n        if maxlen_y is None:\n            self.maxlen_y = max(len(seq) for seq in y)\n        else:\n            self.maxlen_y = maxlen_y\n        \n        print(f\"NFLDataSequence initialized: {len(self.X)} samples, batch_size={batch_size}\")\n        print(f\"Max sequence lengths - X: {self.maxlen_x}, y: {self.maxlen_y}\")\n        \n        if self.shuffle:\n            np.random.shuffle(self.indices)\n    \n    def __len__(self):\n        \"\"\"Number of batches per epoch\"\"\"\n        return int(np.ceil(len(self.X) / self.batch_size))\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Generate one batch of data\n        \"\"\"\n        # Get batch indices\n        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n        \n        # Get batch data\n        batch_X = [self.X[i] for i in batch_indices]\n        batch_y = [self.y[i] for i in batch_indices]\n        \n        # Process X sequences: handle mixed types\n        # The data from process_value() should already have numeric types as floats\n        # and strings as strings. We need to filter out or encode string columns.\n        batch_X_numeric = []\n        for seq in batch_X:\n            seq_numeric = []\n            for frame in seq:\n                frame_numeric = []\n                for item in frame:\n                    # If item is already a float or int (from process_value), keep it\n                    if isinstance(item, (int, float)):\n                        frame_numeric.append(float(item))\n                    # If it's a string, we need to handle it\n                    # For now, let's use a hash or skip it\n                    # Better approach: filter these columns out or use proper encoding\n                    elif isinstance(item, str):\n                        # Try to convert to float, if fails, use hash or 0\n                        try:\n                            frame_numeric.append(float(item))\n                        except ValueError:\n                            # For non-numeric strings, use a simple hash-based encoding\n                            # This is a simple placeholder - ideally use proper categorical encoding\n                            frame_numeric.append(float(hash(item) % 10000))\n                    else:\n                        frame_numeric.append(0.0)\n                seq_numeric.append(frame_numeric)\n            batch_X_numeric.append(seq_numeric)\n        \n        # Use pad_sequences for both X and y\n        # pad_sequences expects sequences of shape (n_samples, n_timesteps) for 2D\n        # For 3D (n_samples, n_timesteps, n_features), we need to pad manually or use padding='post'\n        \n        # Method: Pad each sequence to maxlen, filling with zeros\n        X_padded = pad_sequences(\n            batch_X_numeric, \n            maxlen=self.maxlen_x, \n            dtype='float32',\n            padding='post',\n            truncating='post',\n            value=0.0\n        )\n        \n        y_padded = pad_sequences(\n            batch_y,\n            maxlen=self.maxlen_y,\n            dtype='float32',\n            padding='post',\n            truncating='post',\n            value=0.0\n        )\n        \n        return X_padded, y_padded\n    \n    def on_epoch_end(self):\n        \"\"\"Shuffle indices after each epoch\"\"\"\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n\n\ndef create_tf_datasets(X, y, test_size=0.2, batch_size=32, maxlen_x=None, maxlen_y=None):\n    \"\"\"\n    Splits X and y into training and validation sets and creates Keras Sequence datasets.\n    Uses keras.utils.Sequence with padding to handle variable-length sequences.\n    \n    Args:\n        X (np.ndarray): Input data (object array of variable-length sequences).\n        y (np.ndarray): Output data (object array of variable-length sequences).\n        test_size (float): Proportion of the dataset to include in the validation split.\n        batch_size (int): Batch size for the datasets.\n        maxlen_x (int, optional): Maximum length for input sequences. If None, auto-detects.\n        maxlen_y (int, optional): Maximum length for output sequences. If None, auto-detects.\n        \n    Returns:\n        train_sequence (NFLDataSequence): Training data sequence.\n        val_sequence (NFLDataSequence): Validation data sequence.\n    \"\"\"\n    print(\"\\n--- Creating Keras Sequence Datasets with Padding ---\")\n    \n    try:\n        # Convert object arrays to lists\n        X_list = X.tolist()\n        y_list = y.tolist()\n        \n        # Split into train and validation\n        print(f\"Splitting data (test_size={test_size})...\")\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_list, y_list, \n            test_size=test_size, \n            random_state=42\n        )\n        \n        print(f\"Train size: {len(X_train)}\")\n        print(f\"Val size: {len(X_val)}\")\n        \n        # Create Sequence objects\n        print(\"Creating Training Sequence...\")\n        train_sequence = NFLDataSequence(\n            X_train, y_train, \n            batch_size=batch_size,\n            maxlen_x=maxlen_x,\n            maxlen_y=maxlen_y,\n            shuffle=True\n        )\n        \n        print(\"Creating Validation Sequence...\")\n        val_sequence = NFLDataSequence(\n            X_val, y_val,\n            batch_size=batch_size,\n            maxlen_x=train_sequence.maxlen_x,  # Use same max lengths as training\n            maxlen_y=train_sequence.maxlen_y,\n            shuffle=False\n        )\n        \n        print(\"Sequences created successfully.\")\n        print(f\"Training batches per epoch: {len(train_sequence)}\")\n        print(f\"Validation batches per epoch: {len(val_sequence)}\")\n        \n        return train_sequence, val_sequence\n\n    except Exception as e:\n        print(f\"Error creating Keras sequences: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None, None\n\nif __name__ == \"__main__\":\n    TRAIN_DIR = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n    \n    loader = NFLDataLoader(TRAIN_DIR)\n    X, y = loader.get_aligned_data()\n\n    print(\"\\n--- Final Data Shapes ---\")\n    print(f\"X (Input) Shape: {X.shape}\")\n    print(f\"y (Output) Shape: {y.shape}\")\n\n    if len(X) > 0:\n        print(f\"Sample Input Sequence Length: {len(X[0])}\")\n        print(f\"Sample Output Sequence Length: {len(y[0])}\")\n\n    # Create Keras Sequences with padding\n    train_seq, val_seq = create_tf_datasets(X, y, batch_size=32)\n    \n    if train_seq:\n        print(\"\\nVerifying Sequence Element:\")\n        # Get one batch to verify shapes\n        x_batch, y_batch = train_seq[0]\n        print(f\"Batch X shape: {x_batch.shape}\")\n        print(f\"Batch y shape: {y_batch.shape}\")\n        print(f\"Max sequence lengths - X: {train_seq.maxlen_x}, y: {train_seq.maxlen_y}\")\n\n    print(\"\\nData loading, alignment, and sequence creation complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:09:59.204377Z","iopub.execute_input":"2025-11-24T18:09:59.205232Z","iopub.status.idle":"2025-11-24T18:10:58.638918Z","shell.execute_reply.started":"2025-11-24T18:09:59.205196Z","shell.execute_reply":"2025-11-24T18:10:58.638163Z"}},"outputs":[{"name":"stderr","text":"2025-11-24 18:10:01.907680: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764007802.409420      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764007802.599940      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Loading and filtering 18 Input files...\nLoading 18 Output files...\nAligning Input and Output sequences...\nProcessing complete.\nTotal Unique Sequences (Matches): 46045\nInitial X shape: (46045,)\nInitial y shape: (46045,)\n\n--- Final Data Shapes ---\nX (Input) Shape: (46045,)\ny (Output) Shape: (46045,)\nSample Input Sequence Length: 38\nSample Output Sequence Length: 12\n\n--- Creating Keras Sequence Datasets with Padding ---\nSplitting data (test_size=0.2)...\nTrain size: 36836\nVal size: 9209\nCreating Training Sequence...\nNFLDataSequence initialized: 36836 samples, batch_size=32\nMax sequence lengths - X: 123, y: 94\nCreating Validation Sequence...\nNFLDataSequence initialized: 9209 samples, batch_size=32\nMax sequence lengths - X: 123, y: 94\nSequences created successfully.\nTraining batches per epoch: 1152\nValidation batches per epoch: 288\n\nVerifying Sequence Element:\nBatch X shape: (32, 123, 23)\nBatch y shape: (32, 94, 2)\nMax sequence lengths - X: 123, y: 94\n\nData loading, alignment, and sequence creation complete.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport sys\nimport keras_tuner\n\n# Add the manual_data_processing directory to the path\n# sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'manual_data_processing'))\n\n# from csv_to_numpy import NFLDataLoader, create_tf_datasets\n\n\ndef build_model(hp):\n    \"\"\"\n    Builds a compiled Keras LSTM model with hyperparameters to be experimented on.\n\n    This function defines the architecture of the LSTM model for sequence-to-sequence prediction.\n    It incorporates hyperparameter search spaces for key model parameters like learning rate,\n    number of LSTM units, kernel regularization, and activation functions.\n\n    Args:\n        hp (keras_tuner.HyperParameters): An instance of Keras Tuner's HyperParameters class,\n                                          used to define the search space for hyperparameters.\n\n    Returns:\n        keras.Model: The compiled Keras LSTM model with hyperparameters set by Keras Tuner.\n    \"\"\"\n    \n    SEED = 42\n    # Define hyperparameter search spaces for tuning\n    learning_rate = hp.Float(\"lr\", min_value=1e-7, max_value=1e-3, sampling=\"log\")\n    layer_u = hp.Int(\"lu\", min_value=160, max_value=1024, step=8)\n    kernel_r = hp.Float(\"kr\", min_value=1e-10, max_value=1e-5, sampling=\"log\")\n    acti_f = hp.Choice(\"af\", [\"sigmoid\", \"hard_sigmoid\", \"tanh\", \"relu\", \"softmax\", \"linear\"])\n    weight_d = hp.Float(\"wd\", min_value=1e-10, max_value=0.0009, sampling=\"log\")\n\n    # Define the model structure using Keras Sequential API\n    model = keras.Sequential([\n        # Input layer\n        keras.layers.Input(shape=(input_seq_length, input_features)),\n        \n        # Encoder LSTM layers\n        keras.layers.LSTM(\n            units=layer_u,\n            activation=acti_f,\n            return_sequences=True,\n            kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n            seed=SEED,\n        ),\n        keras.layers.LSTM(\n            units=layer_u // 2,\n            activation=acti_f,\n            return_sequences=True,\n            kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n            seed=SEED,\n        ),\n        keras.layers.LSTM(\n            units=layer_u // 2,\n            activation=acti_f,\n            return_sequences=True,\n            kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n            seed=SEED,\n        ),\n        keras.layers.LSTM(\n            units=layer_u // 2,\n            activation=acti_f,\n            return_sequences=True,\n            kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n            seed=SEED,\n        ),\n        keras.layers.LSTM(\n            units=layer_u // 2,\n            activation=acti_f,\n            return_sequences=False,\n            kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n            seed=SEED,\n        ),\n        layers.RepeatVector(output_seq_length),\n        keras.layers.LSTM(\n            units=32,\n            activation=\"sigmoid\",\n            return_sequences=True,\n            # kernel_regularizer=keras.regularizers.L2(l2=0.00000195),\n            seed=SEED,\n        ),\n        # Crop or slice to match output sequence length\n        # layers.Lambda(lambda x: x[:, :output_seq_length, :]),\n        # TimeDistributed dense layer for output features\n        layers.TimeDistributed(\n            keras.layers.Dense(units=output_features, activation=\"linear\")\n        ),\n    ])\n\n    # Compile the model with a tunable optimizer and metrics\n    model.compile(\n        loss=keras.losses.MeanSquaredError(),\n        optimizer=keras.optimizers.Adam(\n            learning_rate=learning_rate,\n            global_clipnorm=1,\n            amsgrad=False,\n            # weight_decay=weight_d, # Tunable weight decay\n        ),\n        metrics=[tf.keras.metrics.MeanAbsoluteError()],\n    )\n\n    return model\n\n\ndef experimenting(training_dataset, validation_data):\n    \"\"\"\n    Runs Keras Tuner experiments for the LSTM model using the RandomSearch algorithm.\n\n    This function initializes a `RandomSearch` tuner with the `build_model` function,\n    configures the search objective (minimizing validation loss), and then executes\n    the hyperparameter search across the defined search spaces. It prints summaries\n    of the search space and the results.\n\n    Args:\n        training_dataset: NFLDataSequence object for training data\n        validation_data: NFLDataSequence object for validation data\n\n    \"\"\"\n\n    hp = keras_tuner.HyperParameters()\n    \n    # Get a batch from the sequence to determine shapes\n    x_batch, y_batch = training_dataset[0]\n    global input_features, input_seq_length, output_seq_length, output_features\n    input_seq_length = x_batch.shape[1]\n    input_features = x_batch.shape[2]\n    output_seq_length = y_batch.shape[1]\n    output_features = y_batch.shape[2]\n    \n    print(f\"\\nDetected shapes:\")\n    print(f\"  Input: ({input_seq_length}, {input_features})\")\n    print(f\"  Output: ({output_seq_length}, {output_features})\")\n    \n    build_model(hp) # Instantiate a dummy model to build the search space\n\n    # Initialize Keras Tuner's RandomSearch algorithm\n    tuner = keras_tuner.RandomSearch(\n        hypermodel=build_model,\n        max_trials=100, # Maximum number of hyperparameter combinations to try\n        objective=keras_tuner.Objective(\"val_loss\", \"min\"),   # Objective is to minimize validation loss\n        executions_per_trial=1, # Number of models to train for each trial (1 for efficiency)\n        overwrite=True, # Overwrite previous results in the directory\n        directory=os.getenv(\"KERAS_TUNER_EXPERIMENTS_DIR\", \"./tuner_results\"), # Directory to save experiment logs and checkpoints\n        project_name=\"nfl_prediction\", # Name of the Keras Tuner project\n        seed = 42,\n        max_consecutive_failed_trials=5,\n    )\n\n    tuner.search_space_summary() # Print a summary of the hyperparameter search space\n\n    # NFLDataSequence is already batched, no need to call batch() again\n    # Run the hyperparameter search experiments\n    tuner.search(\n        training_dataset, \n        validation_data=validation_data, \n        epochs=5\n    )\n\n    tuner.results_summary() # Print a summary of the best performing trials\n\n\nif __name__ == \"__main__\":\n    train_dir = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n    batch_size = 32\n    epochs = 50\n    test_size = 0.2\n    \n    print(\"=\"*60)\n    print(\"NFL Big Data Bowl 2026 - Predictor Training\")\n    print(\"=\"*60)\n    \n    # Load and prepare data\n    print(\"\\n[1/4] Loading data from CSV files...\")\n    loader = NFLDataLoader(train_dir)\n    X, y = loader.get_aligned_data()\n    \n    if len(X) == 0:\n        print(\"Error: No data loaded. Please check the data directory.\")\n    \n    print(f\"\\nData Summary:\")\n    print(f\"  Total sequences: {len(X)}\")\n    print(f\"  Sample input sequence length: {len(X[0])}\")\n    print(f\"  Sample output sequence length: {len(y[0])}\")\n    print(f\"  Input features per timestep: {len(X[0][0]) if len(X[0]) > 0 else 0}\")\n    print(f\"  Output features per timestep: {len(y[0][0]) if len(y[0]) > 0 else 0}\")\n    \n    # Create Keras Sequences with padding\n    print(f\"\\n[2/4] Creating training and validation sequences (test_size={test_size})...\")\n    train_seq, val_seq = create_tf_datasets(X, y, test_size=test_size, batch_size=batch_size)\n    \n    # Run the hyperparameter experimentation\n    experimenting(train_seq, val_seq)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T03:23:40.846171Z","iopub.execute_input":"2025-11-24T03:23:40.846712Z","iopub.status.idle":"2025-11-24T04:45:21.292182Z","shell.execute_reply.started":"2025-11-24T03:23:40.846691Z","shell.execute_reply":"2025-11-24T04:45:21.291053Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Trial 11 Complete [00h 00m 42s]\n\nBest val_loss So Far: 211.86563110351562\nTotal elapsed time: 01h 20m 32s\n\nSearch: Running Trial #12\n\nValue             |Best Value So Far |Hyperparameter\n6e-07             |0.00051519        |lr\n832               |1000              |lu\n1.0469e-08        |2.4032e-08        |kr\nsigmoid           |tanh              |af\n1.1002e-06        |3.1076e-10        |wd\n\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n    except TypeError as e:\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_48/1466575972.py\", line 197, in <cell line: 0>\n\n  File \"/tmp/ipykernel_48/1466575972.py\", line 158, in experimenting\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 234, in search\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nOut of memory while trying to allocate 4842847856 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_563245]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/1466575972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# Run the hyperparameter experimentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mexperimenting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_48/1466575972.py\u001b[0m in \u001b[0;36mexperimenting\u001b[0;34m(training_dataset, validation_data)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# NFLDataSequence is already batched, no need to call batch() again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# Run the hyperparameter search experiments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     tuner.search(\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36mon_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTrial\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mLOCKS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_acquire\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mend_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_consecutive_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36m_check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_consecutive_failed_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    546\u001b[0m                     \u001b[0;34m\"Number of consecutive failures exceeded the limit \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                     \u001b[0;34mf\"of {self.max_consecutive_failed_trials}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 5.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n    except TypeError as e:\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_48/1466575972.py\", line 197, in <cell line: 0>\n\n  File \"/tmp/ipykernel_48/1466575972.py\", line 158, in experimenting\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 234, in search\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nOut of memory while trying to allocate 4842847856 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_563245]\n"],"ename":"RuntimeError","evalue":"Number of consecutive failures exceeded the limit of 5.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n    except TypeError as e:\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_48/1466575972.py\", line 197, in <cell line: 0>\n\n  File \"/tmp/ipykernel_48/1466575972.py\", line 158, in experimenting\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 234, in search\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nOut of memory while trying to allocate 4842847856 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_563245]\n","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport sys\n\n# Add the manual_data_processing directory to the path\n# sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'manual_data_processing'))\n\n# from csv_to_numpy import NFLDataLoader, create_tf_datasets\n\ndef build_seq2seq_model(input_seq_length, input_features, output_seq_length, output_features, lstm_units=128):\n    \"\"\"\n    Builds a sequence-to-sequence model with LSTM layers.\n\n    Args:\n        input_seq_length (int): The length of input sequences (time steps).\n        input_features (int): The number of input features per timestep.\n        output_seq_length (int): The length of output sequences (time steps).\n        output_features (int): The number of output features per timestep.\n        lstm_units (int): The number of units in the LSTM layers.\n\n    Returns:\n        keras.Model: The compiled Keras model.\n    \"\"\"\n\n    SEED = 42\n    # Encoder-decoder architecture for sequence-to-sequence prediction\n    model = keras.Sequential([\n        # Input layer\n        keras.layers.Input(shape=(input_seq_length, input_features)),\n        \n        # Encoder LSTM layers\n        keras.layers.LSTM(\n            units=512,\n            activation=\"tanh\",\n            return_sequences=True,\n            kernel_regularizer=keras.regularizers.L2(l2=2.4032e-08),\n            seed=SEED,\n        ),\n        # keras.layers.LSTM(\n        #     units=256,\n        #     activation=\"tanh\",\n        #     return_sequences=True,\n        #     kernel_regularizer=keras.regularizers.L2(l2=2.4032e-08),\n        #     seed=SEED,\n        # ),\n        # keras.layers.LSTM(\n        #     units=256,\n        #     activation=\"tanh\",\n        #     return_sequences=True,\n        #     kernel_regularizer=keras.regularizers.L2(l2=2.4032e-08),\n        #     seed=SEED,\n        # ),\n        keras.layers.LSTM(\n            units=256,\n            activation=\"tanh\",\n            return_sequences=True,\n            kernel_regularizer=keras.regularizers.L2(l2=2.4032e-08),\n            seed=SEED,\n        ),\n        keras.layers.LSTM(\n            units=128,\n            activation=\"tanh\",\n            return_sequences=False,\n            kernel_regularizer=keras.regularizers.L2(l2=2.4032e-08),\n            seed=SEED,\n        ),\n        layers.RepeatVector(output_seq_length),\n        keras.layers.LSTM(\n            units=32,\n            activation=\"tanh\",\n            return_sequences=True,\n            # kernel_regularizer=keras.regularizers.L2(l2=0.00000195),\n            seed=SEED,\n        ),\n        # Crop or slice to match output sequence length\n        # layers.Lambda(lambda x: x[:, :output_seq_length, :]),\n        # TimeDistributed dense layer for output features\n        layers.TimeDistributed(\n            keras.layers.Dense(units=output_features, activation=\"sigmoid\")\n        ),\n    ])\n\n    cosine_decay = keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=1e-3,\n    decay_steps=415000,\n    alpha=1e-5,\n    )\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=cosine_decay),\n        loss='mse',\n        metrics=['mae']\n    )\n    \n    return model\n\ndef train_model(model, train_sequence, val_sequence, epochs=10, callbacks=None):\n    \"\"\"\n    Trains the Keras model using Keras Sequence objects.\n    \n    Args:\n        model: The Keras model to train\n        train_sequence: Training data sequence (NFLDataSequence)\n        val_sequence: Validation data sequence (NFLDataSequence)\n        epochs (int): Number of training epochs\n        callbacks: List of Keras callbacks\n    \n    Returns:\n        history: Training history object\n    \"\"\"\n    if callbacks is None:\n        callbacks = []\n    \n    # Add early stopping and model checkpoint callbacks\n    early_stopping = keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=5,\n        restore_best_weights=True,\n        verbose=1\n    )\n    \n    model_checkpoint = keras.callbacks.ModelCheckpoint(\n        'best_model.keras',\n        monitor='val_loss',\n        save_best_only=True,\n        verbose=1\n    )\n    \n    callbacks.extend([early_stopping, model_checkpoint])\n    \n    print(\"Starting model training...\")\n    history = model.fit(\n        train_sequence,\n        epochs=epochs,\n        validation_data=val_sequence,\n        callbacks=model_checkpoint,\n        verbose=1\n    )\n    print(\"Model training finished.\")\n    return history\n\ndef main():\n    \"\"\"\n    Main function to load data, build, and train the model.\n    \"\"\"\n    # Configuration\n    train_dir = '/kaggle/input/nfl-big-data-bowl-2026-prediction/train'\n    batch_size = 32\n    epochs = 200\n    test_size = 0.2\n    \n    print(\"=\"*60)\n    print(\"NFL Big Data Bowl 2026 - Predictor Training\")\n    print(\"=\"*60)\n    \n    # Load and prepare data\n    print(\"\\n[1/4] Loading data from CSV files...\")\n    loader = NFLDataLoader(train_dir)\n    X, y = loader.get_aligned_data()\n    \n    if len(X) == 0:\n        print(\"Error: No data loaded. Please check the data directory.\")\n        return\n    \n    print(f\"\\nData Summary:\")\n    print(f\"  Total sequences: {len(X)}\")\n    print(f\"  Sample input sequence length: {len(X[0])}\")\n    print(f\"  Sample output sequence length: {len(y[0])}\")\n    print(f\"  Input features per timestep: {len(X[0][0]) if len(X[0]) > 0 else 0}\")\n    print(f\"  Output features per timestep: {len(y[0][0]) if len(y[0]) > 0 else 0}\")\n    \n    # Create Keras Sequences with padding\n    print(f\"\\n[2/4] Creating training and validation sequences (test_size={test_size})...\")\n    train_seq, val_seq = create_tf_datasets(X, y, test_size=test_size, batch_size=batch_size)\n    \n    if train_seq is None:\n        print(\"Error: Failed to create training sequences.\")\n        return\n    \n    # Get one batch to determine shapes\n    x_sample, y_sample = train_seq[0]\n    input_seq_length = x_sample.shape[1]\n    input_features = x_sample.shape[2]\n    output_seq_length = y_sample.shape[1]\n    output_features = y_sample.shape[2]\n    \n    print(f\"\\nSequence Shapes:\")\n    print(f\"  Input: (batch_size, {input_seq_length}, {input_features})\")\n    print(f\"  Output: (batch_size, {output_seq_length}, {output_features})\")\n    \n    # Build model\n    print(f\"\\n[3/4] Building sequence-to-sequence model...\")\n    model = build_seq2seq_model(\n        input_seq_length=input_seq_length,\n        input_features=input_features,\n        output_seq_length=output_seq_length,\n        output_features=output_features,\n        lstm_units=128\n    )\n    \n    print(\"\\nModel Architecture:\")\n    model.summary()\n    \n    # Train model\n    print(f\"\\n[4/4] Training model for {epochs} epochs...\")\n    history = train_model(model, train_seq, val_seq, epochs=epochs)\n    \n    # Save the final model\n    final_model_path = 'nfl_predictor_final.keras'\n    model.save(final_model_path)\n    print(f\"\\n{'='*60}\")\n    print(f\"Training Complete!\")\n    print(f\"Final model saved to: {final_model_path}\")\n    print(f\"Best model saved to: best_model.keras\")\n    print(f\"{'='*60}\")\n    \n    # Print training summary\n    print(f\"\\nTraining Summary:\")\n    print(f\"  Final training loss: {history.history['loss'][-1]:.4f}\")\n    print(f\"  Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n    print(f\"  Final training MAE: {history.history['mae'][-1]:.4f}\")\n    print(f\"  Final validation MAE: {history.history['val_mae'][-1]:.4f}\")\n    print(f\"  Best validation loss: {min(history.history['val_loss']):.4f}\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:11:04.664315Z","iopub.execute_input":"2025-11-24T18:11:04.664851Z","iopub.status.idle":"2025-11-24T18:24:24.368835Z","shell.execute_reply.started":"2025-11-24T18:11:04.664825Z","shell.execute_reply":"2025-11-24T18:24:24.367748Z"}},"outputs":[{"name":"stdout","text":"============================================================\nNFL Big Data Bowl 2026 - Predictor Training\n============================================================\n\n[1/4] Loading data from CSV files...\nLoading and filtering 18 Input files...\nLoading 18 Output files...\nAligning Input and Output sequences...\nProcessing complete.\nTotal Unique Sequences (Matches): 46045\nInitial X shape: (46045,)\nInitial y shape: (46045,)\n\nData Summary:\n  Total sequences: 46045\n  Sample input sequence length: 38\n  Sample output sequence length: 12\n  Input features per timestep: 23\n  Output features per timestep: 2\n\n[2/4] Creating training and validation sequences (test_size=0.2)...\n\n--- Creating Keras Sequence Datasets with Padding ---\nSplitting data (test_size=0.2)...\nTrain size: 36836\nVal size: 9209\nCreating Training Sequence...\nNFLDataSequence initialized: 36836 samples, batch_size=32\nMax sequence lengths - X: 123, y: 94\nCreating Validation Sequence...\nNFLDataSequence initialized: 9209 samples, batch_size=32\nMax sequence lengths - X: 123, y: 94\nSequences created successfully.\nTraining batches per epoch: 1152\nValidation batches per epoch: 288\n\nSequence Shapes:\n  Input: (batch_size, 123, 23)\n  Output: (batch_size, 94, 2)\n\n[3/4] Building sequence-to-sequence model...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1764007896.232310      48 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1764007896.233012      48 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"\nModel Architecture:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n\n lstm (\u001b[38;5;33mLSTM\u001b[0m)                      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m123\u001b[0m, \u001b[38;5;34m512\u001b[0m)            \u001b[38;5;34m1,097,728\u001b[0m \n\n lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m123\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m787,456\u001b[0m \n\n lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                   \u001b[38;5;34m197,120\u001b[0m \n\n repeat_vector (\u001b[38;5;33mRepeatVector\u001b[0m)     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m128\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n\n lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m32\u001b[0m)                 \u001b[38;5;34m20,608\u001b[0m \n\n time_distributed                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m2\u001b[0m)                      \u001b[38;5;34m66\u001b[0m \n (\u001b[38;5;33mTimeDistributed\u001b[0m)                                                      \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n\n lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">123</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">1,097,728</span> \n\n lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">123</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">787,456</span> \n\n lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> \n\n repeat_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">20,608</span> \n\n time_distributed                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                                                      \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,102,978\u001b[0m (8.02 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,102,978</span> (8.02 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,102,978\u001b[0m (8.02 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,102,978</span> (8.02 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n[4/4] Training model for 200 epochs...\nStarting model training...\nEpoch 1/200\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nI0000 00:00:1764007905.625465     113 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1151/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 326.6335 - mae: 6.3877\nEpoch 1: val_loss improved from inf to 324.56976, saving model to best_model.keras\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 43ms/step - loss: 326.6314 - mae: 6.3877 - val_loss: 324.5698 - val_mae: 6.3960\nEpoch 2/200\n\u001b[1m1151/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 324.9747 - mae: 6.3816\nEpoch 2: val_loss improved from 324.56976 to 324.56860, saving model to best_model.keras\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 43ms/step - loss: 324.9755 - mae: 6.3816 - val_loss: 324.5686 - val_mae: 6.3961\nEpoch 3/200\n\u001b[1m1151/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 324.8340 - mae: 6.3800\nEpoch 3: val_loss improved from 324.56860 to 324.56827, saving model to best_model.keras\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 43ms/step - loss: 324.8350 - mae: 6.3800 - val_loss: 324.5683 - val_mae: 6.3961\nEpoch 4/200\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 322.9148 - mae: 6.3624\nEpoch 4: val_loss improved from 324.56827 to 324.56821, saving model to best_model.keras\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 45ms/step - loss: 322.9170 - mae: 6.3625 - val_loss: 324.5682 - val_mae: 6.3961\nEpoch 5/200\n\u001b[1m1151/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 325.4282 - mae: 6.3884\nEpoch 5: val_loss improved from 324.56821 to 324.56812, saving model to best_model.keras\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 43ms/step - loss: 325.4282 - mae: 6.3884 - val_loss: 324.5681 - val_mae: 6.3961\nEpoch 6/200\n\u001b[1m1151/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 324.2146 - mae: 6.3792\nEpoch 6: val_loss did not improve from 324.56812\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 43ms/step - loss: 324.2167 - mae: 6.3793 - val_loss: 324.5681 - val_mae: 6.3961\nEpoch 7/200\n\u001b[1m1151/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 324.0005 - mae: 6.3783\nEpoch 7: val_loss did not improve from 324.56812\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 43ms/step - loss: 324.0030 - mae: 6.3783 - val_loss: 324.5681 - val_mae: 6.3961\nEpoch 8/200\n\u001b[1m1151/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 324.1779 - mae: 6.3725\nEpoch 8: val_loss did not improve from 324.56812\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 43ms/step - loss: 324.1800 - mae: 6.3726 - val_loss: 324.5681 - val_mae: 6.3961\nEpoch 9/200\n\u001b[1m1151/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 324.9592 - mae: 6.3648\nEpoch 9: val_loss improved from 324.56812 to 324.56805, saving model to best_model.keras\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 43ms/step - loss: 324.9600 - mae: 6.3648 - val_loss: 324.5681 - val_mae: 6.3961\nEpoch 10/200\n\u001b[1m1151/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 324.0886 - mae: 6.3757\nEpoch 10: val_loss did not improve from 324.56805\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 43ms/step - loss: 324.0909 - mae: 6.3757 - val_loss: 324.5681 - val_mae: 6.3961\nEpoch 11/200\n\u001b[1m1151/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 325.0267 - mae: 6.3798\nEpoch 11: val_loss did not improve from 324.56805\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 43ms/step - loss: 325.0274 - mae: 6.3798 - val_loss: 324.5681 - val_mae: 6.3961\nEpoch 12/200\n\u001b[1m1151/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 327.8295 - mae: 6.4192\nEpoch 12: val_loss did not improve from 324.56805\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 43ms/step - loss: 327.8253 - mae: 6.4191 - val_loss: 324.5681 - val_mae: 6.3961\nEpoch 13/200\n\u001b[1m1151/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 325.6978 - mae: 6.3905\nEpoch 13: val_loss did not improve from 324.56805\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 43ms/step - loss: 325.6973 - mae: 6.3905 - val_loss: 324.5681 - val_mae: 6.3961\nEpoch 14/200\n\u001b[1m1151/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 324.3296 - mae: 6.3751\nEpoch 14: val_loss did not improve from 324.56805\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 43ms/step - loss: 324.3315 - mae: 6.3751 - val_loss: 324.5681 - val_mae: 6.3961\nEpoch 15/200\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 323.7880 - mae: 6.3734\nEpoch 15: val_loss did not improve from 324.56805\n\u001b[1m1152/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 45ms/step - loss: 323.7895 - mae: 6.3734 - val_loss: 324.5681 - val_mae: 6.3961\nEpoch 16/200\n\u001b[1m 225/1152\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 39ms/step - loss: 326.5623 - mae: 6.4334","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2028892948.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_48/2028892948.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n[4/4] Training model for {epochs} epochs...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Save the final model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/2028892948.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_sequence, val_sequence, epochs, callbacks)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting model training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mtrain_sequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":2}]}