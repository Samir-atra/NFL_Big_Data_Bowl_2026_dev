{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class NFLDataLoader:\n",
    "    \"\"\"\n",
    "    Loads and processes NFL Big Data Bowl 2026 data from CSV files.\n",
    "    Filters input data for 'player_to_predict' == True and aligns with output data.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_dir):\n",
    "        self.train_dir = train_dir\n",
    "        self.input_sequences = {}\n",
    "        self.output_sequences = {}\n",
    "        self.input_header = []\n",
    "        self.output_header = []\n",
    "\n",
    "    def process_value(self, val):\n",
    "        \"\"\"\n",
    "        Converts a CSV string value into the appropriate type.\n",
    "        \"\"\"\n",
    "        val_lower = val.lower()\n",
    "\n",
    "        # Handle Booleans\n",
    "        if val_lower == 'true':\n",
    "            return 1.0\n",
    "        if val_lower == 'false':\n",
    "            return 0.0\n",
    "        \n",
    "        # Handle Direction (left/right)\n",
    "        if val_lower == 'left':\n",
    "            return 0.0\n",
    "        if val_lower == 'right':\n",
    "            return 1.0\n",
    "\n",
    "        # Handle Player Side (defense/offense)\n",
    "        if val_lower == 'defense':\n",
    "            return 0.0\n",
    "        if val_lower == 'offense':\n",
    "            return 1.0\n",
    "        \n",
    "        # Handle Numbers (Integers and Floats)\n",
    "        try:\n",
    "            return float(val)\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "        # Handle Strings (Object type)\n",
    "        return str(val)\n",
    "\n",
    "    def load_input_files(self):\n",
    "        \"\"\"\n",
    "        Loads and filters input CSV files.\n",
    "        \"\"\"\n",
    "        input_files = sorted([f for f in os.listdir(self.train_dir) if f.startswith('input') and f.endswith('.csv')])\n",
    "        print(f\"Loading and filtering {len(input_files)} Input files...\")\n",
    "        \n",
    "        for input_file in input_files:\n",
    "            input_path = os.path.join(self.train_dir, input_file)\n",
    "            with open(input_path, 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                first_row = True\n",
    "                \n",
    "                # Indices for ID columns\n",
    "                player_to_predict_idx = -1\n",
    "                game_id_idx = -1\n",
    "                play_id_idx = -1\n",
    "                nfl_id_idx = -1\n",
    "\n",
    "                for row in reader:\n",
    "                    if first_row:\n",
    "                        if not self.input_header:\n",
    "                            self.input_header = row\n",
    "                        \n",
    "                        try:\n",
    "                            player_to_predict_idx = row.index('player_to_predict')\n",
    "                            game_id_idx = row.index('game_id')\n",
    "                            play_id_idx = row.index('play_id')\n",
    "                            nfl_id_idx = row.index('nfl_id')\n",
    "                        except ValueError as e:\n",
    "                            print(f\"Error finding columns in {input_file}: {e}\")\n",
    "                            break\n",
    "                        \n",
    "                        first_row = False\n",
    "                        continue\n",
    "                    \n",
    "                    # Filter: Only keep rows where player_to_predict is True\n",
    "                    if player_to_predict_idx != -1:\n",
    "                        val = row[player_to_predict_idx].lower()\n",
    "                        if val != 'true':\n",
    "                            continue \n",
    "\n",
    "                    # Extract Key\n",
    "                    key = (row[game_id_idx], row[play_id_idx], row[nfl_id_idx])\n",
    "                    \n",
    "                    if key not in self.input_sequences:\n",
    "                        self.input_sequences[key] = []\n",
    "                    self.input_sequences[key].append([self.process_value(item) for item in row])\n",
    "\n",
    "    def load_output_files(self):\n",
    "        \"\"\"\n",
    "        Loads output CSV files.\n",
    "        \"\"\"\n",
    "        output_files = sorted([f for f in os.listdir(self.train_dir) if f.startswith('output') and f.endswith('.csv')])\n",
    "        print(f\"Loading {len(output_files)} Output files...\")\n",
    "        \n",
    "        for output_file in output_files:\n",
    "            output_path = os.path.join(self.train_dir, output_file)\n",
    "            with open(output_path, 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                first_row = True\n",
    "                \n",
    "                # Indices for ID columns\n",
    "                game_id_idx = -1\n",
    "                play_id_idx = -1\n",
    "                nfl_id_idx = -1\n",
    "\n",
    "                for row in reader:\n",
    "                    if first_row:\n",
    "                        if not self.output_header:\n",
    "                            self.output_header = row\n",
    "                        \n",
    "                        try:\n",
    "                            game_id_idx = row.index('game_id')\n",
    "                            play_id_idx = row.index('play_id')\n",
    "                            nfl_id_idx = row.index('nfl_id')\n",
    "                        except ValueError as e:\n",
    "                            print(f\"Error finding columns in {output_file}: {e}\")\n",
    "                            break\n",
    "\n",
    "                        first_row = False\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract Key\n",
    "                    key = (row[game_id_idx], row[play_id_idx], row[nfl_id_idx])\n",
    "                    \n",
    "                    if key not in self.output_sequences:\n",
    "                        self.output_sequences[key] = []\n",
    "                    self.output_sequences[key].append([float(item) for item in row])\n",
    "\n",
    "    def get_aligned_data(self):\n",
    "        \"\"\"\n",
    "        Aligns input and output sequences and returns NumPy arrays.\n",
    "        Returns:\n",
    "            X (np.ndarray): Input sequences\n",
    "            y (np.ndarray): Output sequences\n",
    "        \"\"\"\n",
    "        self.load_input_files()\n",
    "        self.load_output_files()\n",
    "\n",
    "        print(\"Aligning Input and Output sequences...\")\n",
    "        common_keys = sorted(list(set(self.input_sequences.keys()).intersection(set(self.output_sequences.keys()))))\n",
    "\n",
    "        aligned_X = []\n",
    "        aligned_y = []\n",
    "\n",
    "        for key in common_keys:\n",
    "            aligned_X.append(self.input_sequences[key])\n",
    "            aligned_y.append(self.output_sequences[key])\n",
    "\n",
    "        print(f\"Processing complete.\")\n",
    "        print(f\"Total Unique Sequences (Matches): {len(common_keys)}\")\n",
    "\n",
    "        if not aligned_X:\n",
    "            print(\"No matching data found.\")\n",
    "            return np.array([]), np.array([])\n",
    "\n",
    "        # Convert to NumPy arrays\n",
    "        # Using dtype=object to handle potential variable lengths or mixed types safely\n",
    "        try:\n",
    "            X = np.array(aligned_X, dtype=object)\n",
    "            print(f\"Initial X shape: {X.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating X array: {e}\")\n",
    "            X = np.array([])\n",
    "\n",
    "        try:\n",
    "            y = np.array(aligned_y, dtype=object)\n",
    "            print(f\"Initial y shape: {y.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating y array: {e}\")\n",
    "            y = np.array([])\n",
    "            \n",
    "        return X, y\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class NFLDataSequence(Sequence):\n",
    "    \"\"\"\n",
    "    Keras Sequence for NFL data with automatic padding of variable-length sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size=32, maxlen_x=None, maxlen_y=None, shuffle=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (list): List of input sequences (each sequence is a list of time steps)\n",
    "            y (list): List of output sequences (each sequence is a list of time steps)\n",
    "            batch_size (int): Batch size\n",
    "            maxlen_x (int, optional): Maximum length for input sequences. If None, uses max length in data.\n",
    "            maxlen_y (int, optional): Maximum length for output sequences. If None, uses max length in data.\n",
    "            shuffle (bool): Whether to shuffle data at the end of each epoch\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        \n",
    "        # Determine max lengths if not provided\n",
    "        if maxlen_x is None:\n",
    "            self.maxlen_x = max(len(seq) for seq in X)\n",
    "        else:\n",
    "            self.maxlen_x = maxlen_x\n",
    "            \n",
    "        if maxlen_y is None:\n",
    "            self.maxlen_y = max(len(seq) for seq in y)\n",
    "        else:\n",
    "            self.maxlen_y = maxlen_y\n",
    "        \n",
    "        print(f\"NFLDataSequence initialized: {len(self.X)} samples, batch_size={batch_size}\")\n",
    "        print(f\"Max sequence lengths - X: {self.maxlen_x}, y: {self.maxlen_y}\")\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches per epoch\"\"\"\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generate one batch of data\n",
    "        \"\"\"\n",
    "        # Get batch indices\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        # Get batch data\n",
    "        batch_X = [self.X[i] for i in batch_indices]\n",
    "        batch_y = [self.y[i] for i in batch_indices]\n",
    "        \n",
    "        # Process X sequences: handle mixed types\n",
    "        # The data from process_value() should already have numeric types as floats\n",
    "        # and strings as strings. We need to filter out or encode string columns.\n",
    "        batch_X_numeric = []\n",
    "        for seq in batch_X:\n",
    "            seq_numeric = []\n",
    "            for frame in seq:\n",
    "                frame_numeric = []\n",
    "                for item in frame:\n",
    "                    # If item is already a float or int (from process_value), keep it\n",
    "                    if isinstance(item, (int, float)):\n",
    "                        frame_numeric.append(float(item))\n",
    "                    # If it's a string, we need to handle it\n",
    "                    # For now, let's use a hash or skip it\n",
    "                    # Better approach: filter these columns out or use proper encoding\n",
    "                    elif isinstance(item, str):\n",
    "                        # Try to convert to float, if fails, use hash or 0\n",
    "                        try:\n",
    "                            frame_numeric.append(float(item))\n",
    "                        except ValueError:\n",
    "                            # For non-numeric strings, use a simple hash-based encoding\n",
    "                            # This is a simple placeholder - ideally use proper categorical encoding\n",
    "                            frame_numeric.append(float(hash(item) % 10000))\n",
    "                    else:\n",
    "                        frame_numeric.append(0.0)\n",
    "                seq_numeric.append(frame_numeric)\n",
    "            batch_X_numeric.append(seq_numeric)\n",
    "        \n",
    "        # Use pad_sequences for both X and y\n",
    "        # pad_sequences expects sequences of shape (n_samples, n_timesteps) for 2D\n",
    "        # For 3D (n_samples, n_timesteps, n_features), we need to pad manually or use padding='post'\n",
    "        \n",
    "        # Method: Pad each sequence to maxlen, filling with zeros\n",
    "        X_padded = pad_sequences(\n",
    "            batch_X_numeric, \n",
    "            maxlen=self.maxlen_x, \n",
    "            dtype='float32',\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0.0\n",
    "        )\n",
    "        \n",
    "        y_padded = pad_sequences(\n",
    "            batch_y,\n",
    "            maxlen=self.maxlen_y,\n",
    "            dtype='float32',\n",
    "            padding='post',\n",
    "            truncating='post',\n",
    "            value=0.0\n",
    "        )\n",
    "        \n",
    "        return X_padded, y_padded\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffle indices after each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "\n",
    "def create_tf_datasets(X, y, test_size=0.2, batch_size=32, maxlen_x=None, maxlen_y=None):\n",
    "    \"\"\"\n",
    "    Splits X and y into training and validation sets and creates Keras Sequence datasets.\n",
    "    Uses keras.utils.Sequence with padding to handle variable-length sequences.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Input data (object array of variable-length sequences).\n",
    "        y (np.ndarray): Output data (object array of variable-length sequences).\n",
    "        test_size (float): Proportion of the dataset to include in the validation split.\n",
    "        batch_size (int): Batch size for the datasets.\n",
    "        maxlen_x (int, optional): Maximum length for input sequences. If None, auto-detects.\n",
    "        maxlen_y (int, optional): Maximum length for output sequences. If None, auto-detects.\n",
    "        \n",
    "    Returns:\n",
    "        train_sequence (NFLDataSequence): Training data sequence.\n",
    "        val_sequence (NFLDataSequence): Validation data sequence.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating Keras Sequence Datasets with Padding ---\")\n",
    "    \n",
    "    try:\n",
    "        # Convert object arrays to lists\n",
    "        X_list = X.tolist()\n",
    "        y_list = y.tolist()\n",
    "        \n",
    "        # Split into train and validation\n",
    "        print(f\"Splitting data (test_size={test_size})...\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_list, y_list, \n",
    "            test_size=test_size, \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"Train size: {len(X_train)}\")\n",
    "        print(f\"Val size: {len(X_val)}\")\n",
    "        \n",
    "        # Create Sequence objects\n",
    "        print(\"Creating Training Sequence...\")\n",
    "        train_sequence = NFLDataSequence(\n",
    "            X_train, y_train, \n",
    "            batch_size=batch_size,\n",
    "            maxlen_x=maxlen_x,\n",
    "            maxlen_y=maxlen_y,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        print(\"Creating Validation Sequence...\")\n",
    "        val_sequence = NFLDataSequence(\n",
    "            X_val, y_val,\n",
    "            batch_size=batch_size,\n",
    "            maxlen_x=train_sequence.maxlen_x,  # Use same max lengths as training\n",
    "            maxlen_y=train_sequence.maxlen_y,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        print(\"Sequences created successfully.\")\n",
    "        print(f\"Training batches per epoch: {len(train_sequence)}\")\n",
    "        print(f\"Validation batches per epoch: {len(val_sequence)}\")\n",
    "        \n",
    "        return train_sequence, val_sequence\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Keras sequences: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TRAIN_DIR = '/home/samer/Desktop/competitions/NFL_Big_Data_Bowl_2026_dev/nfl-big-data-bowl-2026-prediction/train/'\n",
    "    \n",
    "    loader = NFLDataLoader(TRAIN_DIR)\n",
    "    X, y = loader.get_aligned_data()\n",
    "\n",
    "    print(\"\\n--- Final Data Shapes ---\")\n",
    "    print(f\"X (Input) Shape: {X.shape}\")\n",
    "    print(f\"y (Output) Shape: {y.shape}\")\n",
    "\n",
    "    if len(X) > 0:\n",
    "        print(f\"Sample Input Sequence Length: {len(X[0])}\")\n",
    "        print(f\"Sample Output Sequence Length: {len(y[0])}\")\n",
    "\n",
    "    # Create Keras Sequences with padding\n",
    "    train_seq, val_seq = create_tf_datasets(X, y, batch_size=32)\n",
    "    \n",
    "    if train_seq:\n",
    "        print(\"\\nVerifying Sequence Element:\")\n",
    "        # Get one batch to verify shapes\n",
    "        x_batch, y_batch = train_seq[0]\n",
    "        print(f\"Batch X shape: {x_batch.shape}\")\n",
    "        print(f\"Batch y shape: {y_batch.shape}\")\n",
    "        print(f\"Max sequence lengths - X: {train_seq.maxlen_x}, y: {train_seq.maxlen_y}\")\n",
    "\n",
    "    print(\"\\nData loading, alignment, and sequence creation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e263020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import sys\n",
    "import keras_tuner\n",
    "\n",
    "# Add the manual_data_processing directory to the path\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'manual_data_processing'))\n",
    "\n",
    "from csv_to_numpy import NFLDataLoader, create_tf_datasets\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    Builds a compiled Keras LSTM model with hyperparameters to be experimented on.\n",
    "\n",
    "    This function defines the architecture of the LSTM model for sequence-to-sequence prediction.\n",
    "    It incorporates hyperparameter search spaces for key model parameters like learning rate,\n",
    "    number of LSTM units, kernel regularization, and activation functions.\n",
    "\n",
    "    Args:\n",
    "        hp (keras_tuner.HyperParameters): An instance of Keras Tuner's HyperParameters class,\n",
    "                                          used to define the search space for hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Keras LSTM model with hyperparameters set by Keras Tuner.\n",
    "    \"\"\"\n",
    "    \n",
    "    SEED = 42\n",
    "    # Define hyperparameter search spaces for tuning\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-5, max_value=1e-3, sampling=\"log\")\n",
    "    layer_u = hp.Int(\"lu\", min_value=160, max_value=1024, step=8)\n",
    "    kernel_r = hp.Float(\"kr\", min_value=1e-10, max_value=1e-5, sampling=\"log\")\n",
    "    acti_f = hp.Choice(\"af\", [\"sigmoid\", \"hard_sigmoid\", \"tanh\"])\n",
    "    weight_d = hp.Float(\"wd\", min_value=1e-10, max_value=0.0009, sampling=\"log\")\n",
    "\n",
    "    # Define the model structure using Keras Sequential API\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        keras.layers.Input(shape=(input_seq_length, input_features)),\n",
    "        \n",
    "        # Encoder LSTM layers\n",
    "        keras.layers.LSTM(\n",
    "            units=layer_u,\n",
    "            activation=acti_f,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        # keras.layers.LSTM(\n",
    "        #     units=layer_u // 2,\n",
    "        #     activation=acti_f,\n",
    "        #     return_sequences=True,\n",
    "        #     kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "        #     seed=SEED,\n",
    "        # ),\n",
    "        # keras.layers.LSTM(\n",
    "        #     units=layer_u // 2,\n",
    "        #     activation=acti_f,\n",
    "        #     return_sequences=True,\n",
    "        #     kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "        #     seed=SEED,\n",
    "        # ),\n",
    "        \n",
    "        # Crop to output sequence length\n",
    "        layers.Lambda(lambda x: x[:, :output_seq_length, :]),\n",
    "        \n",
    "        # TimeDistributed output layer\n",
    "        keras.layers.TimeDistributed(\n",
    "            keras.layers.Dense(units=output_features, activation=\"linear\")\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # Compile the model with a tunable optimizer and metrics\n",
    "    model.compile(\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            global_clipnorm=1,\n",
    "            amsgrad=False,\n",
    "            # weight_decay=weight_d, # Tunable weight decay\n",
    "        ),\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError()],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def experimenting(training_dataset, validation_data):\n",
    "    \"\"\"\n",
    "    Runs Keras Tuner experiments for the LSTM model using the RandomSearch algorithm.\n",
    "\n",
    "    This function initializes a `RandomSearch` tuner with the `build_model` function,\n",
    "    configures the search objective (minimizing validation loss), and then executes\n",
    "    the hyperparameter search across the defined search spaces. It prints summaries\n",
    "    of the search space and the results.\n",
    "\n",
    "    Args:\n",
    "        training_dataset: NFLDataSequence object for training data\n",
    "        validation_data: NFLDataSequence object for validation data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    hp = keras_tuner.HyperParameters()\n",
    "    \n",
    "    # Get a batch from the sequence to determine shapes\n",
    "    x_batch, y_batch = training_dataset[0]\n",
    "    global input_features, input_seq_length, output_seq_length, output_features\n",
    "    input_seq_length = x_batch.shape[1]\n",
    "    input_features = x_batch.shape[2]\n",
    "    output_seq_length = y_batch.shape[1]\n",
    "    output_features = y_batch.shape[2]\n",
    "    \n",
    "    print(f\"\\nDetected shapes:\")\n",
    "    print(f\"  Input: ({input_seq_length}, {input_features})\")\n",
    "    print(f\"  Output: ({output_seq_length}, {output_features})\")\n",
    "    \n",
    "    build_model(hp) # Instantiate a dummy model to build the search space\n",
    "\n",
    "    # Initialize Keras Tuner's RandomSearch algorithm\n",
    "    tuner = keras_tuner.RandomSearch(\n",
    "        hypermodel=build_model,\n",
    "        max_trials=10, # Maximum number of hyperparameter combinations to try\n",
    "        objective=keras_tuner.Objective(\"val_loss\", \"min\"),   # Objective is to minimize validation loss\n",
    "        executions_per_trial=1, # Number of models to train for each trial (1 for efficiency)\n",
    "        overwrite=True, # Overwrite previous results in the directory\n",
    "        directory=os.getenv(\"KERAS_TUNER_EXPERIMENTS_DIR\", \"./tuner_results\"), # Directory to save experiment logs and checkpoints\n",
    "        project_name=\"nfl_prediction\", # Name of the Keras Tuner project\n",
    "    )\n",
    "\n",
    "    tuner.search_space_summary() # Print a summary of the hyperparameter search space\n",
    "\n",
    "    # NFLDataSequence is already batched, no need to call batch() again\n",
    "    # Run the hyperparameter search experiments\n",
    "    tuner.search(\n",
    "        training_dataset, \n",
    "        validation_data=validation_data, \n",
    "        epochs=2\n",
    "    )\n",
    "\n",
    "    tuner.results_summary() # Print a summary of the best performing trials\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dir = '/home/samer/Desktop/competitions/NFL_Big_Data_Bowl_2026_dev/nfl-big-data-bowl-2026-prediction/train/'\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    test_size = 0.2\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"NFL Big Data Bowl 2026 - Predictor Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n[1/4] Loading data from CSV files...\")\n",
    "    loader = NFLDataLoader(train_dir)\n",
    "    X, y = loader.get_aligned_data()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No data loaded. Please check the data directory.\")\n",
    "    \n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  Total sequences: {len(X)}\")\n",
    "    print(f\"  Sample input sequence length: {len(X[0])}\")\n",
    "    print(f\"  Sample output sequence length: {len(y[0])}\")\n",
    "    print(f\"  Input features per timestep: {len(X[0][0]) if len(X[0]) > 0 else 0}\")\n",
    "    print(f\"  Output features per timestep: {len(y[0][0]) if len(y[0]) > 0 else 0}\")\n",
    "    \n",
    "    # Create Keras Sequences with padding\n",
    "    print(f\"\\n[2/4] Creating training and validation sequences (test_size={test_size})...\")\n",
    "    train_seq, val_seq = create_tf_datasets(X, y, test_size=test_size, batch_size=batch_size)\n",
    "    \n",
    "    # Run the hyperparameter experimentation\n",
    "    experimenting(train_seq, val_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cacd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the manual_data_processing directory to the path\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'manual_data_processing'))\n",
    "\n",
    "from csv_to_numpy import NFLDataLoader, create_tf_datasets\n",
    "\n",
    "def build_seq2seq_model(input_seq_length, input_features, output_seq_length, output_features, lstm_units=128):\n",
    "    \"\"\"\n",
    "    Builds a sequence-to-sequence model with LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        input_seq_length (int): The length of input sequences (time steps).\n",
    "        input_features (int): The number of input features per timestep.\n",
    "        output_seq_length (int): The length of output sequences (time steps).\n",
    "        output_features (int): The number of output features per timestep.\n",
    "        lstm_units (int): The number of units in the LSTM layers.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = 42\n",
    "    # Encoder-decoder architecture for sequence-to-sequence prediction\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(input_seq_length, input_features)),\n",
    "        \n",
    "        keras.layers.LSTM(\n",
    "            units=123,\n",
    "            activation=\"sigmoid\", # Fixed activation for the first layer\n",
    "            return_sequences=True,\n",
    "            # kernel_regularizer=keras.regularizers.L2(l2=0.00000195),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=64,\n",
    "            activation=\"sigmoid\", # Tunable activation function\n",
    "            return_sequences=True,\n",
    "            # kernel_regularizer=keras.regularizers.L2(l2=kernel_r), # Tunable kernel regularization\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=64, # Tunable number of units\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            # kernel_regularizer=keras.regularizers.L2(l2=kernel_r),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        keras.layers.LSTM(\n",
    "            units=32,\n",
    "            activation=\"sigmoid\",\n",
    "            return_sequences=True,\n",
    "            # kernel_regularizer=keras.regularizers.L2(l2=0.00000195),\n",
    "            seed=SEED,\n",
    "        ),\n",
    "        # Crop or slice to match output sequence length\n",
    "        layers.Lambda(lambda x: x[:, :output_seq_length, :]),\n",
    "        # TimeDistributed dense layer for output features\n",
    "        layers.TimeDistributed(keras.layers.Dense(units=output_features, activation=\"linear\")),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, train_sequence, val_sequence, epochs=10, callbacks=None):\n",
    "    \"\"\"\n",
    "    Trains the Keras model using Keras Sequence objects.\n",
    "    \n",
    "    Args:\n",
    "        model: The Keras model to train\n",
    "        train_sequence: Training data sequence (NFLDataSequence)\n",
    "        val_sequence: Validation data sequence (NFLDataSequence)\n",
    "        epochs (int): Number of training epochs\n",
    "        callbacks: List of Keras callbacks\n",
    "    \n",
    "    Returns:\n",
    "        history: Training history object\n",
    "    \"\"\"\n",
    "    if callbacks is None:\n",
    "        callbacks = []\n",
    "    \n",
    "    # Add early stopping and model checkpoint callbacks\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    callbacks.extend([early_stopping, model_checkpoint])\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    history = model.fit(\n",
    "        train_sequence,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_sequence,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Model training finished.\")\n",
    "    return history\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data, build, and train the model.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    train_dir = '/home/samer/Desktop/competitions/NFL_Big_Data_Bowl_2026_dev/nfl-big-data-bowl-2026-prediction/train/'\n",
    "    batch_size = 32\n",
    "    epochs = 5\n",
    "    test_size = 0.2\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"NFL Big Data Bowl 2026 - Predictor Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n[1/4] Loading data from CSV files...\")\n",
    "    loader = NFLDataLoader(train_dir)\n",
    "    X, y = loader.get_aligned_data()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"Error: No data loaded. Please check the data directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  Total sequences: {len(X)}\")\n",
    "    print(f\"  Sample input sequence length: {len(X[0])}\")\n",
    "    print(f\"  Sample output sequence length: {len(y[0])}\")\n",
    "    print(f\"  Input features per timestep: {len(X[0][0]) if len(X[0]) > 0 else 0}\")\n",
    "    print(f\"  Output features per timestep: {len(y[0][0]) if len(y[0]) > 0 else 0}\")\n",
    "    \n",
    "    # Create Keras Sequences with padding\n",
    "    print(f\"\\n[2/4] Creating training and validation sequences (test_size={test_size})...\")\n",
    "    train_seq, val_seq = create_tf_datasets(X, y, test_size=test_size, batch_size=batch_size)\n",
    "    \n",
    "    if train_seq is None:\n",
    "        print(\"Error: Failed to create training sequences.\")\n",
    "        return\n",
    "    \n",
    "    # Get one batch to determine shapes\n",
    "    x_sample, y_sample = train_seq[0]\n",
    "    input_seq_length = x_sample.shape[1]\n",
    "    input_features = x_sample.shape[2]\n",
    "    output_seq_length = y_sample.shape[1]\n",
    "    output_features = y_sample.shape[2]\n",
    "    \n",
    "    print(f\"\\nSequence Shapes:\")\n",
    "    print(f\"  Input: (batch_size, {input_seq_length}, {input_features})\")\n",
    "    print(f\"  Output: (batch_size, {output_seq_length}, {output_features})\")\n",
    "    \n",
    "    # Build model\n",
    "    print(f\"\\n[3/4] Building sequence-to-sequence model...\")\n",
    "    model = build_seq2seq_model(\n",
    "        input_seq_length=input_seq_length,\n",
    "        input_features=input_features,\n",
    "        output_seq_length=output_seq_length,\n",
    "        output_features=output_features,\n",
    "        lstm_units=128\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\n[4/4] Training model for {epochs} epochs...\")\n",
    "    history = train_model(model, train_seq, val_seq, epochs=epochs)\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = 'nfl_predictor_final.keras'\n",
    "    model.save(final_model_path)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Final model saved to: {final_model_path}\")\n",
    "    print(f\"Best model saved to: best_model.keras\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Print training summary\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"  Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "    print(f\"  Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"  Final training MAE: {history.history['mae'][-1]:.4f}\")\n",
    "    print(f\"  Final validation MAE: {history.history['val_mae'][-1]:.4f}\")\n",
    "    print(f\"  Best validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
